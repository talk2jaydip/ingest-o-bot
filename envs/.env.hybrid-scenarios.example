# ============================================================================
# HYBRID SCENARIOS - Mix & Match Configuration Guide
# ============================================================================
# This file demonstrates various hybrid scenarios combining different components:
#   - Extraction: Azure DI vs Markitdown
#   - Embeddings: Azure OpenAI vs Hugging Face vs Cohere vs OpenAI
#   - Vector Store: Azure Search vs ChromaDB
#   - Vision: GPT-4o vs Local vs None
#
# Copy the relevant section for your use case and remove unused configurations
# ============================================================================

# ============================================================================
# SCENARIO 1: Cost-Optimized (Markitdown + Hugging Face + Azure Search)
# ============================================================================
# USE CASE: Need cloud search scalability but want free extraction/embeddings
# COST: $ (Only Azure Search, ~$250/month Basic tier)
# PROS: Scalable search, no per-document costs
# CONS: Lower quality extraction than Azure DI

# Extraction: Markitdown (free)
EXTRACTION_MODE=markitdown
INPUT_MODE=local

# Embeddings: Hugging Face (free, local)
EMBEDDINGS_MODE=huggingface
HUGGINGFACE_MODEL=all-mpnet-base-v2
EMBEDDING_DIMENSIONS=768
HUGGINGFACE_DEVICE=auto

# Vector Store: Azure Search (paid)
VECTOR_STORE=azure_search
AZURE_SEARCH_ENDPOINT=https://your-search.search.windows.net
AZURE_SEARCH_KEY=your_search_key
AZURE_SEARCH_INDEX_NAME=ingestor-cost-optimized
VECTOR_SEARCH_DIMENSIONS=768

# Vision: Disabled (fully local processing)
ENABLE_MEDIA_DESCRIPTION=false

# ============================================================================
# SCENARIO 2: Quality Extraction + Cost-Effective Storage
# ============================================================================
# USE CASE: Need best OCR quality but don't need cloud search scale
# COST: $$ (Azure DI only, ~$1.50 per 1000 pages)
# PROS: Best extraction quality, free embeddings and storage
# CONS: Limited search scalability

# Extraction: Azure DI (paid, best quality)
EXTRACTION_MODE=azure_di
INPUT_MODE=local
AZURE_DI_ENDPOINT=https://your-di.cognitiveservices.azure.com/
AZURE_DI_KEY=your_di_key
AZURE_DI_MODEL=prebuilt-layout
AZURE_DI_EXTRACT_TABLES=true

# Embeddings: Hugging Face (free)
EMBEDDINGS_MODE=huggingface
HUGGINGFACE_MODEL=all-mpnet-base-v2
EMBEDDING_DIMENSIONS=768

# Vector Store: ChromaDB (free)
VECTOR_STORE=chromadb
CHROMADB_MODE=persistent
CHROMADB_PATH=./chroma_db
CHROMADB_COLLECTION_NAME=quality-extraction
VECTOR_SEARCH_DIMENSIONS=768

# Vision: Disabled
ENABLE_MEDIA_DESCRIPTION=false

# ============================================================================
# SCENARIO 3: Multilingual Support (Markitdown + Cohere + ChromaDB)
# ============================================================================
# USE CASE: Need 100+ language support, cost-conscious
# COST: $ (Cohere embeddings, ~$0.10 per 1M tokens)
# PROS: 100+ languages, good quality, reasonable cost
# CONS: Requires API key, not fully offline

# Extraction: Markitdown (free, supports many formats)
EXTRACTION_MODE=markitdown
INPUT_MODE=local

# Embeddings: Cohere (paid, multilingual)
EMBEDDINGS_MODE=cohere
COHERE_API_KEY=your_cohere_api_key
COHERE_MODEL=embed-multilingual-v3.0  # or embed-english-v3.0
EMBEDDING_DIMENSIONS=1024
COHERE_INPUT_TYPE=search_document  # search_document | search_query | classification

# Vector Store: ChromaDB (free)
VECTOR_STORE=chromadb
CHROMADB_MODE=persistent
CHROMADB_PATH=./chroma_db_multilingual
CHROMADB_COLLECTION_NAME=multilingual-docs
VECTOR_SEARCH_DIMENSIONS=1024

# Vision: Disabled
ENABLE_MEDIA_DESCRIPTION=false

# ============================================================================
# SCENARIO 4: OpenAI Alternative (Markitdown + OpenAI + ChromaDB)
# ============================================================================
# USE CASE: Prefer OpenAI over Azure, don't need Azure infrastructure
# COST: $ (OpenAI embeddings, ~$0.13 per 1M tokens)
# PROS: Simple OpenAI integration, no Azure account needed
# CONS: Requires OpenAI API key

# Extraction: Markitdown (free)
EXTRACTION_MODE=markitdown
INPUT_MODE=local

# Embeddings: OpenAI (paid)
EMBEDDINGS_MODE=openai
OPENAI_API_KEY=your_openai_api_key
OPENAI_EMBEDDING_MODEL=text-embedding-3-large  # or text-embedding-3-small
EMBEDDING_DIMENSIONS=3072  # 3072 for large, 1536 for small

# Vector Store: ChromaDB (free)
VECTOR_STORE=chromadb
CHROMADB_MODE=persistent
CHROMADB_PATH=./chroma_db_openai
CHROMADB_COLLECTION_NAME=openai-docs
VECTOR_SEARCH_DIMENSIONS=3072

# Vision: Optional with OpenAI GPT-4o (not Azure)
ENABLE_MEDIA_DESCRIPTION=false
# If enabling vision with OpenAI:
# ENABLE_MEDIA_DESCRIPTION=true
# OPENAI_VISION_MODEL=gpt-4o
# VISION_DETAIL_LEVEL=high

# ============================================================================
# SCENARIO 5: Maximum Quality (Azure DI + Azure OpenAI + Azure Search + Vision)
# ============================================================================
# USE CASE: Enterprise production, quality > cost
# COST: $$$ (All Azure services, ~$10-15 per 1000 pages)
# PROS: Best quality across all components, enterprise scale
# CONS: Highest cost, requires Azure subscriptions

# Extraction: Azure DI (best OCR)
EXTRACTION_MODE=azure_di
INPUT_MODE=local
AZURE_DI_ENDPOINT=https://your-di.cognitiveservices.azure.com/
AZURE_DI_KEY=your_di_key
AZURE_DI_MODEL=prebuilt-layout
AZURE_DI_EXTRACT_TABLES=true

# Embeddings: Azure OpenAI (high quality)
EMBEDDINGS_MODE=azure_openai
AZURE_OPENAI_ENDPOINT=https://your-openai.openai.azure.com/
AZURE_OPENAI_API_KEY=your_azure_openai_key
AZURE_OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-large
AZURE_OPENAI_EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_DIMENSIONS=3072

# Vector Store: Azure Search (enterprise scale)
VECTOR_STORE=azure_search
AZURE_SEARCH_ENDPOINT=https://your-search.search.windows.net
AZURE_SEARCH_KEY=your_search_key
AZURE_SEARCH_INDEX_NAME=enterprise-docs
AZURE_SEARCH_API_VERSION=2024-07-01
VECTOR_SEARCH_DIMENSIONS=3072
ENABLE_SEMANTIC_RANKING=true

# Vision: GPT-4o (describe all media)
ENABLE_MEDIA_DESCRIPTION=true
AZURE_OPENAI_VISION_DEPLOYMENT=gpt-4o
AZURE_OPENAI_VISION_MODEL=gpt-4o
VISION_DESCRIBE_CHARTS=true
VISION_DESCRIBE_DIAGRAMS=true
VISION_DESCRIBE_PHOTOS=true
VISION_DETAIL_LEVEL=high

# ============================================================================
# SCENARIO 6: Development/Testing (All Free, Local)
# ============================================================================
# USE CASE: Local development, testing, no costs
# COST: FREE
# PROS: Zero cost, fast iteration, no API limits
# CONS: Lower quality, not for production

# Extraction: Markitdown (free)
EXTRACTION_MODE=markitdown
INPUT_MODE=local

# Embeddings: Hugging Face (free, local)
EMBEDDINGS_MODE=huggingface
HUGGINGFACE_MODEL=all-MiniLM-L6-v2  # Fast, small model
EMBEDDING_DIMENSIONS=384
HUGGINGFACE_DEVICE=cpu

# Vector Store: ChromaDB in-memory (fast, temporary)
VECTOR_STORE=chromadb
CHROMADB_MODE=memory  # Lost on restart, perfect for testing
CHROMADB_COLLECTION_NAME=dev-test

# Vision: Disabled
ENABLE_MEDIA_DESCRIPTION=false

# ============================================================================
# SCENARIO 7: Secure/Private (Azure DI + Local Embeddings + ChromaDB)
# ============================================================================
# USE CASE: Need OCR quality but vectors must stay local (HIPAA/GDPR)
# COST: $$ (Azure DI only)
# PROS: Good extraction, data privacy, no cloud storage of vectors
# CONS: Azure DI still sees documents (uses their API)

# Extraction: Azure DI (documents sent to Azure for processing)
EXTRACTION_MODE=azure_di
INPUT_MODE=local
AZURE_DI_ENDPOINT=https://your-di.cognitiveservices.azure.com/
AZURE_DI_KEY=your_di_key
AZURE_DI_MODEL=prebuilt-layout

# Embeddings: Hugging Face (local, vectors never leave your machine)
EMBEDDINGS_MODE=huggingface
HUGGINGFACE_MODEL=all-mpnet-base-v2
EMBEDDING_DIMENSIONS=768

# Vector Store: ChromaDB (local, vectors never leave your machine)
VECTOR_STORE=chromadb
CHROMADB_MODE=persistent
CHROMADB_PATH=./chroma_db_private
CHROMADB_COLLECTION_NAME=private-docs
VECTOR_SEARCH_DIMENSIONS=768

# Vision: Disabled (maximum privacy)
ENABLE_MEDIA_DESCRIPTION=false

# NOTE: For MAXIMUM privacy, use EXTRACTION_MODE=markitdown instead
# This keeps ALL data local (no Azure DI API calls)

# ============================================================================
# SCENARIO 8: Distributed ChromaDB (Any Extraction/Embeddings + Remote ChromaDB)
# ============================================================================
# USE CASE: Multiple processing nodes, shared vector store
# COST: Depends on extraction/embeddings choices
# PROS: Scalable ChromaDB, multiple writers/readers
# CONS: Requires ChromaDB server setup

# Extraction: Your choice (example: Markitdown)
EXTRACTION_MODE=markitdown
INPUT_MODE=local

# Embeddings: Your choice (example: Hugging Face)
EMBEDDINGS_MODE=huggingface
HUGGINGFACE_MODEL=all-mpnet-base-v2
EMBEDDING_DIMENSIONS=768

# Vector Store: ChromaDB HTTP (remote server)
VECTOR_STORE=chromadb
CHROMADB_MODE=http
CHROMADB_HOST=chromadb-server.example.com  # Or localhost for local server
CHROMADB_PORT=8000
CHROMADB_SSL=true  # Use HTTPS
CHROMADB_HEADERS={"Authorization": "Bearer your_token"}  # Optional auth
CHROMADB_COLLECTION_NAME=distributed-docs
VECTOR_SEARCH_DIMENSIONS=768

# Vision: Your choice
ENABLE_MEDIA_DESCRIPTION=false

# To run ChromaDB server:
# docker run -p 8000:8000 chromadb/chroma
# Or see docker-compose.yml

# ============================================================================
# COMMON CONFIGURATIONS (Add to Any Scenario)
# ============================================================================

# Chunking (adjust based on your needs)
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
CHUNKING_STRATEGY=recursive
MIN_CHUNK_SIZE=100
MAX_CHUNK_SIZE=2000

# Processing
BATCH_SIZE=10
EMBEDDING_BATCH_SIZE=100
UPLOAD_BATCH_SIZE=50
MAX_RETRIES=3
RETRY_DELAY=1

# Artifacts
ARTIFACTS_DIR=./artifacts
KEEP_ARTIFACTS=true
ARTIFACTS_MAX_AGE_DAYS=30
SAVE_EXTRACTED_TEXT=true
SAVE_CHUNKS=true
SAVE_EMBEDDINGS=false

# Logging
LOG_LEVEL=INFO
LOG_FILE=./logs/ingestor.log
VERBOSE=true
SHOW_PROGRESS=true

# Metadata
EXTRACT_FILE_METADATA=true
EXTRACT_TEXT_STATS=true
VALIDATE_BEFORE_PROCESSING=true

# ============================================================================
# DECISION MATRIX: Choose Your Scenario
# ============================================================================
#
# Priority: Cost Optimization
#   → Scenario 6 (all free) or Scenario 1 (free extraction/embeddings)
#
# Priority: Quality
#   → Scenario 5 (all Azure) or Scenario 2 (Azure DI + local)
#
# Priority: Multilingual
#   → Scenario 3 (Cohere) or use Hugging Face multilingual models
#
# Priority: Privacy/Security
#   → Scenario 7 (local vectors) or Scenario 6 (fully local)
#
# Priority: Scalability
#   → Scenario 5 (Azure Search) or Scenario 8 (ChromaDB server)
#
# Priority: Simplicity
#   → Scenario 4 (OpenAI) or Scenario 6 (all local)
#
# ============================================================================
# COMPONENT COMPATIBILITY MATRIX
# ============================================================================
#
# Extraction Options:
#   - markitdown: ✅ Free, multi-format, moderate quality
#   - azure_di: ✅ Paid, best OCR, table extraction
#
# Embeddings Options:
#   - huggingface: ✅ Free, local, good quality, many models
#   - azure_openai: ✅ Paid, high quality, 1536-3072 dims
#   - cohere: ✅ Paid, multilingual (100+ langs), 1024 dims
#   - openai: ✅ Paid, high quality, 1536-3072 dims
#
# Vector Store Options:
#   - chromadb: ✅ Free, local/remote, good for <10M vectors
#   - azure_search: ✅ Paid, enterprise scale, semantic ranking
#
# Vision Options:
#   - None: ✅ Free, skip images
#   - GPT-4o (Azure): ✅ Paid, best quality descriptions
#   - GPT-4o (OpenAI): ✅ Paid, best quality descriptions
#   - Local models: ⚠️ Experimental, lower quality, resource-intensive
#
# All combinations are supported! Mix and match based on your needs.
#
# ============================================================================
# EXAMPLE: Build Your Own
# ============================================================================
# 1. Choose extraction:
#    EXTRACTION_MODE=markitdown  # or azure_di
#
# 2. Choose embeddings:
#    EMBEDDINGS_MODE=huggingface  # or azure_openai, cohere, openai
#
# 3. Choose vector store:
#    VECTOR_STORE=chromadb  # or azure_search
#
# 4. Configure chosen services (see examples above)
#
# 5. Validate:
#    python -m ingestor.scenario_validator
#
# 6. Test:
#    python -m ingestor.cli --pdf ./test.pdf --validate
#
# ============================================================================
# MIGRATION GUIDE
# ============================================================================
#
# From Azure Full Stack to Hybrid:
#   1. Keep EXTRACTION_MODE=azure_di (quality)
#   2. Change EMBEDDINGS_MODE=huggingface (save $)
#   3. Change VECTOR_STORE=chromadb (save $$)
#   4. Update dimensions: EMBEDDING_DIMENSIONS=768
#
# From Fully Local to Hybrid:
#   1. Keep EMBEDDINGS_MODE=huggingface (free)
#   2. Change EXTRACTION_MODE=azure_di (quality+)
#   3. Keep VECTOR_STORE=chromadb (free)
#   4. Add Azure DI credentials
#
# From OpenAI to Azure:
#   1. Change EMBEDDINGS_MODE=azure_openai
#   2. Update credentials (Azure OpenAI)
#   3. Keep dimensions same (3072 or 1536)
#
# ============================================================================
# COST COMPARISON (per 1000 pages)
# ============================================================================
# Scenario 1 (Cost-Optimized): ~$8/day (Azure Search only)
# Scenario 2 (Quality + Savings): ~$1.50 (Azure DI only)
# Scenario 3 (Multilingual): ~$0.50 (Cohere only)
# Scenario 4 (OpenAI): ~$0.50 (OpenAI only)
# Scenario 5 (Maximum Quality): ~$10-15 (full Azure stack)
# Scenario 6 (Dev/Test): $0 (all free)
# Scenario 7 (Secure): ~$1.50 (Azure DI only)
# Scenario 8 (Distributed): Depends on choices + ChromaDB hosting
# ============================================================================

# ============================================================================
# TROUBLESHOOTING
# ============================================================================
# Issue: "Provider not available"
# Solution: Check EMBEDDINGS_MODE and VECTOR_STORE match installed packages
#   pip install chromadb  # for ChromaDB
#   pip install sentence-transformers  # for Hugging Face
#   pip install cohere  # for Cohere
#   pip install openai  # for OpenAI
#
# Issue: "Dimension mismatch"
# Solution: Ensure EMBEDDING_DIMENSIONS matches model output:
#   - all-MiniLM-L6-v2: 384
#   - all-mpnet-base-v2: 768
#   - gte-large: 1024
#   - text-embedding-3-small: 1536
#   - text-embedding-3-large: 3072
#
# Issue: "Authentication failed"
# Solution: Verify API keys for your chosen services
#
# For scenario-specific help:
#   python -m ingestor.scenario_validator [scenario_name]
# ============================================================================
