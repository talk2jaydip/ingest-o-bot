# ==========================================
# Azure AI Search + Hugging Face - HYBRID CLOUD/LOCAL
# ==========================================
#
# This configuration uses Azure AI Search for enterprise storage
# with local Hugging Face embeddings to optimize costs.
#
# SETUP:
# ☐ 1. Install: pip install -r requirements-embeddings.txt
# ☐ 2. Create Azure AI Search service and Storage account
# ☐ 3. Copy this file to .env and fill in Azure credentials
# ☐ 4. Run: ingestor --glob "documents/*.pdf"
#
# BENEFITS:
# ✅ Azure Search for enterprise search features
# ✅ Local embeddings = zero embedding API costs
# ✅ GPU-accelerated embeddings for speed
# ✅ Best multilingual quality (multilingual-e5-large)
# ==========================================

# ==========================================
# Vector Store: Azure AI Search
# ==========================================
VECTOR_STORE_MODE=azure_search
AZURE_SEARCH_SERVICE=your-search-service
AZURE_SEARCH_INDEX=hybrid-documents
AZURE_SEARCH_KEY=your-search-admin-key-here

# ==========================================
# Embeddings: Hugging Face (Local GPU-Accelerated)
# ==========================================
EMBEDDINGS_MODE=huggingface

# Best multilingual model
HUGGINGFACE_MODEL_NAME=intfloat/multilingual-e5-large
HUGGINGFACE_DEVICE=cuda  # Use GPU for 5-10x speedup
HUGGINGFACE_BATCH_SIZE=64  # Larger batch for GPU
HUGGINGFACE_NORMALIZE=true

# For CPU only:
# HUGGINGFACE_DEVICE=cpu
# HUGGINGFACE_BATCH_SIZE=32

# For Apple Silicon (M1/M2/M3):
# HUGGINGFACE_DEVICE=mps
# HUGGINGFACE_BATCH_SIZE=48

# IMPORTANT: Disable integrated vectorization (we're using Hugging Face, not Azure OpenAI)
AZURE_USE_INTEGRATED_VECTORIZATION=false

# ==========================================
# Azure Storage Account
# ==========================================
AZURE_STORAGE_ACCOUNT=your-storage-account
AZURE_STORAGE_ACCOUNT_KEY=your-storage-account-key-here

# ==========================================
# Input: Blob Storage
# ==========================================
INPUT_MODE=blob
AZURE_STORAGE_CONTAINER=documents

# ==========================================
# Artifacts: Blob Storage
# ==========================================
ARTIFACTS_MODE=blob
AZURE_ARTIFACTS_CONTAINER=artifacts

# ==========================================
# Document Intelligence
# ==========================================
AZURE_DOC_INT_ENDPOINT=https://your-docint.cognitiveservices.azure.com/
AZURE_DOC_INT_KEY=your-document-intelligence-key-here
AZURE_DI_MAX_CONCURRENCY=3
AZURE_OFFICE_EXTRACTOR_MODE=hybrid

# ==========================================
# Azure OpenAI (Optional - for media descriptions only)
# ==========================================
# AZURE_OPENAI_ENDPOINT=https://your-openai.openai.azure.com/
# AZURE_OPENAI_KEY=your-openai-key-here
# AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o-mini

# ==========================================
# Media Description
# ==========================================
AZURE_MEDIA_DESCRIBER=disabled  # or gpt4o (requires Azure OpenAI)

# ==========================================
# Chunking Settings (Dynamic Adjustment)
# ==========================================
# The pipeline automatically adjusts chunking limits based on the
# embedding model's max_seq_length to prevent truncation.
#
# For multilingual-e5-large (max_seq_length = 512):
#   Safe limit = 512 * (1 - 0.15 - 0.10) = 384 tokens
#   (15% safety buffer + 10% overlap allowance)
#
# Generic parameter names (recommended):
CHUNKING_MAX_CHARS=2000
CHUNKING_MAX_TOKENS=500
CHUNKING_OVERLAP_PERCENT=10

# Azure-prefixed names also work (backward compatibility):
# AZURE_CHUNKING_MAX_CHARS=2000
# AZURE_CHUNKING_MAX_TOKENS=500
# AZURE_CHUNKING_OVERLAP_PERCENT=10

# Note: Hugging Face models report max_seq_length automatically
# Pipeline will adjust chunking if limits exceed model capacity

# ==========================================
# Table Rendering
# ==========================================
AZURE_TABLE_RENDER=markdown

# ==========================================
# Performance Settings
# ==========================================
AZURE_CHUNKING_MAX_WORKERS=4
AZURE_CHUNKING_MAX_BATCH_UPLOAD_CONCURRENCY=5

# ==========================================
# Logging
# ==========================================
LOG_LEVEL=INFO
LOG_FILE_LEVEL=DEBUG

# ==========================================
# COST SAVINGS:
# Before (Azure OpenAI embeddings): $0.10 per 1M tokens
# After (Hugging Face local): $0 (only GPU compute)
#
# For 10M tokens/month:
# - Before: $1,000 in embedding costs
# - After: $0 in embedding costs
# - Savings: $1,000/month
#
# Total cost: ~$250-500/month (Azure Search + Storage only)
#
# NOTES:
# - Dynamic chunking automatically adjusts based on model's max_seq_length
# - multilingual-e5-large has 512 token limit (safe limit: 384 tokens)
# - You'll see warning messages if chunking limits are reduced
# - GPU acceleration recommended for best performance
# ==========================================
