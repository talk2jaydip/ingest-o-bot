# ============================================================================
# SCENARIO: Fully Offline with Optional Cloud Vision
# ============================================================================
# USE CASE: Private/air-gapped processing with optional cloud vision for images
# COST: FREE (or $ if using GPT-4o vision)
# SETUP TIME: 10 minutes
# PROS:
#   - 100% FREE for base functionality
#   - No API keys required (except optional vision)
#   - Works without internet (except vision)
#   - Complete data privacy
#   - No rate limits or quotas
#   - Runs on any machine with Python
# CONS:
#   - Lower quality text extraction than Azure DI
#   - Smaller embedding dimensions (384-1024 vs 1536-3072)
#   - Requires local compute resources
#   - No OCR for scanned PDFs (unless using vision)
# ============================================================================

# ============================================================================
# EXTRACTION MODE - Local Markitdown (100% Offline)
# ============================================================================
# Use Markitdown for text extraction - supports many formats natively
# Formats: PDF, DOCX, PPTX, XLSX, HTML, images (with vision), audio, video

EXTRACTION_MODE=markitdown
INPUT_MODE=local

# Markitdown configuration
MARKITDOWN_EXTRACT_IMAGES=true    # Extract images from documents
MARKITDOWN_EXTRACT_TABLES=true    # Extract tables as markdown
MARKITDOWN_PRESERVE_LAYOUT=true   # Try to preserve document layout

# Image handling in Markitdown
MARKITDOWN_IMAGE_MODE=describe    # describe | skip | placeholder
# - describe: Use vision model to describe images (requires ENABLE_MEDIA_DESCRIPTION=true)
# - skip: Ignore images completely (fully offline)
# - placeholder: Add [Image] placeholder text (offline)

# For fully offline: use skip or placeholder
# For hybrid: use describe with GPT-4o

# ============================================================================
# EMBEDDINGS - Local Hugging Face (100% Offline)
# ============================================================================
# Use local sentence-transformers models - no API calls, no costs!

EMBEDDINGS_MODE=huggingface

# Model selection (downloaded once, cached locally)
# Small & Fast (384 dims): all-MiniLM-L6-v2 (22MB)
# Balanced (768 dims): all-mpnet-base-v2 (420MB)
# Large & Accurate (1024 dims): gte-large (670MB)
HUGGINGFACE_MODEL=all-mpnet-base-v2

# Embedding dimensions (must match model)
EMBEDDING_DIMENSIONS=768  # 384 for MiniLM, 768 for mpnet, 1024 for gte-large

# Local model cache directory
TRANSFORMERS_CACHE=./models/huggingface
HF_HOME=./models/huggingface

# Device selection
# - 'cpu': Always available, slower
# - 'cuda': NVIDIA GPU, much faster (requires torch with CUDA)
# - 'mps': Apple Silicon GPU (M1/M2/M3), faster on Mac
# - 'auto': Automatically detect best device
HUGGINGFACE_DEVICE=auto

# Batch processing for embeddings
HUGGINGFACE_BATCH_SIZE=32         # Larger = faster but more memory
HUGGINGFACE_MAX_LENGTH=512        # Token limit per chunk (model dependent)
HUGGINGFACE_NORMALIZE=true        # Normalize embeddings to unit length

# ============================================================================
# VECTOR STORE - Local ChromaDB (100% Offline)
# ============================================================================
VECTOR_STORE=chromadb

# Storage mode
CHROMADB_MODE=persistent          # persistent | memory | http
CHROMADB_PATH=./chroma_db_offline
CHROMADB_COLLECTION_NAME=ingestor-docs-offline

# ChromaDB settings
CHROMADB_DISTANCE_METRIC=cosine   # cosine | l2 | ip
VECTOR_SEARCH_DIMENSIONS=768      # Must match EMBEDDING_DIMENSIONS

# Search parameters
DEFAULT_TOP_K=10
SIMILARITY_THRESHOLD=0.7

# ============================================================================
# OPTIONAL: GPT-4o Vision for Media Description (Cloud Hybrid)
# ============================================================================
# THIS IS THE ONLY CLOUD COMPONENT - Set to false for 100% offline
# If enabled, will use Azure OpenAI GPT-4o to describe images/charts
# Trade-off: Better image understanding vs internet requirement + cost

ENABLE_MEDIA_DESCRIPTION=false    # Set to true to enable vision

# If ENABLE_MEDIA_DESCRIPTION=true, configure these:
# (Otherwise, they are ignored and system stays offline)

# Azure OpenAI GPT-4o for vision
AZURE_OPENAI_ENDPOINT=https://your-openai-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your_azure_openai_api_key_here
AZURE_OPENAI_API_VERSION=2024-02-15-preview

AZURE_OPENAI_VISION_DEPLOYMENT=gpt-4o
AZURE_OPENAI_VISION_MODEL=gpt-4o

# Vision settings
VISION_MAX_IMAGES_PER_DOC=50
VISION_MIN_IMAGE_SIZE=100
VISION_DETAIL_LEVEL=high          # low | high (high = better but $$$)

VISION_DESCRIBE_CHARTS=true
VISION_DESCRIBE_DIAGRAMS=true
VISION_DESCRIBE_PHOTOS=true
VISION_DESCRIBE_TABLES=false

# Vision cost optimization
VISION_BATCH_SIZE=5               # Images to process in one call
VISION_CACHE_DESCRIPTIONS=true    # Cache image descriptions to avoid reprocessing
VISION_CACHE_DIR=./cache/vision

# ============================================================================
# ALTERNATIVE: Fully Offline Vision with Local Models (Experimental)
# ============================================================================
# Use local vision models instead of GPT-4o (requires more setup)
# Options:
#   - BLIP-2 (image captioning)
#   - LLaVA (visual question answering)
#   - moondream (lightweight vision-language model)

# Uncomment to use local vision:
# ENABLE_LOCAL_VISION=true
# LOCAL_VISION_MODEL=Salesforce/blip2-opt-2.7b  # or llava-hf/llava-1.5-7b-hf
# LOCAL_VISION_DEVICE=auto
# LOCAL_VISION_MAX_LENGTH=100

# Note: Local vision models require significant resources:
#   - BLIP-2: ~11GB disk, ~6GB RAM
#   - LLaVA: ~28GB disk, ~16GB RAM
#   - moondream: ~8GB disk, ~4GB RAM

# ============================================================================
# CHUNKING CONFIGURATION
# ============================================================================
CHUNK_SIZE=800                    # Smaller for local models (they have token limits)
CHUNK_OVERLAP=150
CHUNKING_STRATEGY=recursive
MIN_CHUNK_SIZE=100
MAX_CHUNK_SIZE=1500

# ============================================================================
# PROCESSING CONFIGURATION
# ============================================================================
BATCH_SIZE=5                      # Lower for CPU processing
EMBEDDING_BATCH_SIZE=32           # Adjust based on available RAM
UPLOAD_BATCH_SIZE=50

# No rate limits needed (local processing)
MAX_RETRIES=3
RETRY_DELAY=1

# ============================================================================
# ARTIFACT MANAGEMENT
# ============================================================================
ARTIFACTS_DIR=./artifacts
KEEP_ARTIFACTS=true
ARTIFACTS_MAX_AGE_DAYS=30
SAVE_EXTRACTED_TEXT=true
SAVE_CHUNKS=true
SAVE_EMBEDDINGS=false

# ============================================================================
# LOGGING & OUTPUT
# ============================================================================
LOG_LEVEL=INFO
LOG_FILE=./logs/ingestor-offline.log
LOG_FORMAT=detailed
VERBOSE=true
NO_COLORS=false
SHOW_PROGRESS=true

# ============================================================================
# METADATA
# ============================================================================
DOCUMENT_SOURCE=offline_local
ENVIRONMENT=development
PROCESSING_VERSION=1.0

EXTRACT_FILE_METADATA=true
EXTRACT_TEXT_STATS=true

VALIDATE_BEFORE_PROCESSING=true

# ============================================================================
# OFFLINE MODE GUARANTEES
# ============================================================================
# When properly configured, this setup NEVER makes network calls except:
#   1. First-time model download (Hugging Face models, one-time only)
#   2. Optional GPT-4o vision (if ENABLE_MEDIA_DESCRIPTION=true)
#
# For 100% offline after initial setup:
#   1. Download models once with internet
#   2. Disconnect from internet
#   3. Set ENABLE_MEDIA_DESCRIPTION=false
#   4. Set MARKITDOWN_IMAGE_MODE=skip or placeholder
#   5. Process documents completely offline!
# ============================================================================

# ============================================================================
# EXAMPLE USAGE - 100% Offline
# ============================================================================
# 1. Copy this file:
#    cp envs/.env.offline-with-vision.example .env
#
# 2. Install offline dependencies:
#    pip install chromadb sentence-transformers torch markitdown
#
# 3. Download models (requires internet, ONE TIME ONLY):
#    python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-mpnet-base-v2')"
#
# 4. Verify offline setup:
#    python -m ingestor.cli --validate
#
# 5. DISCONNECT FROM INTERNET (optional, to verify offline capability)
#
# 6. Process documents (100% offline):
#    python -m ingestor.cli --pdf ./document.pdf
#    python -m ingestor.cli --glob "docs/**/*.pdf"
#
# 7. Query documents (100% offline):
#    python -m ingestor.cli --query "What is the main topic?"
#
# ============================================================================
# EXAMPLE USAGE - Hybrid with Vision
# ============================================================================
# 1. Same as above, but:
#    ENABLE_MEDIA_DESCRIPTION=true
#    MARKITDOWN_IMAGE_MODE=describe
#
# 2. Configure Azure OpenAI GPT-4o credentials
#
# 3. Process documents:
#    python -m ingestor.cli --pdf ./document-with-charts.pdf
#
# 4. Images/charts will be described by GPT-4o, everything else offline
#
# ============================================================================
# MODEL COMPARISON (Hugging Face)
# ============================================================================
# | Model              | Dims | Size  | Speed | Quality | Use Case          |
# |--------------------|------|-------|-------|---------|-------------------|
# | all-MiniLM-L6-v2   | 384  | 22MB  | Fast  | Good    | Quick testing     |
# | all-mpnet-base-v2  | 768  | 420MB | Med   | Better  | Recommended       |
# | gte-large          | 1024 | 670MB | Slow  | Best    | High quality      |
# | multilingual-e5    | 768  | 560MB | Med   | Good    | 100+ languages    |
#
# Recommended: all-mpnet-base-v2 (best balance of quality, speed, size)
# ============================================================================

# ============================================================================
# PERFORMANCE OPTIMIZATION - Offline Mode
# ============================================================================
# For faster processing on CPU:
#   - Use all-MiniLM-L6-v2 (smaller, faster)
#   - Increase HUGGINGFACE_BATCH_SIZE to 64
#   - Increase CHUNK_SIZE to 1000 (fewer chunks)
#   - Reduce BATCH_SIZE to 3 (less memory)
#
# For GPU acceleration (CUDA/MPS):
#   - Install torch with GPU support: pip install torch --index-url https://download.pytorch.org/whl/cu118
#   - Set HUGGINGFACE_DEVICE=cuda or HUGGINGFACE_DEVICE=mps
#   - Increase HUGGINGFACE_BATCH_SIZE to 128
#   - Increase BATCH_SIZE to 10
#
# For maximum quality:
#   - Use gte-large model (1024 dims)
#   - Reduce CHUNK_SIZE to 600 (more granular)
#   - Enable vision: ENABLE_MEDIA_DESCRIPTION=true
# ============================================================================

# ============================================================================
# COST ESTIMATION
# ============================================================================
# Base (100% offline): $0 forever!
#   - No API costs
#   - No subscription fees
#   - Only compute costs (electricity)
#
# With GPT-4o vision: ~$2-5 per 1000 pages (depends on image count)
#   - Input: $2.50 per 1M tokens
#   - Output: $10 per 1M tokens
#   - Typical doc: 10-50 images, ~$0.002-0.005 per image
#
# Hardware recommendations:
#   - Minimum: 8GB RAM, 2GB disk space
#   - Recommended: 16GB RAM, 5GB disk, GPU
#   - Optimal: 32GB RAM, 10GB disk, modern GPU
# ============================================================================

# ============================================================================
# TROUBLESHOOTING - Offline Mode
# ============================================================================
# Issue: "Could not import sentence_transformers"
# Solution: pip install sentence-transformers torch
#
# Issue: "Model not found" or "Connection error"
# Solution: Download model manually:
#   python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-mpnet-base-v2')"
#
# Issue: "Out of memory"
# Solution:
#   - Use smaller model: all-MiniLM-L6-v2
#   - Reduce HUGGINGFACE_BATCH_SIZE to 16
#   - Reduce BATCH_SIZE to 3
#   - Increase CHUNK_SIZE to 1200
#
# Issue: "Slow processing"
# Solution:
#   - Use GPU: HUGGINGFACE_DEVICE=cuda or mps
#   - Use smaller model: all-MiniLM-L6-v2
#   - Increase batch sizes
#
# Issue: "Markitdown can't extract text from scanned PDF"
# Solution:
#   - Scanned PDFs require OCR
#   - Enable vision: ENABLE_MEDIA_DESCRIPTION=true (treats pages as images)
#   - Or use Azure DI for OCR: EXTRACTION_MODE=azure_di
#
# Issue: "Images not being described"
# Solution:
#   - Check ENABLE_MEDIA_DESCRIPTION=true
#   - Check MARKITDOWN_IMAGE_MODE=describe
#   - Verify Azure OpenAI credentials if using GPT-4o
#
# For more help: python -m ingestor.scenario_validator offline
# ============================================================================

# ============================================================================
# SECURITY & PRIVACY
# ============================================================================
# This configuration is suitable for:
#   ✅ Sensitive documents (medical, legal, financial)
#   ✅ Classified information (with vision disabled)
#   ✅ Air-gapped networks (with vision disabled)
#   ✅ GDPR/HIPAA compliance (data never leaves your machine)
#   ✅ Development/testing without cloud accounts
#
# Important notes:
#   - Set ENABLE_MEDIA_DESCRIPTION=false for maximum privacy
#   - Set MARKITDOWN_IMAGE_MODE=skip to avoid any image processing
#   - All data stays on your local machine
#   - No telemetry or analytics
#   - No network calls (except initial model download)
# ============================================================================
