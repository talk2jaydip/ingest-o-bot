# ============================================================================
# SCENARIO: Local Development Environment
# ============================================================================
#
# DESCRIPTION:
# Fast local development setup using ChromaDB for vector storage and
# lightweight Hugging Face embeddings. All data stays local, perfect for
# rapid iteration and testing without cloud dependencies or costs.
#
# USE CASE:
# - Local development and testing
# - CI/CD pipelines
# - Prototyping new features
# - Learning the pipeline
# - No cloud access required
#
# COST ESTIMATE: $0/month (100% free)
#
# REQUIREMENTS:
# - Python 3.10+
# - pip install chromadb sentence-transformers torch
# - No Azure subscription needed
# - No internet connection needed (after initial model download)
#
# ============================================================================

# ============================================================================
# VECTOR STORE: ChromaDB (In-Memory for Fast Iteration)
# ============================================================================
VECTOR_STORE_MODE=chromadb
CHROMADB_COLLECTION_NAME=dev-documents

# Option 1: In-Memory (fastest, data lost on restart)
# No CHROMADB_PERSIST_DIR means in-memory mode
# BEST FOR: Quick testing, CI/CD, temporary experiments

# Option 2: Persistent (data survives restarts)
# Uncomment to enable persistence:
# CHROMADB_PERSIST_DIR=./chroma_dev
# BEST FOR: Iterative development, keeping test data

CHROMADB_BATCH_SIZE=100

# ============================================================================
# EMBEDDINGS: Hugging Face (Fast Lightweight Model)
# ============================================================================
EMBEDDINGS_MODE=huggingface

# RECOMMENDED: All-MiniLM-L6-v2 (fastest, smallest)
HUGGINGFACE_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
# - 384 dimensions, 256 max tokens
# - ~90MB model size (quick download)
# - Fast CPU inference (~1-2 seconds per page)
# - Perfect for development

# Alternative: All-MPNet-Base-v2 (better quality, still fast)
# HUGGINGFACE_MODEL_NAME=sentence-transformers/all-mpnet-base-v2
# - 768 dimensions, 384 max tokens
# - ~420MB model size
# - Good balance of speed and quality

# Device Configuration:
HUGGINGFACE_DEVICE=cpu  # Most devs don't have GPU
HUGGINGFACE_BATCH_SIZE=16  # Conservative for laptop/desktop
HUGGINGFACE_NORMALIZE=true

# ============================================================================
# INPUT: Local Files
# ============================================================================
AZURE_INPUT_MODE=local

# Development data patterns:
AZURE_LOCAL_GLOB=data/test/*.pdf  # Test files only

# Alternative patterns for different workflows:
# AZURE_LOCAL_GLOB=data/**/*.pdf  # All PDFs recursively
# AZURE_LOCAL_GLOB=data/sample.pdf  # Single test file
# AZURE_LOCAL_GLOB=data/**/*.{pdf,docx,txt}  # Multiple formats

# ============================================================================
# ARTIFACTS: Local Storage
# ============================================================================
AZURE_ARTIFACTS_DIR=./artifacts_dev

# Tip: Use different artifact dirs for different tests:
# AZURE_ARTIFACTS_DIR=./artifacts_test1
# AZURE_ARTIFACTS_DIR=./artifacts_experiment_new_chunking

# ============================================================================
# DOCUMENT PROCESSING: Lightweight
# ============================================================================

# Office Extractor: MarkItDown (pure Python, no Azure)
AZURE_OFFICE_EXTRACTOR_MODE=markitdown

# Optional: LibreOffice for DOC files (if installed)
# AZURE_OFFICE_LIBREOFFICE_PATH=/usr/bin/soffice  # Linux/Mac
# AZURE_OFFICE_LIBREOFFICE_PATH=C:\Program Files\LibreOffice\program\soffice.exe  # Windows

# Media Description: Disabled (no AI costs)
AZURE_MEDIA_DESCRIBER=disabled

# Integrated Vectorization: Disabled (local embeddings)
AZURE_USE_INTEGRATED_VECTORIZATION=false

# ============================================================================
# CHUNKING: Conservative Limits for Fast Model
# ============================================================================
# For all-MiniLM-L6-v2 (max_seq_length = 256):
#   Safe limit = 256 * (1 - 0.15 - 0.10) = 192 tokens
CHUNKING_MAX_TOKENS=192  # Safe for 256-token model
CHUNKING_MAX_CHARS=1000
CHUNKING_OVERLAP_PERCENT=10

# ============================================================================
# PERFORMANCE: Development Tuning
# ============================================================================
AZURE_CHUNKING_MAX_WORKERS=2  # Don't overwhelm laptop
AZURE_CHUNKING_MAX_IMAGE_CONCURRENCY=4
AZURE_CHUNKING_MAX_BATCH_UPLOAD_CONCURRENCY=2

# ============================================================================
# TABLE RENDERING
# ============================================================================
AZURE_TABLE_RENDER=markdown  # Lightweight
AZURE_TABLE_SUMMARIES=false

# ============================================================================
# LOGGING: Verbose for Development
# ============================================================================
LOG_LEVEL=DEBUG  # Show everything
LOG_FILE_LEVEL=DEBUG
LOG_ARTIFACTS=true  # Save artifacts for inspection
LOG_USE_COLORS=true  # Pretty console output

# Optional: Save logs to file
# AZURE_LOG_TO_FILE=true
# AZURE_LOG_FILE=logs/dev_pipeline.log

# ============================================================================
# DUMMY AZURE CONFIG (Required by Schema)
# ============================================================================
# These dummy values are required but not used in local development
AZURE_SEARCH_SERVICE=dev-dummy
AZURE_SEARCH_INDEX=dev-index
AZURE_SEARCH_KEY=dummy-key
AZURE_STORAGE_ACCOUNT=devdummy
AZURE_STORAGE_ACCOUNT_KEY=dummykey123==
AZURE_DOC_INT_ENDPOINT=https://dev.cognitiveservices.azure.com/
AZURE_DOC_INT_KEY=dummy-doc-key
AZURE_OPENAI_ENDPOINT=https://dev.openai.azure.com/
AZURE_OPENAI_KEY=dummy-openai-key
AZURE_OPENAI_CHAT_DEPLOYMENT=dummy-chat
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=dummy-embedding

# ============================================================================
# DEVELOPMENT WORKFLOWS
# ============================================================================
#
# WORKFLOW 1: Quick Test (Single File)
# -------------------------------------
# 1. Place test.pdf in data/test/
# 2. AZURE_LOCAL_GLOB=data/test/test.pdf
# 3. Run: python -m ingestor.cli --env-file .env.dev
# 4. Check artifacts in ./artifacts_dev/
# 5. Query ChromaDB to verify results
#
# WORKFLOW 2: Iterative Development (Persistent DB)
# --------------------------------------------------
# 1. Uncomment CHROMADB_PERSIST_DIR=./chroma_dev
# 2. Process documents: python -m ingestor.cli
# 3. Make code changes
# 4. Re-run without re-processing all docs
# 5. Data persists between runs
#
# WORKFLOW 3: Clean Slate Testing
# --------------------------------
# 1. Use in-memory ChromaDB (no CHROMADB_PERSIST_DIR)
# 2. Each run starts fresh
# 3. Perfect for testing changes to chunking logic
# 4. No cleanup needed
#
# WORKFLOW 4: Feature Branch Testing
# -----------------------------------
# 1. Create branch-specific artifacts dir:
#    AZURE_ARTIFACTS_DIR=./artifacts_feature_xyz
# 2. Test new feature
# 3. Compare artifacts with main branch:
#    diff -r ./artifacts_main ./artifacts_feature_xyz
#
# WORKFLOW 5: CI/CD Integration
# ------------------------------
# 1. Use in-memory ChromaDB (fast, no cleanup)
# 2. Small test corpus: data/test/*.pdf
# 3. Assert on artifact contents
# 4. Validate chunking quality
# 5. No cloud credentials needed
#
# ============================================================================
# PERFORMANCE EXPECTATIONS
# ============================================================================
#
# Processing Speed (all-MiniLM-L6-v2 on CPU):
# - Single page: ~0.5-1 seconds
# - 10-page PDF: ~10-20 seconds
# - 100-page PDF: ~2-3 minutes
#
# Memory Usage:
# - Model: ~500MB RAM
# - ChromaDB in-memory: ~100-500MB (depends on corpus size)
# - Total: ~1GB RAM for small corpus
#
# Disk Usage:
# - Model cache: ~90MB (~/.cache/huggingface/)
# - Artifacts: ~1-10MB per document (depends on pages/images)
# - ChromaDB persistent: ~10-100MB (depends on corpus size)
#
# First Run:
# - Downloads model (~90MB) - requires internet
# - Subsequent runs: no internet needed
#
# ============================================================================
# TIPS & TRICKS
# ============================================================================
#
# Speed up testing:
# - Use in-memory ChromaDB
# - Process single test file
# - Reduce batch sizes if memory constrained
#
# Debug chunking:
# - Set LOG_LEVEL=DEBUG
# - Enable LOG_ARTIFACTS=true
# - Check artifacts_dev/chunks/ for chunk inspection
#
# Test different models:
# - Swap HUGGINGFACE_MODEL_NAME
# - Adjust CHUNKING_MAX_TOKENS accordingly
# - Compare artifact quality
#
# Isolate experiments:
# - Use unique CHROMADB_COLLECTION_NAME
# - Use unique AZURE_ARTIFACTS_DIR
# - Keep different test configurations
#
# ============================================================================
# UPGRADE PATH TO PRODUCTION
# ============================================================================
#
# When ready to move from dev to production:
#
# 1. Switch to Azure AI Search:
#    VECTOR_STORE_MODE=azure_search
#
# 2. Choose embedding strategy:
#    Option A: Keep Hugging Face (cost optimized)
#    Option B: Switch to Azure OpenAI (simpler)
#
# 3. Enable Azure Document Intelligence:
#    AZURE_OFFICE_EXTRACTOR_MODE=hybrid
#
# 4. Switch to blob storage:
#    AZURE_INPUT_MODE=blob
#    AZURE_ARTIFACTS_MODE=blob
#
# 5. Enable media descriptions (optional):
#    AZURE_MEDIA_DESCRIBER=gpt4o
#
# See .env.scenario-azure-openai-default.example for production config
#
# ============================================================================
