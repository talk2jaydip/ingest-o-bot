# ============================================================================
# SCENARIO: Multilingual Document Processing (100+ Languages)
# ============================================================================
#
# DESCRIPTION:
# Optimized configuration for processing documents in multiple languages
# using state-of-the-art multilingual embeddings. Supports 100+ languages
# with excellent cross-lingual semantic search capabilities.
#
# USE CASE:
# - International organizations with multilingual content
# - Cross-language search and retrieval
# - Global knowledge bases
# - Multilingual customer support
# - Academic research with international documents
#
# SUPPORTED LANGUAGES (100+):
# Arabic, Bengali, Bulgarian, Catalan, Chinese (Simplified/Traditional),
# Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French,
# German, Greek, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese,
# Korean, Latvian, Lithuanian, Norwegian, Polish, Portuguese, Romanian,
# Russian, Slovak, Slovenian, Spanish, Swedish, Thai, Turkish, Ukrainian,
# Vietnamese, and many more...
#
# COST ESTIMATE (ChromaDB + Hugging Face): $0/month (free)
# COST ESTIMATE (Azure Search + Hugging Face): $280-700/month
# COST ESTIMATE (Azure Search + Azure OpenAI): Not recommended for multilingual
#
# ============================================================================

# ============================================================================
# DEPLOYMENT OPTIONS: Choose One
# ============================================================================

# OPTION A: Fully Offline (ChromaDB + Hugging Face) - RECOMMENDED
# ----------------------------------------------------------------
# Best for: Cost optimization, data privacy, offline environments
# Trade-off: No enterprise search features

VECTOR_STORE_MODE=chromadb
CHROMADB_COLLECTION_NAME=multilingual-documents
CHROMADB_PERSIST_DIR=./chroma_multilingual
CHROMADB_BATCH_SIZE=100

# OPTION B: Hybrid Cloud/Local (Azure Search + Hugging Face)
# -----------------------------------------------------------
# Best for: Enterprise features + cost optimization
# Trade-off: Requires Azure subscription

# VECTOR_STORE_MODE=azure_search
# AZURE_SEARCH_SERVICE=your-search-service
# AZURE_SEARCH_KEY=your-search-admin-key-here
# AZURE_SEARCH_INDEX=multilingual-documents

# ============================================================================
# EMBEDDINGS: Multilingual Models (Choose One)
# ============================================================================
EMBEDDINGS_MODE=huggingface

# OPTION 1: Multilingual E5 Large (RECOMMENDED - Best Quality)
# -------------------------------------------------------------
HUGGINGFACE_MODEL_NAME=intfloat/multilingual-e5-large
# - 1024 dimensions
# - 512 max tokens
# - ~2.2GB model size
# - 100+ languages
# - State-of-the-art quality
# - Instruction-tuned for queries and documents
# - Best cross-lingual performance

# For queries, prefix with "query: "
# For documents, prefix with "passage: "
# (The pipeline handles this automatically)

# OPTION 2: BGE-M3 (Alternative - Unified Multi-Functionality)
# ------------------------------------------------------------
# HUGGINGFACE_MODEL_NAME=BAAI/bge-m3
# - 1024 dimensions
# - 8192 max tokens (great for longer documents!)
# - ~2.2GB model size
# - 100+ languages
# - Supports dense, sparse, and multi-vector retrieval
# - Excellent for Chinese and English

# OPTION 3: LaBSE (Lightweight Multilingual)
# -------------------------------------------
# HUGGINGFACE_MODEL_NAME=sentence-transformers/LaBSE
# - 768 dimensions
# - 512 max tokens
# - ~1.8GB model size
# - 109 languages
# - Good for resource-constrained environments

# OPTION 4: Paraphrase Multilingual MPNet (Balanced)
# ---------------------------------------------------
# HUGGINGFACE_MODEL_NAME=sentence-transformers/paraphrase-multilingual-mpnet-base-v2
# - 768 dimensions
# - 384 max tokens
# - ~1GB model size
# - 50+ languages
# - Good balance of size and quality

# Device Configuration:
# ----------------------------------------------------------------------------
# RECOMMENDED: GPU for large multilingual models
HUGGINGFACE_DEVICE=cuda  # NVIDIA GPU
HUGGINGFACE_BATCH_SIZE=64

# CPU (slower, but works):
# HUGGINGFACE_DEVICE=cpu
# HUGGINGFACE_BATCH_SIZE=32

# Apple Silicon:
# HUGGINGFACE_DEVICE=mps
# HUGGINGFACE_BATCH_SIZE=48

HUGGINGFACE_NORMALIZE=true

# ============================================================================
# INPUT/OUTPUT: Choose Based on Deployment Option
# ============================================================================

# For OPTION A (ChromaDB - Fully Offline):
# -----------------------------------------
AZURE_INPUT_MODE=local
AZURE_LOCAL_GLOB=data/multilingual/**/*.pdf  # All languages in subfolders
AZURE_ARTIFACTS_DIR=./artifacts_multilingual

# Example folder structure:
# data/multilingual/
#   ├── english/*.pdf
#   ├── spanish/*.pdf
#   ├── chinese/*.pdf
#   ├── arabic/*.pdf
#   └── mixed/*.pdf

# For OPTION B (Azure Search - Hybrid):
# --------------------------------------
# AZURE_INPUT_MODE=blob
# AZURE_STORAGE_ACCOUNT=your-storage-account
# AZURE_STORAGE_ACCOUNT_KEY=your-storage-account-key-here
# AZURE_STORAGE_CONTAINER=multilingual-documents
# AZURE_ARTIFACTS_MODE=blob
# AZURE_BLOB_CONTAINER_PREFIX=multilingual

# ============================================================================
# DOCUMENT PROCESSING
# ============================================================================

# Office Extractor:
AZURE_OFFICE_EXTRACTOR_MODE=markitdown  # Works for all languages

# Optional: Azure DI for better quality (supports many languages)
# AZURE_OFFICE_EXTRACTOR_MODE=hybrid
# AZURE_DOC_INT_ENDPOINT=https://your-docint.cognitiveservices.azure.com/
# AZURE_DOC_INT_KEY=your-document-intelligence-key-here

# Media Description: Disabled (or use GPT-4 for multilingual)
AZURE_MEDIA_DESCRIBER=disabled

# For multilingual image descriptions:
# AZURE_MEDIA_DESCRIBER=gpt4o
# AZURE_OPENAI_ENDPOINT=https://your-openai.openai.azure.com/
# AZURE_OPENAI_KEY=your-openai-key-here
# AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o  # GPT-4 supports many languages

AZURE_USE_INTEGRATED_VECTORIZATION=false

# ============================================================================
# CHUNKING CONFIGURATION (Dynamic Adjustment)
# ============================================================================
# For multilingual-e5-large (max_seq_length = 512):
#   Safe limit = 512 * (1 - 0.15 - 0.10) = 384 tokens

# IMPORTANT: Multilingual text tokenizes differently!
# - Latin scripts (English, Spanish, French): ~4 chars per token
# - CJK languages (Chinese, Japanese, Korean): ~1-2 chars per token
# - Arabic scripts: ~3-4 chars per token
# - The pipeline automatically handles this with token-based limits

CHUNKING_MAX_TOKENS=384  # Safe for 512-token model
CHUNKING_MAX_CHARS=2000  # Soft limit (tokens take precedence)
CHUNKING_OVERLAP_PERCENT=10

# Alternative for BGE-M3 (8192 token limit):
# CHUNKING_MAX_TOKENS=6000  # Much larger chunks possible!

# ============================================================================
# TABLE RENDERING (Multilingual Support)
# ============================================================================
AZURE_TABLE_RENDER=markdown  # Unicode-friendly
AZURE_TABLE_SUMMARIES=false

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================
AZURE_CHUNKING_MAX_WORKERS=4
AZURE_CHUNKING_MAX_IMAGE_CONCURRENCY=8
AZURE_CHUNKING_MAX_BATCH_UPLOAD_CONCURRENCY=5

# ============================================================================
# LOGGING (UTF-8 Support)
# ============================================================================
LOG_LEVEL=INFO
LOG_FILE_LEVEL=DEBUG
LOG_ARTIFACTS=true
LOG_USE_COLORS=true

# ============================================================================
# DUMMY AZURE CONFIG (For ChromaDB Option)
# ============================================================================
AZURE_SEARCH_SERVICE=multilingual-dummy
AZURE_SEARCH_INDEX=multilingual-index
AZURE_SEARCH_KEY=dummy-key
AZURE_STORAGE_ACCOUNT=multilingdummy
AZURE_STORAGE_ACCOUNT_KEY=dummykey123==
AZURE_DOC_INT_ENDPOINT=https://multilingual.cognitiveservices.azure.com/
AZURE_DOC_INT_KEY=dummy-doc-key
AZURE_OPENAI_ENDPOINT=https://multilingual.openai.azure.com/
AZURE_OPENAI_KEY=dummy-openai-key
AZURE_OPENAI_CHAT_DEPLOYMENT=dummy-chat
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=dummy-embedding

# ============================================================================
# MULTILINGUAL SEARCH BEST PRACTICES
# ============================================================================
#
# 1. Model Selection:
# -------------------
# - multilingual-e5-large: Best overall quality and cross-lingual search
# - BGE-M3: Best for longer documents and Chinese/English heavy workloads
# - LaBSE: Best for 100+ languages on limited hardware
#
# 2. Cross-Lingual Search:
# ------------------------
# Query in one language, find documents in another:
# - Query: "climate change policy" (English)
# - Results: Documents in Spanish, French, Chinese, etc.
# - multilingual-e5-large excels at this
#
# 3. Same-Language Search:
# ------------------------
# Query and documents in the same language:
# - Still benefits from multilingual models
# - Better semantic understanding
# - Handles code-switching (mixed languages)
#
# 4. Chunking Considerations:
# ---------------------------
# - CJK languages: Shorter character limits (~500-1000 chars)
# - Latin scripts: Standard limits (~1500-2500 chars)
# - Use token-based limits (automatically handles all languages)
#
# 5. Performance:
# ---------------
# - GPU highly recommended for large multilingual models
# - CPU processing: 2-3x slower than smaller English-only models
# - GPU processing: Similar speed to English-only models
#
# ============================================================================
# LANGUAGE-SPECIFIC NOTES
# ============================================================================
#
# Chinese, Japanese, Korean (CJK):
# --------------------------------
# - Tokenizes at ~1-2 chars per token (vs 4 for English)
# - BGE-M3 has excellent CJK performance
# - Consider larger chunk token limits
# - Ensure UTF-8 encoding in source documents
#
# Arabic, Hebrew (RTL):
# ---------------------
# - Right-to-left scripts fully supported
# - Markdown/HTML rendering handles RTL
# - Check PDF extraction quality (some PDFs have RTL issues)
#
# Indic Scripts (Hindi, Bengali, Tamil, etc.):
# ---------------------------------------------
# - Full support in multilingual-e5-large
# - Complex character rendering supported
# - May need specialized fonts for visualization
#
# European Languages:
# -------------------
# - Excellent support across all models
# - Accented characters fully supported
# - Similar performance to English
#
# ============================================================================
# TESTING YOUR MULTILINGUAL SETUP
# ============================================================================
#
# 1. Prepare test corpus with documents in multiple languages
# 2. Process documents: python -m ingestor.cli
# 3. Test same-language queries:
#    - Query in Spanish, expect Spanish documents
# 4. Test cross-lingual queries:
#    - Query in English, expect Spanish/French/Chinese documents
# 5. Check chunk quality in artifacts_multilingual/chunks/
# 6. Verify UTF-8 encoding is preserved
#
# Example queries to test:
# - English: "artificial intelligence applications"
# - Spanish: "aplicaciones de inteligencia artificial"
# - Chinese: "人工智能应用"
# - Arabic: "تطبيقات الذكاء الاصطناعي"
#
# All should return semantically similar results!
#
# ============================================================================
# PERFORMANCE BENCHMARKS
# ============================================================================
#
# Processing Speed (100-page mixed-language PDF):
# - CPU (multilingual-e5-large): ~8-12 minutes
# - GPU (multilingual-e5-large): ~1-2 minutes
# - CPU (BGE-M3): ~10-15 minutes
# - GPU (BGE-M3): ~2-3 minutes
#
# Memory Usage:
# - multilingual-e5-large: ~3GB RAM + model
# - BGE-M3: ~3.5GB RAM + model
# - GPU VRAM: ~4-6GB
#
# Search Quality (Cross-Lingual):
# - multilingual-e5-large: 95%+ accuracy
# - BGE-M3: 93%+ accuracy
# - LaBSE: 88%+ accuracy
# (Based on XTREME and MIRACL benchmarks)
#
# ============================================================================
