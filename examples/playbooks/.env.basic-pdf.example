# ==========================================
# BASIC PDF INGESTION PLAYBOOK - ENVIRONMENT CONFIGURATION
# ==========================================
# Companion .env file for: 01_basic_pdf_ingestion.py
#
# This configuration demonstrates a simple setup for ingesting PDF documents
# with intelligent chunking and semantic search capabilities.
#
# QUICK START:
# 1. Copy this file: cp examples/playbooks/.env.basic-pdf.example .env
# 2. Choose your setup (Option A or B below)
# 3. Fill in credentials for your chosen option
# 4. Run: python examples/playbooks/01_basic_pdf_ingestion.py
# ==========================================

# ==========================================
# OPTION A: CLOUD SETUP (Azure Stack)
# ==========================================
# Best for: Production deployments, enterprise features
# Cost: $$$ (pay per use)
# Requires: Azure subscription and resources

# Vector Store: Azure AI Search
VECTOR_STORE_MODE=azure_search
AZURE_SEARCH_SERVICE=your-search-service
AZURE_SEARCH_INDEX=documents
AZURE_SEARCH_KEY=your-search-admin-key-here

# Embeddings: Azure OpenAI
EMBEDDINGS_MODE=azure_openai
AZURE_OPENAI_ENDPOINT=https://your-openai.openai.azure.com/
AZURE_OPENAI_KEY=your-openai-key-here
AZURE_OPENAI_API_VERSION=2024-12-01-preview
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002

# Document Processing: Azure Document Intelligence
AZURE_DOC_INT_ENDPOINT=https://your-docint.cognitiveservices.azure.com/
AZURE_DOC_INT_KEY=your-document-intelligence-key-here

# Enable integrated vectorization (Azure-specific optimization)
AZURE_USE_INTEGRATED_VECTORIZATION=true

# ==========================================
# OPTION B: OFFLINE SETUP (Local Stack)
# ==========================================
# Best for: Development, testing, air-gapped environments
# Cost: FREE (no cloud services)
# Requires: pip install -r requirements-chromadb.txt requirements-embeddings.txt

# Uncomment these lines to use offline mode:
# VECTOR_STORE_MODE=chromadb
# CHROMADB_COLLECTION_NAME=documents
# CHROMADB_PERSIST_DIR=./chroma_db
# CHROMADB_BATCH_SIZE=1000
#
# EMBEDDINGS_MODE=huggingface
# HUGGINGFACE_MODEL_NAME=sentence-transformers/all-mpnet-base-v2
# HUGGINGFACE_DEVICE=cpu
# HUGGINGFACE_BATCH_SIZE=32
# HUGGINGFACE_NORMALIZE=true
#
# # For offline document processing (requires LibreOffice)
# AZURE_OFFICE_EXTRACTOR_MODE=markitdown

# ==========================================
# INPUT CONFIGURATION
# ==========================================
# Where to find documents to process

# Input Mode: Read from local filesystem
INPUT_MODE=local

# Input Pattern: Process all PDFs in documents/ directory
# Supports glob patterns:
#   - documents/*.pdf (only top-level PDFs)
#   - documents/**/*.pdf (recursive, all PDFs)
#   - documents/**/*.{pdf,docx} (multiple formats)
LOCAL_INPUT_GLOB=documents/**/*.pdf

# ==========================================
# OUTPUT CONFIGURATION
# ==========================================
# Where to save artifacts (pages, chunks, images)

# Artifacts Mode: Save to local filesystem
ARTIFACTS_MODE=local

# Artifacts Directory: Where to save processed artifacts
LOCAL_ARTIFACTS_DIR=./artifacts

# Enable artifact logging for debugging
LOG_ARTIFACTS=true

# ==========================================
# CHUNKING SETTINGS
# ==========================================
# How to split documents into searchable chunks

# Maximum characters per chunk
# Recommendation: 1500-3000 for most use cases
CHUNKING_MAX_CHARS=2000

# Maximum tokens per chunk
# Recommendation: 400-600 (will auto-adjust based on model)
CHUNKING_MAX_TOKENS=500

# Overlap between chunks (percentage)
# Recommendation: 10-15% to maintain context
CHUNKING_OVERLAP_PERCENT=10

# Cross-page overlap (preserve context across page boundaries)
CHUNKING_CROSS_PAGE_OVERLAP=false

# ==========================================
# DOCUMENT PROCESSING
# ==========================================

# Office Document Extraction Mode
# Options:
#   - azure_di: Use Azure Document Intelligence (best quality)
#   - markitdown: Use MarkItDown with LibreOffice (offline, free)
#   - hybrid: Try Azure DI first, fallback to MarkItDown
AZURE_OFFICE_EXTRACTOR_MODE=hybrid

# Media Description (optional, for image-heavy documents)
# Options: disabled, gpt4o
# Note: Requires AZURE_OPENAI_CHAT_DEPLOYMENT if enabled
AZURE_MEDIA_DESCRIBER=disabled

# If enabled, specify chat model:
# AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o-mini

# Table Rendering
# Options: markdown, html
AZURE_TABLE_RENDER=markdown

# Generate table summaries (uses extra API calls)
AZURE_TABLE_SUMMARIES=false

# ==========================================
# PERFORMANCE SETTINGS
# ==========================================

# Parallel processing workers
AZURE_CHUNKING_MAX_WORKERS=4

# Concurrent image processing
AZURE_CHUNKING_MAX_IMAGE_CONCURRENCY=8

# Batch upload concurrency
AZURE_CHUNKING_MAX_BATCH_UPLOAD_CONCURRENCY=5

# Document Intelligence concurrency
AZURE_DI_MAX_CONCURRENCY=3

# OpenAI API settings
AZURE_OPENAI_MAX_RETRIES=3
AZURE_OPENAI_TIMEOUT=30

# ==========================================
# LOGGING CONFIGURATION
# ==========================================

# Log Level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# File logging level (more detailed than console)
LOG_FILE_LEVEL=DEBUG

# Enable colored console output
LOG_USE_COLORS=true

# ==========================================
# EXPECTED OUTPUTS
# ==========================================
# After running this playbook, you should see:
#
# 1. ARTIFACTS DIRECTORY (./artifacts/):
#    - <document>_pages/ - Extracted pages with layout info
#    - <document>_chunks/ - Chunked text ready for embedding
#    - <document>_images/ - Extracted figures and images
#
# 2. VECTOR STORE:
#    - Azure Search: Chunks indexed in specified index
#    - ChromaDB: Chunks stored in ./chroma_db/
#
# 3. LOGS:
#    - Console: INFO level progress
#    - File: ingestion_YYYYMMDD_HHMMSS.log with DEBUG details
#
# 4. PROCESSING STATISTICS:
#    - Number of documents processed
#    - Number of chunks generated
#    - Success/failure counts
#    - Processing time per document

# ==========================================
# TESTING CHECKLIST
# ==========================================
# ☐ 1. Configuration file created (.env)
# ☐ 2. Credentials filled in (Azure OR offline setup)
# ☐ 3. Documents placed in ./documents/ directory
# ☐ 4. Dependencies installed (pip install requirements)
# ☐ 5. For offline: LibreOffice installed (if processing Office docs)
# ☐ 6. For Azure: Index created (run with --setup-index first)
# ☐ 7. Test with 1-2 documents first before bulk processing

# ==========================================
# TROUBLESHOOTING
# ==========================================
#
# "No documents found":
#   - Check LOCAL_INPUT_GLOB pattern matches your files
#   - Verify documents exist in ./documents/
#   - Try absolute path instead of relative
#
# "Authentication failed":
#   - Verify API keys are correct
#   - Check endpoints are properly formatted
#   - Ensure no trailing spaces in keys
#
# "Index not found":
#   - Run with --setup-index flag first
#   - Or create index manually in Azure Portal
#
# "Validation failed":
#   - Check all required dependencies installed
#   - Verify vector store is accessible
#   - For offline: Check ChromaDB is installed
#
# "Chunking errors":
#   - Reduce CHUNKING_MAX_TOKENS if truncation warnings
#   - Increase for longer context needs
#   - Check overlap percent is 0-50
#
# "Out of memory":
#   - Reduce AZURE_CHUNKING_MAX_WORKERS
#   - Reduce HUGGINGFACE_BATCH_SIZE
#   - Process fewer documents at once

# ==========================================
# SECURITY NOTES
# ==========================================
# ⚠️  NEVER commit .env to version control
# ⚠️  Add .env to .gitignore
# ⚠️  Use Azure Key Vault for production secrets
# ⚠️  Rotate credentials regularly
# ⚠️  Use service principals, not personal accounts
