{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Features\n",
    "\n",
    "This notebook explores advanced features of the ingestor library.\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "- Advanced chunking strategies\n",
    "- Custom metadata extraction\n",
    "- Table and figure processing\n",
    "- Formula extraction\n",
    "- Multi-language processing\n",
    "- Index management\n",
    "- Batch processing patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import Pipeline, PipelineConfig, IndexDeploymentManager\n",
    "from ingestor.config import ChunkingMode, TableRenderMode, InputMode\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n",
    "print(\"âœ… Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Chunking Strategies\n",
    "\n",
    "### Layout-Aware Chunking\n",
    "\n",
    "Respects document structure (sections, paragraphs, tables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Layout-aware chunking\n",
    "config.chunking.mode = ChunkingMode.LAYOUT\n",
    "config.chunking.target_chunk_size = 1000\n",
    "config.chunking.chunk_overlap = 150\n",
    "\n",
    "# Preserve section boundaries\n",
    "config.chunking.preserve_section_boundaries = True\n",
    "\n",
    "print(\"Layout-aware chunking configuration:\")\n",
    "print(f\"  Mode: {config.chunking.mode}\")\n",
    "print(f\"  Target size: {config.chunking.target_chunk_size}\")\n",
    "print(f\"  Preserve sections: {config.chunking.preserve_section_boundaries}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Layout-aware chunking:\")\n",
    "print(\"  - Keeps paragraphs intact\")\n",
    "print(\"  - Preserves table structures\")\n",
    "print(\"  - Respects section headers\")\n",
    "print(\"  - Better semantic coherence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Table Processing\n",
    "\n",
    "### Table Rendering Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare table rendering modes\n",
    "modes = [\n",
    "    (TableRenderMode.MARKDOWN_DETAILED, \"Best for search and LLMs\"),\n",
    "    (TableRenderMode.MARKDOWN_COMPACT, \"Space-efficient\"),\n",
    "    (TableRenderMode.HTML, \"For HTML rendering systems\"),\n",
    "    (TableRenderMode.TEXT, \"Most compact, loses structure\")\n",
    "]\n",
    "\n",
    "print(\"Table Rendering Modes:\\n\")\n",
    "for mode, description in modes:\n",
    "    print(f\"{mode.value}:\")\n",
    "    print(f\"  {description}\\n\")\n",
    "\n",
    "# Use detailed markdown for best results\n",
    "config.chunking.table_render_mode = TableRenderMode.MARKDOWN_DETAILED\n",
    "print(f\"âœ… Using: {config.chunking.table_render_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table-Specific Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep tables together in chunks\n",
    "config.chunking.keep_tables_intact = True\n",
    "\n",
    "print(\"Table chunking strategy:\")\n",
    "print(f\"  Keep tables intact: {config.chunking.keep_tables_intact}\")\n",
    "print(\"\\nðŸ’¡ This ensures tables aren't split across chunks\")\n",
    "print(\"   Large tables may create chunks larger than target size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Formula and Math Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable formula extraction\n",
    "config.document_intelligence.features = [\n",
    "    \"OCR_HIGH_RESOLUTION\",\n",
    "    \"FORMULAS\",  # Extract mathematical formulas\n",
    "    \"LANGUAGES\"\n",
    "]\n",
    "\n",
    "print(\"Formula extraction enabled\")\n",
    "print(\"\\nðŸ’¡ Formulas are extracted as LaTeX and included in chunks\")\n",
    "print(\"   Example: $E = mc^2$ or $$\\\\frac{a}{b}$$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multi-language documents\n",
    "config.document_intelligence.features = [\n",
    "    \"LANGUAGES\"  # Auto-detect languages\n",
    "]\n",
    "\n",
    "# Metadata will include detected language\n",
    "# Index field 'language' will be populated automatically\n",
    "\n",
    "print(\"Multi-language support:\")\n",
    "print(\"  - Auto-detects document language\")\n",
    "print(\"  - Stores in 'language' field\")\n",
    "print(\"  - Supports 100+ languages\")\n",
    "print(\"\\nSupported languages include:\")\n",
    "print(\"  English, Spanish, French, German, Chinese, Japanese, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Metadata Extraction\n",
    "\n",
    "Add custom metadata to documents during processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Add custom metadata based on filename patterns\n",
    "from ingestor.models import DocumentMetadata\n",
    "\n",
    "def extract_custom_metadata(filename: str) -> dict:\n",
    "    \"\"\"Extract custom metadata from filename.\"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    # Example: Extract product from filename\n",
    "    # \"pacemaker-5000-manual.pdf\" -> product_family = \"pacemaker\"\n",
    "    if \"pacemaker\" in filename.lower():\n",
    "        metadata[\"product_family\"] = [\"Pacemaker\"]\n",
    "        metadata[\"category\"] = \"Manual\"\n",
    "    elif \"defibrillator\" in filename.lower():\n",
    "        metadata[\"product_family\"] = [\"Defibrillator\"]\n",
    "        metadata[\"category\"] = \"Manual\"\n",
    "    \n",
    "    # Extract year from filename\n",
    "    import re\n",
    "    year_match = re.search(r'20\\d{2}', filename)\n",
    "    if year_match:\n",
    "        metadata[\"published_year\"] = int(year_match.group())\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Test\n",
    "filename = \"pacemaker-5000-2024-manual.pdf\"\n",
    "metadata = extract_custom_metadata(filename)\n",
    "print(f\"Extracted metadata for '{filename}':\")\n",
    "print(f\"  {metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Index Management\n",
    "\n",
    "### Creating and Managing Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create index manager\n",
    "manager = IndexDeploymentManager(\n",
    "    endpoint=f\"https://{os.getenv('AZURE_SEARCH_SERVICE')}.search.windows.net\",\n",
    "    api_key=os.getenv('AZURE_SEARCH_KEY'),\n",
    "    index_name=\"my-advanced-index\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Index manager created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Index Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exists = manager.index_exists()\n",
    "if exists:\n",
    "    print(\"âœ… Index exists\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  Index does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Index with Optimized Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy index (creates if doesn't exist)\n",
    "success = manager.deploy_index(\n",
    "    force=False,           # Don't delete if exists\n",
    "    dry_run=False,         # Actually create\n",
    "    skip_if_exists=True    # Skip if already exists\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"âœ… Index deployed successfully\")\n",
    "    print(\"\\nIndex includes:\")\n",
    "    print(\"  - 21 fields (content, metadata, vectors)\")\n",
    "    print(\"  - 2 scoring profiles (product, content)\")\n",
    "    print(\"  - Semantic search configuration\")\n",
    "    print(\"  - Vector search with compression\")\n",
    "    print(\"  - BM25 similarity tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Index Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate index\n",
    "valid = manager.validate_index()\n",
    "\n",
    "if valid:\n",
    "    print(\"âœ… Index validation passed\")\n",
    "else:\n",
    "    print(\"âš ï¸  Index validation found issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Processing Patterns\n",
    "\n",
    "### Processing Multiple Document Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple document types in batches\n",
    "document_sets = [\n",
    "    {\"glob\": \"manuals/*.pdf\", \"category\": \"Manual\"},\n",
    "    {\"glob\": \"specifications/*.pdf\", \"category\": \"Specification\"},\n",
    "    {\"glob\": \"procedures/*.pdf\", \"category\": \"Procedure\"}\n",
    "]\n",
    "\n",
    "total_chunks = 0\n",
    "total_docs = 0\n",
    "\n",
    "for doc_set in document_sets:\n",
    "    print(f\"\\nðŸ“‚ Processing {doc_set['category']}...\")\n",
    "    \n",
    "    config = PipelineConfig.from_env()\n",
    "    config.input.local.glob = doc_set['glob']\n",
    "    \n",
    "    pipeline = Pipeline(config)\n",
    "    try:\n",
    "        status = await pipeline.run()\n",
    "        total_chunks += status.total_chunks_indexed\n",
    "        total_docs += status.successful_documents\n",
    "        \n",
    "        print(f\"  âœ… {status.successful_documents} documents\")\n",
    "        print(f\"  ðŸ“¦ {status.total_chunks_indexed} chunks\")\n",
    "    finally:\n",
    "        await pipeline.close()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"Total documents: {total_docs}\")\n",
    "print(f\"Total chunks: {total_chunks}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Artifact Management\n",
    "\n",
    "Save processing artifacts for debugging and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor.config import ArtifactsMode\n",
    "\n",
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Enable artifact saving\n",
    "config.artifacts.mode = ArtifactsMode.LOCAL\n",
    "config.artifacts.local_path = \"./processing_artifacts\"\n",
    "\n",
    "print(\"Artifacts will be saved to: ./processing_artifacts/\")\n",
    "print(\"\\nArtifact types:\")\n",
    "print(\"  - {filename}.json: Document Intelligence results\")\n",
    "print(\"  - {filename}.md: Extracted markdown\")\n",
    "print(\"  - {filename}_chunks.json: Chunk metadata\")\n",
    "\n",
    "# Process with artifacts\n",
    "pipeline = Pipeline(config)\n",
    "try:\n",
    "    status = await pipeline.run()\n",
    "    print(f\"\\nâœ… Processing complete with artifacts saved\")\n",
    "finally:\n",
    "    await pipeline.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Handling and Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process with custom error handling\n",
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Set retry configuration\n",
    "config.retry_max_attempts = 3\n",
    "config.retry_delay_seconds = 2.0\n",
    "\n",
    "pipeline = Pipeline(config)\n",
    "try:\n",
    "    status = await pipeline.run()\n",
    "    \n",
    "    # Check for failures\n",
    "    if status.failed_documents > 0:\n",
    "        print(f\"\\nâš ï¸  {status.failed_documents} documents failed\")\n",
    "        print(\"\\nFailed documents:\")\n",
    "        for result in status.results:\n",
    "            if not result.success:\n",
    "                print(f\"  - {result.filename}: {result.error_message}\")\n",
    "finally:\n",
    "    await pipeline.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Process and collect timing metrics\n",
    "config = PipelineConfig.from_env()\n",
    "pipeline = Pipeline(config)\n",
    "\n",
    "try:\n",
    "    status = await pipeline.run()\n",
    "    \n",
    "    # Analyze performance\n",
    "    timings = []\n",
    "    for result in status.results:\n",
    "        if result.success:\n",
    "            timings.append({\n",
    "                \"Filename\": result.filename,\n",
    "                \"Time (s)\": result.processing_time_seconds,\n",
    "                \"Chunks\": result.chunks_indexed,\n",
    "                \"Chunks/sec\": result.chunks_indexed / result.processing_time_seconds if result.processing_time_seconds > 0 else 0\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(timings)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Performance Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(df)), df['Time (s)'])\n",
    "    plt.xlabel('Document')\n",
    "    plt.ylabel('Processing Time (seconds)')\n",
    "    plt.title('Document Processing Times')\n",
    "    plt.show()\n",
    "    \n",
    "finally:\n",
    "    await pipeline.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "âœ… Advanced chunking strategies (layout-aware)  \n",
    "âœ… Table and formula processing  \n",
    "âœ… Multi-language support  \n",
    "âœ… Custom metadata extraction  \n",
    "âœ… Index management operations  \n",
    "âœ… Batch processing patterns  \n",
    "âœ… Artifact management  \n",
    "âœ… Error handling and retry logic  \n",
    "âœ… Performance monitoring  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **04_real_world_legal.ipynb**: Legal document processing\n",
    "- **05_real_world_medical.ipynb**: Medical device manual processing\n",
    "- **07_performance_tuning.ipynb**: Large-scale optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
