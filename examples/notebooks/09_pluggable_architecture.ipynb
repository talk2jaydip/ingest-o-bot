{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pluggable Architecture: Vector Stores & Embeddings Providers\n",
    "\n",
    "This notebook demonstrates the pluggable architecture that enables mixing and matching different vector stores and embeddings providers.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Vector Store Options**: Azure AI Search vs ChromaDB\n",
    "2. **Embeddings Options**: Azure OpenAI, Hugging Face, Cohere, OpenAI\n",
    "3. **Mix & Match**: Combine any vector store with any embeddings provider\n",
    "4. **Offline Setup**: Run completely offline with ChromaDB + Hugging Face\n",
    "5. **Cost Optimization**: Choose free or low-cost options\n",
    "\n",
    "## Supported Combinations (8 total)\n",
    "\n",
    "| Vector Store | Embeddings | Offline | Use Case |\n",
    "|--------------|------------|---------|----------|\n",
    "| Azure Search | Azure OpenAI | ‚ùå | Production cloud (default) |\n",
    "| Azure Search | Hugging Face | ‚ùå | Hybrid (save on embedding costs) |\n",
    "| Azure Search | Cohere | ‚ùå | Cloud optimized |\n",
    "| Azure Search | OpenAI | ‚ùå | Native OpenAI |\n",
    "| ChromaDB | Azure OpenAI | ‚ùå | Local storage, cloud embeddings |\n",
    "| **ChromaDB** | **Hugging Face** | **‚úÖ** | **Fully offline!** |\n",
    "| ChromaDB | Cohere | ‚ùå | Local storage, cloud embeddings |\n",
    "| ChromaDB | OpenAI | ‚ùå | Local storage, cloud embeddings |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Default Configuration (Azure Search + Azure OpenAI)\n",
    "\n",
    "This is the existing configuration that works unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ingestor import ConfigBuilder, Pipeline\n",
    "\n",
    "# Legacy configuration (still works!)\n",
    "config = (\n",
    "    ConfigBuilder()\n",
    "    .with_local_files(\"../test_data/*.pdf\")\n",
    "    .with_azure_search(\n",
    "        service_name=\"your-search-service\",\n",
    "        index_name=\"documents\",\n",
    "        api_key=\"your-key\"\n",
    "    )\n",
    "    .with_azure_openai(\n",
    "        endpoint=\"https://your-openai.openai.azure.com/\",\n",
    "        api_key=\"your-key\",\n",
    "        embedding_deployment=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    .with_local_artifacts(\"../artifacts\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(f\"Vector Store Mode: {config.vector_store_mode}\")\n",
    "print(f\"Embeddings Mode: {config.embeddings_mode}\")\n",
    "print(\"\\n‚úÖ Backward compatibility maintained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Fully Offline (ChromaDB + Hugging Face)\n",
    "\n",
    "Process documents completely offline with no cloud dependencies or API costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies first (run once):\n",
    "# !pip install -r ../../requirements-chromadb.txt\n",
    "# !pip install -r ../../requirements-embeddings.txt\n",
    "\n",
    "from ingestor import ConfigBuilder, Pipeline\n",
    "from ingestor.config import VectorStoreMode, EmbeddingsMode\n",
    "\n",
    "# Fully offline configuration\n",
    "config = (\n",
    "    ConfigBuilder()\n",
    "    .with_local_files(\"../test_data/*.pdf\")\n",
    "    .with_local_artifacts(\"../artifacts_offline\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Override for ChromaDB + Hugging Face\n",
    "config.vector_store_mode = VectorStoreMode.CHROMADB\n",
    "config.vector_store_config = {\n",
    "    'collection_name': 'offline-docs',\n",
    "    'persist_directory': './chroma_db',\n",
    "    'batch_size': 1000\n",
    "}\n",
    "\n",
    "config.embeddings_mode = EmbeddingsMode.HUGGINGFACE\n",
    "config.embeddings_config = {\n",
    "    'model_name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'device': 'cpu',\n",
    "    'batch_size': 32,\n",
    "    'normalize_embeddings': True\n",
    "}\n",
    "\n",
    "# Disable Azure services\n",
    "config.media_describer_mode = 'disabled'\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Vector Store: {config.vector_store_mode}\")\n",
    "print(f\"  Embeddings: {config.embeddings_mode}\")\n",
    "print(f\"  Offline: ‚úÖ YES\")\n",
    "print(f\"  Cost: $0/month\")\n",
    "\n",
    "# Note: First run will download the model (~90MB)\n",
    "# Subsequent runs will use cached model from ~/.cache/huggingface/\n",
    "\n",
    "# Run pipeline\n",
    "# pipeline = Pipeline(config)\n",
    "# await pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Using Environment Variables\n",
    "\n",
    "The easiest way to configure is using environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ.update({\n",
    "    # ChromaDB configuration\n",
    "    'VECTOR_STORE_MODE': 'chromadb',\n",
    "    'CHROMADB_COLLECTION_NAME': 'my-documents',\n",
    "    'CHROMADB_PERSIST_DIR': './chroma_db',\n",
    "    \n",
    "    # Hugging Face configuration\n",
    "    'EMBEDDINGS_MODE': 'huggingface',\n",
    "    'HUGGINGFACE_MODEL_NAME': 'all-MiniLM-L6-v2',\n",
    "    'HUGGINGFACE_DEVICE': 'cpu',\n",
    "    \n",
    "    # Input/Output\n",
    "    'INPUT_MODE': 'local',\n",
    "    'LOCAL_INPUT_GLOB': '../test_data/*.pdf',\n",
    "    'ARTIFACTS_MODE': 'local',\n",
    "    'LOCAL_ARTIFACTS_DIR': '../artifacts',\n",
    "})\n",
    "\n",
    "# Load configuration from environment\n",
    "from ingestor.config import PipelineConfig\n",
    "\n",
    "# This will fail because we're missing some required configs, but shows auto-detection\n",
    "try:\n",
    "    config = PipelineConfig.from_env()\n",
    "    print(f\"‚úÖ Auto-detected vector store: {config.vector_store_mode}\")\n",
    "    print(f\"‚úÖ Auto-detected embeddings: {config.embeddings_mode}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {str(e)[:100]}...\")\n",
    "    print(\"\\nThis is expected - some Azure configs are required even for offline mode.\")\n",
    "    print(\"Use the examples/*.py scripts for complete working examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Model Comparison\n",
    "\n",
    "Compare different embedding models for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor.config import (\n",
    "    HuggingFaceEmbeddingsConfig,\n",
    "    CohereEmbeddingsConfig,\n",
    "    OpenAIEmbeddingsConfig,\n",
    "    AzureOpenAIConfig\n",
    ")\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        'name': 'Azure OpenAI ada-002',\n",
    "        'config': AzureOpenAIConfig,\n",
    "        'dimensions': 1536,\n",
    "        'languages': 'English++',\n",
    "        'cost': '$$$',\n",
    "        'offline': False\n",
    "    },\n",
    "    {\n",
    "        'name': 'HF all-MiniLM-L6-v2',\n",
    "        'config': HuggingFaceEmbeddingsConfig,\n",
    "        'dimensions': 384,\n",
    "        'languages': 'English',\n",
    "        'cost': 'Free',\n",
    "        'offline': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'HF multilingual-e5-large',\n",
    "        'config': HuggingFaceEmbeddingsConfig,\n",
    "        'dimensions': 1024,\n",
    "        'languages': '100+',\n",
    "        'cost': 'Free',\n",
    "        'offline': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Cohere v3 multilingual',\n",
    "        'config': CohereEmbeddingsConfig,\n",
    "        'dimensions': 1024,\n",
    "        'languages': '100+',\n",
    "        'cost': '$$',\n",
    "        'offline': False\n",
    "    },\n",
    "    {\n",
    "        'name': 'OpenAI text-embedding-3-large',\n",
    "        'config': OpenAIEmbeddingsConfig,\n",
    "        'dimensions': 3072,\n",
    "        'languages': 'English++',\n",
    "        'cost': '$$$',\n",
    "        'offline': False\n",
    "    },\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(models)\n",
    "print(\"\\nEmbedding Models Comparison:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Testing Factory Functions\n",
    "\n",
    "The factory pattern makes it easy to switch between implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor.config import VectorStoreMode, EmbeddingsMode, SearchConfig\n",
    "from ingestor.vector_store import create_vector_store\n",
    "from ingestor.embeddings_provider import create_embeddings_provider\n",
    "\n",
    "# Create Azure Search vector store\n",
    "search_config = SearchConfig(\n",
    "    endpoint='https://test.search.windows.net',\n",
    "    index_name='test',\n",
    "    api_key='test-key'\n",
    ")\n",
    "\n",
    "store = create_vector_store(VectorStoreMode.AZURE_SEARCH, search_config)\n",
    "print(f\"‚úÖ Vector Store Created: {type(store).__name__}\")\n",
    "print(f\"   Expected dimensions: {store.get_dimensions()}\")\n",
    "print(f\"   Methods: {[m for m in dir(store) if not m.startswith('_')]}\")\n",
    "\n",
    "# Test embeddings provider creation\n",
    "from ingestor.config import OpenAIEmbeddingsConfig\n",
    "\n",
    "openai_config = OpenAIEmbeddingsConfig(\n",
    "    api_key='test-key',\n",
    "    model_name='text-embedding-3-small'\n",
    ")\n",
    "\n",
    "provider = create_embeddings_provider(EmbeddingsMode.OPENAI, openai_config)\n",
    "print(f\"\\n‚úÖ Embeddings Provider Created: {type(provider).__name__}\")\n",
    "print(f\"   Model: {provider.get_model_name()}\")\n",
    "print(f\"   Dimensions: {provider.get_dimensions()}\")\n",
    "print(f\"   Methods: {[m for m in dir(provider) if not m.startswith('_')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Configuration Scenarios\n",
    "\n",
    "### Scenario 1: Fully Offline (Zero Cloud Dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fully Offline Setup\noffline_env = \"\"\"\n# Vector Store\nVECTOR_STORE_MODE=chromadb\nCHROMADB_PERSIST_DIR=./chroma_db\n\n# Embeddings\nEMBEDDINGS_MODE=huggingface\nHUGGINGFACE_MODEL_NAME=all-MiniLM-L6-v2\n\n# Input/Output\nINPUT_MODE=local\nLOCAL_INPUT_GLOB=./documents/**/*.pdf\nARTIFACTS_MODE=local\n\n# Processing\nEXTRACTION_MODE=markitdown\nMEDIA_DESCRIBER_MODE=disabled\n\"\"\"\n\nprint(\"Fully Offline Configuration:\")\nprint(offline_env)\nprint(\"\\nBenefits:\")\nprint(\"  ‚úÖ Zero API costs\")\nprint(\"  ‚úÖ Complete data privacy\")\nprint(\"  ‚úÖ Works without internet (after initial model download)\")\nprint(\"  ‚úÖ Fast local development\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: Hybrid Cloud/Local (Cost Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid: Azure Search + Local Embeddings\n",
    "hybrid_env = \"\"\"\n",
    "# Vector Store: Azure Search (enterprise features)\n",
    "VECTOR_STORE_MODE=azure_search\n",
    "AZURE_SEARCH_SERVICE=your-service\n",
    "AZURE_SEARCH_INDEX=documents\n",
    "\n",
    "# Embeddings: Hugging Face (zero cost)\n",
    "EMBEDDINGS_MODE=huggingface\n",
    "HUGGINGFACE_MODEL_NAME=intfloat/multilingual-e5-large\n",
    "HUGGINGFACE_DEVICE=cuda  # GPU acceleration\n",
    "\n",
    "# Disable integrated vectorization\n",
    "AZURE_USE_INTEGRATED_VECTORIZATION=false\n",
    "\"\"\"\n",
    "\n",
    "print(\"Hybrid Configuration:\")\n",
    "print(hybrid_env)\n",
    "print(\"\\nBenefits:\")\n",
    "print(\"  ‚úÖ Azure Search enterprise features\")\n",
    "print(\"  ‚úÖ Zero embedding costs (local)\")\n",
    "print(\"  ‚úÖ Best multilingual quality\")\n",
    "print(\"  ‚úÖ GPU-accelerated embeddings\")\n",
    "print(\"\\nCost Savings:\")\n",
    "print(\"  Before: $1,000/month (Azure OpenAI embeddings for 10M tokens)\")\n",
    "print(\"  After: $0/month (local embeddings)\")\n",
    "print(\"  Savings: $1,000/month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: Cloud Optimized (Cohere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud: Azure Search + Cohere\n",
    "cohere_env = \"\"\"\n",
    "# Vector Store: Azure Search\n",
    "VECTOR_STORE_MODE=azure_search\n",
    "AZURE_SEARCH_SERVICE=your-service\n",
    "\n",
    "# Embeddings: Cohere v3 Multilingual\n",
    "EMBEDDINGS_MODE=cohere\n",
    "COHERE_API_KEY=your-cohere-key\n",
    "COHERE_MODEL_NAME=embed-multilingual-v3.0\n",
    "\n",
    "# Disable integrated vectorization\n",
    "AZURE_USE_INTEGRATED_VECTORIZATION=false\n",
    "\"\"\"\n",
    "\n",
    "print(\"Cohere Configuration:\")\n",
    "print(cohere_env)\n",
    "print(\"\\nBenefits:\")\n",
    "print(\"  ‚úÖ Latest multilingual models (100+ languages)\")\n",
    "print(\"  ‚úÖ Competitive pricing\")\n",
    "print(\"  ‚úÖ Optimized for semantic search\")\n",
    "print(\"  ‚úÖ Simple API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Troubleshooting\n",
    "\n",
    "Common issues and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dependency availability\n",
    "print(\"Checking dependencies...\\n\")\n",
    "\n",
    "# Check ChromaDB\n",
    "try:\n",
    "    import chromadb\n",
    "    print(\"‚úÖ chromadb installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå chromadb not installed\")\n",
    "    print(\"   Install with: pip install chromadb\")\n",
    "\n",
    "# Check sentence-transformers\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    print(\"‚úÖ sentence-transformers installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå sentence-transformers not installed\")\n",
    "    print(\"   Install with: pip install sentence-transformers\")\n",
    "\n",
    "# Check cohere\n",
    "try:\n",
    "    import cohere\n",
    "    print(\"‚úÖ cohere installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå cohere not installed\")\n",
    "    print(\"   Install with: pip install cohere\")\n",
    "\n",
    "# Check openai\n",
    "try:\n",
    "    import openai\n",
    "    print(\"‚úÖ openai installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå openai not installed\")\n",
    "    print(\"   Install with: pip install openai\")\n",
    "\n",
    "# Check torch\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ torch installed\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   üöÄ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        print(f\"   üöÄ MPS (Apple Silicon) available\")\n",
    "    else:\n",
    "        print(f\"   üíª CPU only\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå torch not installed\")\n",
    "    print(\"   Install with: pip install torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: Quick Reference\n",
    "\n",
    "### Installation Commands\n",
    "\n",
    "```bash\n",
    "# Base package\n",
    "pip install -e .\n",
    "\n",
    "# ChromaDB support\n",
    "pip install -r requirements-chromadb.txt\n",
    "\n",
    "# All embeddings providers\n",
    "pip install -r requirements-embeddings.txt\n",
    "\n",
    "# Individual providers\n",
    "pip install chromadb\n",
    "pip install sentence-transformers torch\n",
    "pip install cohere\n",
    "```\n",
    "\n",
    "### Environment Variables Quick Reference\n",
    "\n",
    "**Vector Store:**\n",
    "```bash\n",
    "VECTOR_STORE_MODE=azure_search  # or chromadb\n",
    "```\n",
    "\n",
    "**Embeddings:**\n",
    "```bash\n",
    "EMBEDDINGS_MODE=azure_openai  # or huggingface, cohere, openai\n",
    "```\n",
    "\n",
    "### Documentation Links\n",
    "\n",
    "- [Vector Stores Guide](../../docs/vector_stores.md)\n",
    "- [Embeddings Guide](../../docs/embeddings_providers.md)\n",
    "- [Configuration Examples](../../docs/configuration_examples.md)\n",
    "- [Implementation Summary](../../PLUGGABLE_ARCHITECTURE_SUMMARY.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Example 9: Dynamic Chunking\n\nThe pipeline now features **automatic chunk size adjustment** based on embedding model token limits.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generic vs Azure-prefixed parameter names\ngeneric_params = \"\"\"\n# Generic style (recommended for new configs)\nCHUNKING_MAX_TOKENS=500\nCHUNKING_MAX_CHARS=2000\nCHUNKING_OVERLAP_PERCENT=10\n\"\"\"\n\nazure_params = \"\"\"\n# Azure-prefixed style (backward compatibility)\nAZURE_CHUNKING_MAX_TOKENS=500\nAZURE_CHUNKING_MAX_CHARS=2000\nAZURE_CHUNKING_OVERLAP_PERCENT=10\n\"\"\"\n\nprint(\"Generic Parameters (Recommended):\")\nprint(generic_params)\nprint(\"\\nAzure-Prefixed Parameters (Backward Compatibility):\")\nprint(azure_params)\nprint(\"\\nBoth work identically - choose based on your preference!\")\nprint(\"Generic names are cleaner for multi-provider configurations.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Generic vs Azure-Prefixed Parameters\n\nBoth parameter naming styles are supported:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Manual override example\nimport os\n\n# Set environment variable fallback\nos.environ['EMBEDDINGS_MAX_SEQ_LENGTH'] = '384'\n\nprint(\"Manual Override:\")\nprint(f\"  EMBEDDINGS_MAX_SEQ_LENGTH=384\")\nprint()\nprint(\"Use cases:\")\nprint(\"  ‚Ä¢ Provider doesn't report max_seq_length\")\nprint(\"  ‚Ä¢ Custom embedding model\")\nprint(\"  ‚Ä¢ Testing different limits\")\nprint()\nprint(\"This fallback is checked if get_max_seq_length() fails.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Manual Override\n\nIf you need to set a specific token limit manually:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compare max_seq_length across different models\nimport pandas as pd\n\nmodels_comparison = [\n    {\n        \"Provider\": \"Azure OpenAI\",\n        \"Model\": \"text-embedding-ada-002\",\n        \"Max Tokens\": 8191,\n        \"Safe Limit (15% buffer + 10% overlap)\": 6143\n    },\n    {\n        \"Provider\": \"Hugging Face\",\n        \"Model\": \"all-MiniLM-L6-v2\",\n        \"Max Tokens\": 256,\n        \"Safe Limit (15% buffer + 10% overlap)\": 192\n    },\n    {\n        \"Provider\": \"Hugging Face\",\n        \"Model\": \"all-mpnet-base-v2\",\n        \"Max Tokens\": 384,\n        \"Safe Limit (15% buffer + 10% overlap)\": 288\n    },\n    {\n        \"Provider\": \"Hugging Face\",\n        \"Model\": \"multilingual-e5-large\",\n        \"Max Tokens\": 512,\n        \"Safe Limit (15% buffer + 10% overlap)\": 384\n    },\n    {\n        \"Provider\": \"Cohere\",\n        \"Model\": \"embed-multilingual-v3.0\",\n        \"Max Tokens\": 512,\n        \"Safe Limit (15% buffer + 10% overlap)\": 384\n    },\n    {\n        \"Provider\": \"OpenAI\",\n        \"Model\": \"text-embedding-3-large\",\n        \"Max Tokens\": 8191,\n        \"Safe Limit (15% buffer + 10% overlap)\": 6143\n    }\n]\n\ndf = pd.DataFrame(models_comparison)\nprint(\"\\nEmbedding Model Token Limits:\")\nprint(df.to_string(index=False))\nprint()\nprint(\"Note: Safe limits are calculated automatically by the pipeline.\")\nprint(\"Formula: max_tokens * (1 - 0.15 - overlap_percent/100)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### How max_seq_length Works\n\nDifferent embedding models have different token limits:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example: Dynamic chunking with small model\nfrom ingestor.config import HuggingFaceEmbeddingsConfig\n\n# Small model with 256 token limit\nsmall_model_config = HuggingFaceEmbeddingsConfig(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n    device=\"cpu\"\n)\n\nprint(\"Model Configuration:\")\nprint(f\"  Model: {small_model_config.model_name}\")\nprint(f\"  Expected max_seq_length: 256 tokens\")\nprint()\n\n# Chunking configuration\nprint(\"Initial Chunking Config:\")\nprint(f\"  CHUNKING_MAX_TOKENS: 500\")\nprint(f\"  CHUNKING_OVERLAP_PERCENT: 10\")\nprint()\n\n# What the pipeline will do:\nmax_seq = 256\nsafety_buffer = 0.15\noverlap_percent = 0.10\nsafe_limit = int(max_seq * (1 - safety_buffer - overlap_percent))\n\nprint(\"Dynamic Adjustment:\")\nprint(f\"  Formula: {max_seq} * (1 - {safety_buffer} - {overlap_percent}) = {safe_limit} tokens\")\nprint()\nprint(f\"‚ö†Ô∏è  Embedding model max_seq_length ({max_seq}) is smaller than\")\nprint(f\"    CHUNKING_MAX_TOKENS (500).\")\nprint(f\"    Automatically reducing chunking limit to {safe_limit} tokens\")\nprint(f\"    (with 15% buffer and 10% overlap allowance)\")\nprint()\nprint(\"Benefits:\")\nprint(\"  ‚úÖ Prevents truncation and information loss\")\nprint(\"  ‚úÖ No manual calculation needed\")\nprint(\"  ‚úÖ Maintains semantic overlap\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}