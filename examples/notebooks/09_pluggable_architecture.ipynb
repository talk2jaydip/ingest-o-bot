{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pluggable Architecture: Vector Stores & Embeddings Providers\n",
    "\n",
    "This notebook demonstrates the pluggable architecture that enables mixing and matching different vector stores and embeddings providers.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Vector Store Options**: Azure AI Search vs ChromaDB\n",
    "2. **Embeddings Options**: Azure OpenAI, Hugging Face, Cohere, OpenAI\n",
    "3. **Mix & Match**: Combine any vector store with any embeddings provider\n",
    "4. **Offline Setup**: Run completely offline with ChromaDB + Hugging Face\n",
    "5. **Cost Optimization**: Choose free or low-cost options\n",
    "\n",
    "## Supported Combinations (8 total)\n",
    "\n",
    "| Vector Store | Embeddings | Offline | Use Case |\n",
    "|--------------|------------|---------|----------|\n",
    "| Azure Search | Azure OpenAI | ‚ùå | Production cloud (default) |\n",
    "| Azure Search | Hugging Face | ‚ùå | Hybrid (save on embedding costs) |\n",
    "| Azure Search | Cohere | ‚ùå | Cloud optimized |\n",
    "| Azure Search | OpenAI | ‚ùå | Native OpenAI |\n",
    "| ChromaDB | Azure OpenAI | ‚ùå | Local storage, cloud embeddings |\n",
    "| **ChromaDB** | **Hugging Face** | **‚úÖ** | **Fully offline!** |\n",
    "| ChromaDB | Cohere | ‚ùå | Local storage, cloud embeddings |\n",
    "| ChromaDB | OpenAI | ‚ùå | Local storage, cloud embeddings |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Default Configuration (Azure Search + Azure OpenAI)\n",
    "\n",
    "This is the existing configuration that works unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ingestor import ConfigBuilder, Pipeline\n",
    "\n",
    "# Legacy configuration (still works!)\n",
    "config = (\n",
    "    ConfigBuilder()\n",
    "    .with_local_files(\"../test_data/*.pdf\")\n",
    "    .with_azure_search(\n",
    "        service_name=\"your-search-service\",\n",
    "        index_name=\"documents\",\n",
    "        api_key=\"your-key\"\n",
    "    )\n",
    "    .with_azure_openai(\n",
    "        endpoint=\"https://your-openai.openai.azure.com/\",\n",
    "        api_key=\"your-key\",\n",
    "        embedding_deployment=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    .with_local_artifacts(\"../artifacts\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(f\"Vector Store Mode: {config.vector_store_mode}\")\n",
    "print(f\"Embeddings Mode: {config.embeddings_mode}\")\n",
    "print(\"\\n‚úÖ Backward compatibility maintained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Fully Offline (ChromaDB + Hugging Face)\n",
    "\n",
    "Process documents completely offline with no cloud dependencies or API costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies first (run once):\n",
    "# !pip install -r ../../requirements-chromadb.txt\n",
    "# !pip install -r ../../requirements-embeddings.txt\n",
    "\n",
    "from ingestor import ConfigBuilder, Pipeline\n",
    "from ingestor.config import VectorStoreMode, EmbeddingsMode\n",
    "\n",
    "# Fully offline configuration\n",
    "config = (\n",
    "    ConfigBuilder()\n",
    "    .with_local_files(\"../test_data/*.pdf\")\n",
    "    .with_local_artifacts(\"../artifacts_offline\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Override for ChromaDB + Hugging Face\n",
    "config.vector_store_mode = VectorStoreMode.CHROMADB\n",
    "config.vector_store_config = {\n",
    "    'collection_name': 'offline-docs',\n",
    "    'persist_directory': './chroma_db',\n",
    "    'batch_size': 1000\n",
    "}\n",
    "\n",
    "config.embeddings_mode = EmbeddingsMode.HUGGINGFACE\n",
    "config.embeddings_config = {\n",
    "    'model_name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'device': 'cpu',\n",
    "    'batch_size': 32,\n",
    "    'normalize_embeddings': True\n",
    "}\n",
    "\n",
    "# Disable Azure services\n",
    "config.media_describer_mode = 'disabled'\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Vector Store: {config.vector_store_mode}\")\n",
    "print(f\"  Embeddings: {config.embeddings_mode}\")\n",
    "print(f\"  Offline: ‚úÖ YES\")\n",
    "print(f\"  Cost: $0/month\")\n",
    "\n",
    "# Note: First run will download the model (~90MB)\n",
    "# Subsequent runs will use cached model from ~/.cache/huggingface/\n",
    "\n",
    "# Run pipeline\n",
    "# pipeline = Pipeline(config)\n",
    "# await pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Using Environment Variables\n",
    "\n",
    "The easiest way to configure is using environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ.update({\n",
    "    # ChromaDB configuration\n",
    "    'VECTOR_STORE_MODE': 'chromadb',\n",
    "    'CHROMADB_COLLECTION_NAME': 'my-documents',\n",
    "    'CHROMADB_PERSIST_DIR': './chroma_db',\n",
    "    \n",
    "    # Hugging Face configuration\n",
    "    'EMBEDDINGS_MODE': 'huggingface',\n",
    "    'HUGGINGFACE_MODEL_NAME': 'all-MiniLM-L6-v2',\n",
    "    'HUGGINGFACE_DEVICE': 'cpu',\n",
    "    \n",
    "    # Input/Output\n",
    "    'INPUT_MODE': 'local',\n",
    "    'LOCAL_INPUT_GLOB': '../test_data/*.pdf',\n",
    "    'ARTIFACTS_MODE': 'local',\n",
    "    'LOCAL_ARTIFACTS_DIR': '../artifacts',\n",
    "})\n",
    "\n",
    "# Load configuration from environment\n",
    "from ingestor.config import PipelineConfig\n",
    "\n",
    "# This will fail because we're missing some required configs, but shows auto-detection\n",
    "try:\n",
    "    config = PipelineConfig.from_env()\n",
    "    print(f\"‚úÖ Auto-detected vector store: {config.vector_store_mode}\")\n",
    "    print(f\"‚úÖ Auto-detected embeddings: {config.embeddings_mode}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {str(e)[:100]}...\")\n",
    "    print(\"\\nThis is expected - some Azure configs are required even for offline mode.\")\n",
    "    print(\"Use the examples/*.py scripts for complete working examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Model Comparison\n",
    "\n",
    "Compare different embedding models for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor.config import (\n",
    "    HuggingFaceEmbeddingsConfig,\n",
    "    CohereEmbeddingsConfig,\n",
    "    OpenAIEmbeddingsConfig,\n",
    "    AzureOpenAIConfig\n",
    ")\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        'name': 'Azure OpenAI ada-002',\n",
    "        'config': AzureOpenAIConfig,\n",
    "        'dimensions': 1536,\n",
    "        'languages': 'English++',\n",
    "        'cost': '$$$',\n",
    "        'offline': False\n",
    "    },\n",
    "    {\n",
    "        'name': 'HF all-MiniLM-L6-v2',\n",
    "        'config': HuggingFaceEmbeddingsConfig,\n",
    "        'dimensions': 384,\n",
    "        'languages': 'English',\n",
    "        'cost': 'Free',\n",
    "        'offline': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'HF multilingual-e5-large',\n",
    "        'config': HuggingFaceEmbeddingsConfig,\n",
    "        'dimensions': 1024,\n",
    "        'languages': '100+',\n",
    "        'cost': 'Free',\n",
    "        'offline': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Cohere v3 multilingual',\n",
    "        'config': CohereEmbeddingsConfig,\n",
    "        'dimensions': 1024,\n",
    "        'languages': '100+',\n",
    "        'cost': '$$',\n",
    "        'offline': False\n",
    "    },\n",
    "    {\n",
    "        'name': 'OpenAI text-embedding-3-large',\n",
    "        'config': OpenAIEmbeddingsConfig,\n",
    "        'dimensions': 3072,\n",
    "        'languages': 'English++',\n",
    "        'cost': '$$$',\n",
    "        'offline': False\n",
    "    },\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(models)\n",
    "print(\"\\nEmbedding Models Comparison:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Testing Factory Functions\n",
    "\n",
    "The factory pattern makes it easy to switch between implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor.config import VectorStoreMode, EmbeddingsMode, SearchConfig\n",
    "from ingestor.vector_store import create_vector_store\n",
    "from ingestor.embeddings_provider import create_embeddings_provider\n",
    "\n",
    "# Create Azure Search vector store\n",
    "search_config = SearchConfig(\n",
    "    endpoint='https://test.search.windows.net',\n",
    "    index_name='test',\n",
    "    api_key='test-key'\n",
    ")\n",
    "\n",
    "store = create_vector_store(VectorStoreMode.AZURE_SEARCH, search_config)\n",
    "print(f\"‚úÖ Vector Store Created: {type(store).__name__}\")\n",
    "print(f\"   Expected dimensions: {store.get_dimensions()}\")\n",
    "print(f\"   Methods: {[m for m in dir(store) if not m.startswith('_')]}\")\n",
    "\n",
    "# Test embeddings provider creation\n",
    "from ingestor.config import OpenAIEmbeddingsConfig\n",
    "\n",
    "openai_config = OpenAIEmbeddingsConfig(\n",
    "    api_key='test-key',\n",
    "    model_name='text-embedding-3-small'\n",
    ")\n",
    "\n",
    "provider = create_embeddings_provider(EmbeddingsMode.OPENAI, openai_config)\n",
    "print(f\"\\n‚úÖ Embeddings Provider Created: {type(provider).__name__}\")\n",
    "print(f\"   Model: {provider.get_model_name()}\")\n",
    "print(f\"   Dimensions: {provider.get_dimensions()}\")\n",
    "print(f\"   Methods: {[m for m in dir(provider) if not m.startswith('_')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Configuration Scenarios\n",
    "\n",
    "### Scenario 1: Fully Offline (Zero Cloud Dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Offline Setup\n",
    "offline_env = \"\"\"\n",
    "# Vector Store\n",
    "VECTOR_STORE_MODE=chromadb\n",
    "CHROMADB_PERSIST_DIR=./chroma_db\n",
    "\n",
    "# Embeddings\n",
    "EMBEDDINGS_MODE=huggingface\n",
    "HUGGINGFACE_MODEL_NAME=all-MiniLM-L6-v2\n",
    "\n",
    "# Input/Output\n",
    "INPUT_MODE=local\n",
    "LOCAL_INPUT_GLOB=./documents/**/*.pdf\n",
    "ARTIFACTS_MODE=local\n",
    "\n",
    "# Processing\n",
    "AZURE_OFFICE_EXTRACTOR_MODE=markitdown\n",
    "AZURE_MEDIA_DESCRIBER=disabled\n",
    "\"\"\"\n",
    "\n",
    "print(\"Fully Offline Configuration:\")\n",
    "print(offline_env)\n",
    "print(\"\\nBenefits:\")\n",
    "print(\"  ‚úÖ Zero API costs\")\n",
    "print(\"  ‚úÖ Complete data privacy\")\n",
    "print(\"  ‚úÖ Works without internet (after initial model download)\")\n",
    "print(\"  ‚úÖ Fast local development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: Hybrid Cloud/Local (Cost Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid: Azure Search + Local Embeddings\n",
    "hybrid_env = \"\"\"\n",
    "# Vector Store: Azure Search (enterprise features)\n",
    "VECTOR_STORE_MODE=azure_search\n",
    "AZURE_SEARCH_SERVICE=your-service\n",
    "AZURE_SEARCH_INDEX=documents\n",
    "\n",
    "# Embeddings: Hugging Face (zero cost)\n",
    "EMBEDDINGS_MODE=huggingface\n",
    "HUGGINGFACE_MODEL_NAME=intfloat/multilingual-e5-large\n",
    "HUGGINGFACE_DEVICE=cuda  # GPU acceleration\n",
    "\n",
    "# Disable integrated vectorization\n",
    "AZURE_USE_INTEGRATED_VECTORIZATION=false\n",
    "\"\"\"\n",
    "\n",
    "print(\"Hybrid Configuration:\")\n",
    "print(hybrid_env)\n",
    "print(\"\\nBenefits:\")\n",
    "print(\"  ‚úÖ Azure Search enterprise features\")\n",
    "print(\"  ‚úÖ Zero embedding costs (local)\")\n",
    "print(\"  ‚úÖ Best multilingual quality\")\n",
    "print(\"  ‚úÖ GPU-accelerated embeddings\")\n",
    "print(\"\\nCost Savings:\")\n",
    "print(\"  Before: $1,000/month (Azure OpenAI embeddings for 10M tokens)\")\n",
    "print(\"  After: $0/month (local embeddings)\")\n",
    "print(\"  Savings: $1,000/month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: Cloud Optimized (Cohere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud: Azure Search + Cohere\n",
    "cohere_env = \"\"\"\n",
    "# Vector Store: Azure Search\n",
    "VECTOR_STORE_MODE=azure_search\n",
    "AZURE_SEARCH_SERVICE=your-service\n",
    "\n",
    "# Embeddings: Cohere v3 Multilingual\n",
    "EMBEDDINGS_MODE=cohere\n",
    "COHERE_API_KEY=your-cohere-key\n",
    "COHERE_MODEL_NAME=embed-multilingual-v3.0\n",
    "\n",
    "# Disable integrated vectorization\n",
    "AZURE_USE_INTEGRATED_VECTORIZATION=false\n",
    "\"\"\"\n",
    "\n",
    "print(\"Cohere Configuration:\")\n",
    "print(cohere_env)\n",
    "print(\"\\nBenefits:\")\n",
    "print(\"  ‚úÖ Latest multilingual models (100+ languages)\")\n",
    "print(\"  ‚úÖ Competitive pricing\")\n",
    "print(\"  ‚úÖ Optimized for semantic search\")\n",
    "print(\"  ‚úÖ Simple API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Troubleshooting\n",
    "\n",
    "Common issues and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dependency availability\n",
    "print(\"Checking dependencies...\\n\")\n",
    "\n",
    "# Check ChromaDB\n",
    "try:\n",
    "    import chromadb\n",
    "    print(\"‚úÖ chromadb installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå chromadb not installed\")\n",
    "    print(\"   Install with: pip install chromadb\")\n",
    "\n",
    "# Check sentence-transformers\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    print(\"‚úÖ sentence-transformers installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå sentence-transformers not installed\")\n",
    "    print(\"   Install with: pip install sentence-transformers\")\n",
    "\n",
    "# Check cohere\n",
    "try:\n",
    "    import cohere\n",
    "    print(\"‚úÖ cohere installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå cohere not installed\")\n",
    "    print(\"   Install with: pip install cohere\")\n",
    "\n",
    "# Check openai\n",
    "try:\n",
    "    import openai\n",
    "    print(\"‚úÖ openai installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå openai not installed\")\n",
    "    print(\"   Install with: pip install openai\")\n",
    "\n",
    "# Check torch\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ torch installed\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   üöÄ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        print(f\"   üöÄ MPS (Apple Silicon) available\")\n",
    "    else:\n",
    "        print(f\"   üíª CPU only\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå torch not installed\")\n",
    "    print(\"   Install with: pip install torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: Quick Reference\n",
    "\n",
    "### Installation Commands\n",
    "\n",
    "```bash\n",
    "# Base package\n",
    "pip install -e .\n",
    "\n",
    "# ChromaDB support\n",
    "pip install -r requirements-chromadb.txt\n",
    "\n",
    "# All embeddings providers\n",
    "pip install -r requirements-embeddings.txt\n",
    "\n",
    "# Individual providers\n",
    "pip install chromadb\n",
    "pip install sentence-transformers torch\n",
    "pip install cohere\n",
    "```\n",
    "\n",
    "### Environment Variables Quick Reference\n",
    "\n",
    "**Vector Store:**\n",
    "```bash\n",
    "VECTOR_STORE_MODE=azure_search  # or chromadb\n",
    "```\n",
    "\n",
    "**Embeddings:**\n",
    "```bash\n",
    "EMBEDDINGS_MODE=azure_openai  # or huggingface, cohere, openai\n",
    "```\n",
    "\n",
    "### Documentation Links\n",
    "\n",
    "- [Vector Stores Guide](../../docs/vector_stores.md)\n",
    "- [Embeddings Guide](../../docs/embeddings_providers.md)\n",
    "- [Configuration Examples](../../docs/configuration_examples.md)\n",
    "- [Implementation Summary](../../PLUGGABLE_ARCHITECTURE_SUMMARY.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
