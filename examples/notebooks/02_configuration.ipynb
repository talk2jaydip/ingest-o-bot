{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestor Configuration Guide\n",
    "\n",
    "This notebook covers all configuration options available in the ingestor library.\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "- Configuration loading (environment vs programmatic)\n",
    "- Input configuration (local files, blob storage)\n",
    "- Azure service configuration\n",
    "- Chunking configuration\n",
    "- Document Intelligence settings\n",
    "- Embedding configuration\n",
    "- Output and artifacts configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import PipelineConfig, create_config\n",
    "from ingestor.config import (\n",
    "    InputMode,\n",
    "    ArtifactsMode,\n",
    "    ChunkingMode,\n",
    "    OverlapMode,\n",
    "    TableRenderMode\n",
    ")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(\"‚úÖ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Loading Methods\n",
    "\n",
    "### Method 1: Load from Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all settings from .env file\n",
    "config = PipelineConfig.from_env()\n",
    "\n",
    "print(\"Configuration loaded from environment:\")\n",
    "print(f\"  Search service: {config.search.service_name}\")\n",
    "print(f\"  Index: {config.search.index_name}\")\n",
    "print(f\"  Document Intelligence: {config.document_intelligence.endpoint}\")\n",
    "print(f\"  OpenAI: {config.openai.endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Programmatic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config from scratch (no environment variables)\n",
    "from ingestor.config import (\n",
    "    SearchConfig,\n",
    "    DocumentIntelligenceConfig,\n",
    "    OpenAIConfig,\n",
    "    InputConfig,\n",
    "    LocalInputConfig\n",
    ")\n",
    "\n",
    "config = PipelineConfig(\n",
    "    search=SearchConfig(\n",
    "        service_name=\"my-search-service\",\n",
    "        api_key=\"my-key\",\n",
    "        index_name=\"my-index\"\n",
    "    ),\n",
    "    document_intelligence=DocumentIntelligenceConfig(\n",
    "        endpoint=\"https://my-di.cognitiveservices.azure.com\",\n",
    "        api_key=\"my-di-key\"\n",
    "    ),\n",
    "    openai=OpenAIConfig(\n",
    "        endpoint=\"https://my-openai.openai.azure.com\",\n",
    "        api_key=\"my-openai-key\",\n",
    "        embedding_deployment=\"text-embedding-ada-002\"\n",
    "    ),\n",
    "    input=InputConfig(\n",
    "        mode=InputMode.LOCAL,\n",
    "        local=LocalInputConfig(\n",
    "            glob=\"documents/*.pdf\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Programmatic configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Hybrid (Environment + Override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from environment, then override specific settings\n",
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Override specific settings\n",
    "config.search.index_name = \"custom-index\"\n",
    "config.chunking.target_chunk_size = 1500\n",
    "config.chunking.chunk_overlap = 300\n",
    "\n",
    "print(\"‚úÖ Hybrid configuration (env + overrides)\")\n",
    "print(f\"  Index: {config.search.index_name}\")\n",
    "print(f\"  Chunk size: {config.chunking.target_chunk_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input Configuration\n",
    "\n",
    "### Local File Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = create_config(\n",
    "    input_glob=\"documents/*.pdf\"\n",
    ")\n",
    "\n",
    "print(f\"Input mode: {config.input.mode}\")\n",
    "print(f\"Glob pattern: {config.input.local.glob}\")\n",
    "\n",
    "# Advanced glob patterns\n",
    "examples = [\n",
    "    \"documents/*.pdf\",              # All PDFs in documents/\n",
    "    \"documents/**/*.pdf\",            # All PDFs recursively\n",
    "    \"documents/*.{pdf,docx}\",        # PDFs and DOCX files\n",
    "    \"documents/2024-*.pdf\",          # PDFs starting with 2024-\n",
    "    \"documents/*/reports/*.pdf\"      # PDFs in reports subdirectories\n",
    "]\n",
    "\n",
    "print(\"\\nüìã Glob pattern examples:\")\n",
    "for pattern in examples:\n",
    "    print(f\"  {pattern}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Blob Storage Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor.config import BlobStorageConfig\n",
    "\n",
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Configure blob storage input\n",
    "config.input.mode = InputMode.BLOB_STORAGE\n",
    "config.input.blob = BlobStorageConfig(\n",
    "    account_name=\"mystorageaccount\",\n",
    "    account_key=\"my-storage-key\",\n",
    "    container_name=\"documents\",\n",
    "    glob=\"**/*.pdf\"  # All PDFs in container\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Blob storage input configured\")\n",
    "print(f\"  Container: {config.input.blob.container_name}\")\n",
    "print(f\"  Pattern: {config.input.blob.glob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunking Configuration\n",
    "\n",
    "Control how documents are split into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Basic chunking settings\n",
    "config.chunking.target_chunk_size = 1000      # Target size in characters\n",
    "config.chunking.chunk_overlap = 200           # Overlap between chunks\n",
    "config.chunking.mode = ChunkingMode.LAYOUT    # Layout-aware chunking\n",
    "\n",
    "print(\"Chunking configuration:\")\n",
    "print(f\"  Target size: {config.chunking.target_chunk_size} chars\")\n",
    "print(f\"  Overlap: {config.chunking.chunk_overlap} chars\")\n",
    "print(f\"  Mode: {config.chunking.mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available chunking modes:\\n\")\n",
    "\n",
    "print(\"1. LAYOUT (recommended)\")\n",
    "print(\"   - Respects document structure (sections, paragraphs)\")\n",
    "print(\"   - Uses Document Intelligence layout analysis\")\n",
    "print(\"   - Best for: Technical manuals, structured documents\\n\")\n",
    "\n",
    "print(\"2. PAGE\")\n",
    "print(\"   - One chunk per page\")\n",
    "print(\"   - Simple and fast\")\n",
    "print(\"   - Best for: Presentations, forms\\n\")\n",
    "\n",
    "print(\"3. FIXED\")\n",
    "print(\"   - Fixed-size chunks by character count\")\n",
    "print(\"   - Ignores document structure\")\n",
    "print(\"   - Best for: Plain text, simple documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Overlap mode: WORDS vs CHARACTERS\n",
    "config.chunking.overlap_mode = OverlapMode.WORDS\n",
    "config.chunking.chunk_overlap = 50  # 50 words overlap\n",
    "\n",
    "print(f\"Overlap mode: {config.chunking.overlap_mode}\")\n",
    "print(f\"Overlap amount: {config.chunking.chunk_overlap}\")\n",
    "\n",
    "# Why overlap matters\n",
    "print(\"\\nüí° Overlap ensures context isn't lost at chunk boundaries\")\n",
    "print(\"   Recommended: 10-20% of chunk size\")\n",
    "print(\"   Example: 1000 char chunks ‚Üí 100-200 char overlap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Intelligence Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Document Intelligence settings\n",
    "config.document_intelligence.model = \"prebuilt-layout\"  # Or \"prebuilt-read\"\n",
    "config.document_intelligence.features = [\n",
    "    \"OCR_HIGH_RESOLUTION\",\n",
    "    \"FORMULAS\",\n",
    "    \"LANGUAGES\"\n",
    "]\n",
    "\n",
    "print(\"Document Intelligence settings:\")\n",
    "print(f\"  Endpoint: {config.document_intelligence.endpoint}\")\n",
    "print(f\"  Model: {config.document_intelligence.model}\")\n",
    "print(f\"  Features: {config.document_intelligence.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Table Processing Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Table rendering mode\n",
    "config.chunking.table_render_mode = TableRenderMode.MARKDOWN_DETAILED\n",
    "\n",
    "print(\"Table render modes:\\n\")\n",
    "print(\"1. MARKDOWN_DETAILED (recommended)\")\n",
    "print(\"   - Full markdown tables with all structure\")\n",
    "print(\"   - Best for search and LLM understanding\\n\")\n",
    "\n",
    "print(\"2. MARKDOWN_COMPACT\")\n",
    "print(\"   - Simplified markdown\")\n",
    "print(\"   - Saves space for simple tables\\n\")\n",
    "\n",
    "print(\"3. HTML\")\n",
    "print(\"   - HTML table format\")\n",
    "print(\"   - Use if your RAG system renders HTML\\n\")\n",
    "\n",
    "print(\"4. TEXT\")\n",
    "print(\"   - Plain text representation\")\n",
    "print(\"   - Most compact, loses structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embedding Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# OpenAI embedding settings\n",
    "config.openai.embedding_deployment = \"text-embedding-ada-002\"\n",
    "config.openai.embedding_dimensions = 1536  # Default for ada-002\n",
    "\n",
    "# For text-embedding-3-large (higher quality)\n",
    "# config.openai.embedding_deployment = \"text-embedding-3-large\"\n",
    "# config.openai.embedding_dimensions = 1536  # or 3072 for full dimensions\n",
    "\n",
    "print(\"Embedding configuration:\")\n",
    "print(f\"  Deployment: {config.openai.embedding_deployment}\")\n",
    "print(f\"  Dimensions: {config.openai.embedding_dimensions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Artifacts Configuration\n",
    "\n",
    "Save intermediate processing artifacts (JSON, markdown) for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Save artifacts locally\n",
    "config.artifacts.mode = ArtifactsMode.LOCAL\n",
    "config.artifacts.local_path = \"./artifacts\"\n",
    "\n",
    "# Or save to blob storage\n",
    "# config.artifacts.mode = ArtifactsMode.BLOB_STORAGE\n",
    "# config.artifacts.blob_container_name = \"artifacts\"\n",
    "\n",
    "print(\"Artifacts configuration:\")\n",
    "print(f\"  Mode: {config.artifacts.mode}\")\n",
    "print(f\"  Path: {config.artifacts.local_path}\")\n",
    "print(\"\\nüí° Artifacts include:\")\n",
    "print(\"  - Document Intelligence JSON results\")\n",
    "print(\"  - Extracted markdown\")\n",
    "print(\"  - Chunk metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Search Index Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Search service settings\n",
    "config.search.index_name = \"my-index\"\n",
    "config.search.semantic_config_name = \"my-semantic-config\"\n",
    "\n",
    "print(\"Search configuration:\")\n",
    "print(f\"  Service: {config.search.service_name}\")\n",
    "print(f\"  Index: {config.search.index_name}\")\n",
    "print(f\"  Semantic config: {config.search.semantic_config_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete Configuration Example\n",
    "\n",
    "A real-world configuration for processing medical device manuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready configuration\n",
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Input: Local PDFs\n",
    "config.input.mode = InputMode.LOCAL\n",
    "config.input.local.glob = \"medical_manuals/**/*.pdf\"\n",
    "\n",
    "# Chunking: Layout-aware with overlap\n",
    "config.chunking.mode = ChunkingMode.LAYOUT\n",
    "config.chunking.target_chunk_size = 1200\n",
    "config.chunking.chunk_overlap = 200\n",
    "config.chunking.overlap_mode = OverlapMode.WORDS\n",
    "config.chunking.table_render_mode = TableRenderMode.MARKDOWN_DETAILED\n",
    "\n",
    "# Document Intelligence: High-res OCR with formulas\n",
    "config.document_intelligence.model = \"prebuilt-layout\"\n",
    "config.document_intelligence.features = [\n",
    "    \"OCR_HIGH_RESOLUTION\",\n",
    "    \"FORMULAS\",\n",
    "    \"LANGUAGES\"\n",
    "]\n",
    "\n",
    "# Embeddings: text-embedding-3-large\n",
    "config.openai.embedding_deployment = \"text-embedding-3-large\"\n",
    "config.openai.embedding_dimensions = 1536\n",
    "\n",
    "# Artifacts: Save to blob storage\n",
    "config.artifacts.mode = ArtifactsMode.BLOB_STORAGE\n",
    "config.artifacts.blob_container_name = \"medical-artifacts\"\n",
    "\n",
    "# Search: Custom index\n",
    "config.search.index_name = \"medical-devices-index\"\n",
    "\n",
    "print(\"‚úÖ Production configuration ready\")\n",
    "print(f\"\\nConfiguration summary:\")\n",
    "print(f\"  Input: {config.input.local.glob}\")\n",
    "print(f\"  Chunking: {config.chunking.mode} ({config.chunking.target_chunk_size} chars)\")\n",
    "print(f\"  Overlap: {config.chunking.chunk_overlap} {config.chunking.overlap_mode}\")\n",
    "print(f\"  Tables: {config.chunking.table_render_mode}\")\n",
    "print(f\"  Embeddings: {config.openai.embedding_deployment}\")\n",
    "print(f\"  Index: {config.search.index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Configuration Validation\n",
    "\n",
    "Validate your configuration before running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_config(config: PipelineConfig):\n",
    "    \"\"\"Validate configuration settings.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check required fields\n",
    "    if not config.search.service_name:\n",
    "        issues.append(\"Missing search service name\")\n",
    "    if not config.search.index_name:\n",
    "        issues.append(\"Missing search index name\")\n",
    "    if not config.document_intelligence.endpoint:\n",
    "        issues.append(\"Missing Document Intelligence endpoint\")\n",
    "    if not config.openai.endpoint:\n",
    "        issues.append(\"Missing OpenAI endpoint\")\n",
    "    \n",
    "    # Check chunking settings\n",
    "    if config.chunking.chunk_overlap >= config.chunking.target_chunk_size:\n",
    "        issues.append(\"Chunk overlap must be less than chunk size\")\n",
    "    \n",
    "    if config.chunking.chunk_overlap > config.chunking.target_chunk_size * 0.3:\n",
    "        issues.append(\"Warning: Overlap is >30% of chunk size (may cause duplication)\")\n",
    "    \n",
    "    # Report\n",
    "    if issues:\n",
    "        print(\"‚ùå Configuration issues found:\\n\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"‚úÖ Configuration is valid\")\n",
    "        return True\n",
    "\n",
    "# Validate\n",
    "validate_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "‚úÖ Three methods to configure ingestor (env, programmatic, hybrid)  \n",
    "‚úÖ Input configuration (local files, blob storage)  \n",
    "‚úÖ Chunking modes and overlap settings  \n",
    "‚úÖ Document Intelligence and table processing  \n",
    "‚úÖ Embedding and search configuration  \n",
    "‚úÖ Artifacts management  \n",
    "‚úÖ Configuration validation  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **03_advanced_features.ipynb**: Advanced chunking strategies and custom processors\n",
    "- **04_real_world_legal.ipynb**: Configuration for legal documents\n",
    "- **07_performance_tuning.ipynb**: Optimize configuration for scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
