{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Example: Legal Document Processing\n",
    "\n",
    "This notebook demonstrates processing legal documents (contracts, case law, regulations) with ingestor.\n",
    "\n",
    "## Use Case\n",
    "\n",
    "Process a collection of legal documents including:\n",
    "- Contracts\n",
    "- Case law / Court decisions\n",
    "- Regulations and statutes\n",
    "- Legal briefs\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Legal documents need:\n",
    "- Precise chunking (don't split clauses/sections)\n",
    "- Preservation of numbered sections\n",
    "- Table of contents awareness\n",
    "- Citation preservation\n",
    "- Metadata extraction (document type, date, parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import Pipeline, PipelineConfig, create_config\n",
    "from ingestor.config import ChunkingMode, TableRenderMode, OverlapMode\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Legal Document Configuration\n",
    "\n",
    "Optimized settings for legal documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipelineConfig.from_env()\n",
    "\n",
    "# Input: Legal documents\n",
    "config.input.local.glob = \"legal_documents/**/*.pdf\"\n",
    "\n",
    "# Chunking: Layout-aware to preserve structure\n",
    "config.chunking.mode = ChunkingMode.LAYOUT\n",
    "config.chunking.target_chunk_size = 1500  # Larger chunks for legal context\n",
    "config.chunking.chunk_overlap = 250       # More overlap for continuity\n",
    "config.chunking.overlap_mode = OverlapMode.WORDS\n",
    "config.chunking.preserve_section_boundaries = True\n",
    "\n",
    "# Tables: Preserve structure for exhibits/schedules\n",
    "config.chunking.table_render_mode = TableRenderMode.MARKDOWN_DETAILED\n",
    "config.chunking.keep_tables_intact = True\n",
    "\n",
    "# Document Intelligence: High-res OCR for scanned documents\n",
    "config.document_intelligence.features = [\n",
    "    \"OCR_HIGH_RESOLUTION\",\n",
    "    \"LANGUAGES\"\n",
    "]\n",
    "\n",
    "# Search index\n",
    "config.search.index_name = \"legal-documents-index\"\n",
    "\n",
    "print(\"‚úÖ Legal document configuration ready\")\n",
    "print(f\"  Chunk size: {config.chunking.target_chunk_size} chars\")\n",
    "print(f\"  Overlap: {config.chunking.chunk_overlap} words\")\n",
    "print(f\"  Section preservation: {config.chunking.preserve_section_boundaries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Metadata Extraction for Legal Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_legal_metadata(filename: str, content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract legal document metadata.\n",
    "    \n",
    "    Args:\n",
    "        filename: Document filename\n",
    "        content: First page content\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metadata fields\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    # Document type from filename\n",
    "    filename_lower = filename.lower()\n",
    "    if \"contract\" in filename_lower or \"agreement\" in filename_lower:\n",
    "        metadata[\"literatureType\"] = [\"Contract\"]\n",
    "        metadata[\"category\"] = \"Contract\"\n",
    "    elif \"brief\" in filename_lower or \"motion\" in filename_lower:\n",
    "        metadata[\"literatureType\"] = [\"Legal Brief\"]\n",
    "        metadata[\"category\"] = \"Brief\"\n",
    "    elif \"regulation\" in filename_lower or \"statute\" in filename_lower:\n",
    "        metadata[\"literatureType\"] = [\"Regulation\"]\n",
    "        metadata[\"category\"] = \"Regulation\"\n",
    "    elif \"case\" in filename_lower or \"decision\" in filename_lower:\n",
    "        metadata[\"literatureType\"] = [\"Case Law\"]\n",
    "        metadata[\"category\"] = \"Case Law\"\n",
    "    \n",
    "    # Extract date patterns (MM/DD/YYYY or YYYY-MM-DD)\n",
    "    date_patterns = [\n",
    "        r'\\b(\\d{1,2})/(\\d{1,2})/(\\d{4})\\b',  # MM/DD/YYYY\n",
    "        r'\\b(\\d{4})-(\\d{2})-(\\d{2})\\b',      # YYYY-MM-DD\n",
    "        r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+(\\d{1,2}),?\\s+(\\d{4})\\b'\n",
    "    ]\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        match = re.search(pattern, content[:2000])  # Search first 2000 chars\n",
    "        if match:\n",
    "            # Store as ISO format\n",
    "            try:\n",
    "                if len(match.groups()) == 3:\n",
    "                    date_str = match.group(0)\n",
    "                    # Simple date parsing (expand for production)\n",
    "                    metadata[\"publishedDate\"] = date_str\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Extract parties (look for \"between X and Y\" patterns)\n",
    "    party_pattern = r'between\\s+([A-Z][\\w\\s,]+?)\\s+and\\s+([A-Z][\\w\\s,]+?)\\s+(\\(|,|\\.|$)'\n",
    "    party_match = re.search(party_pattern, content[:5000], re.IGNORECASE)\n",
    "    if party_match:\n",
    "        metadata[\"parties\"] = [party_match.group(1).strip(), party_match.group(2).strip()]\n",
    "    \n",
    "    # Extract jurisdiction/court\n",
    "    court_keywords = [\n",
    "        \"Supreme Court\", \"District Court\", \"Court of Appeals\",\n",
    "        \"Circuit Court\", \"Federal Court\"\n",
    "    ]\n",
    "    for keyword in court_keywords:\n",
    "        if keyword in content[:3000]:\n",
    "            metadata[\"court\"] = keyword\n",
    "            break\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Test\n",
    "sample_content = \"\"\"\n",
    "CONTRACT AGREEMENT\n",
    "Dated: January 15, 2024\n",
    "\n",
    "This Agreement is entered into between ABC Corporation and XYZ Holdings\n",
    "on the terms and conditions set forth below...\n",
    "\"\"\"\n",
    "\n",
    "metadata = extract_legal_metadata(\"service_agreement_2024.pdf\", sample_content)\n",
    "print(\"\\nExtracted metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Legal Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process legal documents\n",
    "pipeline = Pipeline(config)\n",
    "\n",
    "try:\n",
    "    print(\"üî® Processing legal documents...\\n\")\n",
    "    status = await pipeline.run()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä Processing Results:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Successful: {status.successful_documents}\")\n",
    "    print(f\"Failed: {status.failed_documents}\")\n",
    "    print(f\"Total chunks: {status.total_chunks_indexed}\")\n",
    "    print(f\"Processing time: {status.total_processing_time_seconds:.2f}s\")\n",
    "    \n",
    "    # Per-document details\n",
    "    if status.results:\n",
    "        print(\"\\nüìã Document Details:\")\n",
    "        for result in status.results:\n",
    "            if result.success:\n",
    "                print(f\"  ‚úÖ {result.filename}\")\n",
    "                print(f\"     Chunks: {result.chunks_indexed}\")\n",
    "                print(f\"     Time: {result.processing_time_seconds:.2f}s\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå {result.filename}\")\n",
    "                print(f\"     Error: {result.error_message}\")\n",
    "    \n",
    "finally:\n",
    "    await pipeline.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Legal Documents\n",
    "\n",
    "Search processed legal documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "# Create search client\n",
    "search_client = SearchClient(\n",
    "    endpoint=f\"https://{os.getenv('AZURE_SEARCH_SERVICE')}.search.windows.net\",\n",
    "    index_name=\"legal-documents-index\",\n",
    "    credential=AzureKeyCredential(os.getenv('AZURE_SEARCH_KEY'))\n",
    ")\n",
    "\n",
    "# Example query: Confidentiality clauses\n",
    "query = \"confidentiality non-disclosure obligations\"\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    top=5,\n",
    "    scoring_profile=\"contentRAGProfile\",\n",
    "    select=[\"filename\", \"title\", \"content\", \"category\", \"pageNumber\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nüîç Search Results for: '{query}'\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {result['filename']} (Page {result.get('pageNumber', 'N/A')})\")\n",
    "    print(f\"   Category: {result.get('category', 'N/A')}\")\n",
    "    if result.get('title'):\n",
    "        print(f\"   Section: {result['title']}\")\n",
    "    print(f\"   Content: {result['content'][:300]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Legal Queries\n",
    "\n",
    "### Filter by Document Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search only contracts\n",
    "results = search_client.search(\n",
    "    search_text=\"termination clause\",\n",
    "    filter=\"category eq 'Contract'\",\n",
    "    top=5\n",
    ")\n",
    "\n",
    "print(\"üîç Contracts with termination clauses:\\n\")\n",
    "for result in results:\n",
    "    print(f\"  - {result['filename']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search documents from 2024\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",\n",
    "    filter=\"publishedDate ge 2024-01-01T00:00:00Z and publishedDate lt 2025-01-01T00:00:00Z\",\n",
    "    top=10\n",
    ")\n",
    "\n",
    "print(\"üìÖ Documents from 2024:\\n\")\n",
    "for result in results:\n",
    "    print(f\"  - {result['filename']} ({result.get('publishedDate', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Search for Legal Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use semantic search for concept-based queries\n",
    "results = search_client.search(\n",
    "    search_text=\"liability limitations and indemnification\",\n",
    "    query_type=\"semantic\",\n",
    "    semantic_configuration_name=\"my-semantic-config\",\n",
    "    top=3\n",
    ")\n",
    "\n",
    "print(\"üß† Semantic search results:\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result['filename']}\")\n",
    "    print(f\"   {result['content'][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Legal RAG Pattern\n",
    "\n",
    "Use search results to answer legal questions with LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Create OpenAI client\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "def legal_rag_query(question: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Answer legal question using RAG pattern.\n",
    "    \n",
    "    Args:\n",
    "        question: Legal question\n",
    "        top_k: Number of chunks to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Answer with citations\n",
    "    \"\"\"\n",
    "    # 1. Search for relevant chunks\n",
    "    results = search_client.search(\n",
    "        search_text=question,\n",
    "        query_type=\"semantic\",\n",
    "        semantic_configuration_name=\"my-semantic-config\",\n",
    "        top=top_k,\n",
    "        select=[\"content\", \"filename\", \"pageNumber\"]\n",
    "    )\n",
    "    \n",
    "    # 2. Build context from search results\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        source = f\"{result['filename']} (Page {result.get('pageNumber', 'N/A')})\"\n",
    "        sources.append(source)\n",
    "        context_parts.append(f\"[Source {i}] {source}:\\n{result['content']}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # 3. Generate answer with LLM\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a legal research assistant. Answer questions based ONLY on the provided context.\n",
    "            Cite sources using [Source N] notation. If the context doesn't contain enough information, say so.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Question: {question}\n",
    "            \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Please answer the question based on the context above.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\", \"gpt-4\"),\n",
    "        messages=messages,\n",
    "        temperature=0.1  # Low temperature for factual answers\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources\n",
    "    }\n",
    "\n",
    "# Example query\n",
    "result = legal_rag_query(\n",
    "    \"What are the termination conditions in the service agreements?\"\n",
    ")\n",
    "\n",
    "print(f\"‚ùì Question: {result['question']}\\n\")\n",
    "print(f\"üí° Answer:\\n{result['answer']}\\n\")\n",
    "print(f\"üìö Sources:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"  [{i}] {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Legal document processing with ingestor:\n",
    "\n",
    "‚úÖ Layout-aware chunking preserves document structure  \n",
    "‚úÖ Larger chunks maintain legal context  \n",
    "‚úÖ Custom metadata extraction for document type, dates, parties  \n",
    "‚úÖ Advanced filtering by category, date, jurisdiction  \n",
    "‚úÖ Semantic search for legal concepts  \n",
    "‚úÖ RAG pattern for answering legal questions  \n",
    "\n",
    "## Best Practices for Legal Documents\n",
    "\n",
    "1. **Chunk size**: Use 1500-2000 chars to preserve clause context\n",
    "2. **Overlap**: 15-20% overlap to avoid splitting clauses\n",
    "3. **Tables**: Keep intact for schedules and exhibits\n",
    "4. **Metadata**: Extract document type, date, parties, jurisdiction\n",
    "5. **Search**: Use semantic ranking for conceptual queries\n",
    "6. **RAG**: Always cite sources in answers\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **05_real_world_medical.ipynb**: Medical device manual processing\n",
    "- **06_troubleshooting.ipynb**: Debug common issues\n",
    "- **07_performance_tuning.ipynb**: Optimize for large document sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
