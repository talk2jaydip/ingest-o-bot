{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Ingestor Quick Playbook - All Usage Methods\n",
    "\n",
    "This notebook shows **ALL** the different ways to run the ingestor library:\n",
    "\n",
    "## Methods Covered\n",
    "\n",
    "1. **üêç Python API** - Import and use programmatically\n",
    "2. **üíª CLI** - Command-line interface\n",
    "3. **üñ•Ô∏è UI (Gradio)** - Web interface\n",
    "4. **üì¶ Batch Processing** - Multiple documents\n",
    "5. **‚öôÔ∏è Configuration** - All the ways to configure\n",
    "\n",
    "## Quick Links\n",
    "\n",
    "- [Method 1: Python API (Programmatic)](#method-1-python-api)\n",
    "- [Method 2: CLI (Command Line)](#method-2-cli)\n",
    "- [Method 3: UI (Gradio Web Interface)](#method-3-ui)\n",
    "- [Common Scenarios](#common-scenarios)\n",
    "- [Configuration Guide](#configuration-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Setup: Load Environment\n",
    "\n",
    "All methods need Azure credentials. Set them up once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "env_path = Path(\"../../.env\")\n",
    "if env_path.exists():\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "    print(f\"‚úÖ Loaded environment from: {env_path.absolute()}\")\n",
    "else:\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ Loaded environment from default location\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\nAzure Search Service: {os.getenv('AZURE_SEARCH_SERVICE', 'NOT SET')}\")\n",
    "print(f\"Azure Search Index: {os.getenv('AZURE_SEARCH_INDEX', 'NOT SET')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Method 1: Python API (Programmatic)\n",
    "\n",
    "Use the ingestor as a Python library in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Simple One-Liner (Fastest)\n",
    "\n",
    "Process a document with one function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import run_pipeline\n",
    "\n",
    "# Process a single document\n",
    "status = await run_pipeline(\n",
    "    input_glob=\"../../samples/sample_pages_test.pdf\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Processed {status.successful_documents} document(s)\")\n",
    "print(f\"üìÑ Indexed {status.total_chunks_indexed} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: With Performance Optimizations\n",
    "\n",
    "Enable parallel processing for faster execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import run_pipeline\n",
    "\n",
    "# Optimized processing\n",
    "status = await run_pipeline(\n",
    "    input_glob=\"../../samples/*.pdf\",\n",
    "    \n",
    "    # Performance settings\n",
    "    performance_max_workers=4,              # Process 4 docs in parallel\n",
    "    azure_openai_max_concurrency=10,        # Parallel embedding batches\n",
    "    azure_di_max_concurrency=5,             # Parallel DI requests\n",
    "    use_integrated_vectorization=True       # Server-side embeddings (fastest!)\n",
    ")\n",
    "\n",
    "print(f\"üöÄ Optimized processing complete!\")\n",
    "print(f\"Documents: {status.successful_documents}\")\n",
    "print(f\"Chunks: {status.total_chunks_indexed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Using create_config() Helper\n",
    "\n",
    "Create a configuration object for more control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import create_config, Pipeline\n",
    "\n",
    "# Create config\n",
    "config = create_config(\n",
    "    input_glob=\"../../samples/*.pdf\",\n",
    "    azure_search_index=\"my-custom-index\",\n",
    "    \n",
    "    # Chunking settings\n",
    "    chunking_max_tokens=1000,\n",
    "    chunking_overlap_percent=15,\n",
    "    \n",
    "    # Performance\n",
    "    performance_max_workers=4,\n",
    "    use_integrated_vectorization=True\n",
    ")\n",
    "\n",
    "# Run pipeline\n",
    "pipeline = Pipeline(config)\n",
    "try:\n",
    "    status = await pipeline.run()\n",
    "    print(f\"‚úÖ Custom config processing complete!\")\n",
    "    print(f\"Chunks: {status.total_chunks_indexed}\")\n",
    "finally:\n",
    "    await pipeline.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4: Using ConfigBuilder (Fluent API)\n",
    "\n",
    "Build configuration programmatically with chainable methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import ConfigBuilder, Pipeline\n",
    "\n",
    "# Build config with fluent API\n",
    "config = (\n",
    "    ConfigBuilder()\n",
    "    .with_search(\n",
    "        service_name=\"your-service\",\n",
    "        index_name=\"documents-index\"\n",
    "    )\n",
    "    .with_document_intelligence(\n",
    "        endpoint=os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\"),\n",
    "        max_concurrency=5\n",
    "    )\n",
    "    .with_azure_openai(\n",
    "        endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        embedding_deployment=\"text-embedding-ada-002\",\n",
    "        max_concurrency=10\n",
    "    )\n",
    "    .with_input(\n",
    "        mode=\"local\",\n",
    "        local_glob=\"../../samples/*.pdf\"\n",
    "    )\n",
    "    .with_performance(\n",
    "        max_workers=4,\n",
    "        use_integrated_vectorization=True\n",
    "    )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Run pipeline\n",
    "pipeline = Pipeline(config)\n",
    "try:\n",
    "    status = await pipeline.run()\n",
    "    print(f\"‚úÖ ConfigBuilder processing complete!\")\n",
    "    print(f\"Documents: {status.successful_documents}\")\n",
    "finally:\n",
    "    await pipeline.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5: From Environment Variables\n",
    "\n",
    "Load all configuration from `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import PipelineConfig, Pipeline\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Create config from environment\n",
    "config = PipelineConfig.from_env()\n",
    "\n",
    "print(f\"Configuration loaded from environment:\")\n",
    "print(f\"  Index: {config.search.index_name}\")\n",
    "print(f\"  Max workers: {config.performance.max_workers}\")\n",
    "print(f\"  Integrated vectorization: {config.use_integrated_vectorization}\")\n",
    "\n",
    "# Run pipeline\n",
    "pipeline = Pipeline(config)\n",
    "try:\n",
    "    status = await pipeline.run()\n",
    "    print(f\"\\n‚úÖ Environment-based processing complete!\")\n",
    "    print(f\"Documents: {status.successful_documents}\")\n",
    "finally:\n",
    "    await pipeline.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Method 2: CLI (Command Line Interface)\n",
    "\n",
    "Run the ingestor from the command line without writing Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Basic CLI Usage\n",
    "\n",
    "Process a document from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run from terminal:\n",
    "# python -m ingestor.cli --input \"samples/*.pdf\"\n",
    "\n",
    "# Or use the convenience wrapper:\n",
    "!python ../../examples/scripts/run_cli.py --input \"../../samples/sample_pages_test.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: CLI with All Options\n",
    "\n",
    "Full CLI command with all available options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full CLI command example (run in terminal)\n",
    "\"\"\"\n",
    "python -m ingestor.cli \\\n",
    "  --input \"documents/**/*.pdf\" \\\n",
    "  --env-path \".env\" \\\n",
    "  --search-index \"documents-index\" \\\n",
    "  --max-workers 4 \\\n",
    "  --openai-concurrency 10 \\\n",
    "  --di-concurrency 5 \\\n",
    "  --integrated-vectorization \\\n",
    "  --chunking-max-tokens 1000 \\\n",
    "  --action add\n",
    "\"\"\"\n",
    "\n",
    "print(\"Copy the above command to your terminal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: CLI Common Commands\n",
    "\n",
    "Common CLI patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Process single document ===\n",
    "# python -m ingestor.cli --input \"document.pdf\"\n",
    "\n",
    "# === Process all PDFs in directory ===\n",
    "# python -m ingestor.cli --input \"documents/*.pdf\"\n",
    "\n",
    "# === Process recursively ===\n",
    "# python -m ingestor.cli --input \"documents/**/*.pdf\"\n",
    "\n",
    "# === With custom index ===\n",
    "# python -m ingestor.cli --input \"docs/*.pdf\" --search-index \"my-index\"\n",
    "\n",
    "# === With performance optimizations ===\n",
    "# python -m ingestor.cli --input \"docs/*.pdf\" --max-workers 4 --integrated-vectorization\n",
    "\n",
    "# === Remove documents ===\n",
    "# python -m ingestor.cli --input \"document.pdf\" --action remove\n",
    "\n",
    "# === Remove all documents ===\n",
    "# python -m ingestor.cli --action remove-all\n",
    "\n",
    "print(\"CLI command examples above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4: CLI Environment Files\n",
    "\n",
    "Use different environment files with CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Use default .env ===\n",
    "# python -m ingestor.cli --input \"docs/*.pdf\"\n",
    "\n",
    "# === Use custom env file ===\n",
    "# python -m ingestor.cli --input \"docs/*.pdf\" --env-path \".env.production\"\n",
    "\n",
    "# === Use development settings ===\n",
    "# python -m ingestor.cli --input \"docs/*.pdf\" --env-path \".env.development\"\n",
    "\n",
    "# === Override specific settings ===\n",
    "# python -m ingestor.cli --input \"docs/*.pdf\" --env-path \".env\" --max-workers 8\n",
    "\n",
    "print(\"Environment file examples above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Method 3: UI (Gradio Web Interface)\n",
    "\n",
    "Use the web-based UI for interactive document processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Launch Gradio UI\n",
    "\n",
    "Start the web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch UI from terminal:\n",
    "# python -m ingestor.gradio_app\n",
    "\n",
    "# Or with custom port:\n",
    "# python -m ingestor.gradio_app --port 7860\n",
    "\n",
    "# Or launch from Python:\n",
    "# from ingestor.gradio_app import create_interface\n",
    "# demo = create_interface()\n",
    "# demo.launch()\n",
    "\n",
    "print(\"\"\"\\nTo launch the UI:\n",
    "\n",
    "1. Open a terminal\n",
    "2. Run: python -m ingestor.gradio_app\n",
    "3. Open browser to: http://localhost:7860\n",
    "\n",
    "The UI will let you:\n",
    "- Upload documents via drag-and-drop\n",
    "- Configure all settings visually\n",
    "- Monitor processing progress\n",
    "- View results in real-time\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: UI Features\n",
    "\n",
    "What you can do in the Gradio UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio UI Features:\n",
    "\n",
    "ui_features = \"\"\"\n",
    "üì§ Upload Documents:\n",
    "   - Drag and drop files\n",
    "   - Supports: PDF, DOCX, PPTX, TXT, MD, CSV, JSON\n",
    "   - Multiple file upload\n",
    "\n",
    "‚öôÔ∏è Configure Settings:\n",
    "   - Search service and index\n",
    "   - Document action (add/remove/remove-all)\n",
    "   - Chunking parameters\n",
    "   - Performance settings\n",
    "   - Concurrency limits\n",
    "\n",
    "üîÑ Process Documents:\n",
    "   - Real-time progress updates\n",
    "   - Per-document status\n",
    "   - Error messages\n",
    "\n",
    "üìä View Results:\n",
    "   - Success/failure counts\n",
    "   - Chunks indexed\n",
    "   - Processing times\n",
    "   - Detailed logs\n",
    "\n",
    "üíæ Load/Save Configs:\n",
    "   - Load from .env file\n",
    "   - Save current settings\n",
    "   - Quick presets\n",
    "\"\"\"\n",
    "\n",
    "print(ui_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: UI with Custom Configuration\n",
    "\n",
    "Launch UI with pre-configured settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch UI with custom config (in Python script):\n",
    "\n",
    "from ingestor.gradio_app import create_interface\n",
    "from ingestor import create_config\n",
    "\n",
    "# Create default config\n",
    "config = create_config(\n",
    "    azure_search_index=\"my-custom-index\",\n",
    "    performance_max_workers=4,\n",
    "    use_integrated_vectorization=True\n",
    ")\n",
    "\n",
    "# Launch UI with this config as default\n",
    "# demo = create_interface(default_config=config)\n",
    "# demo.launch()\n",
    "\n",
    "print(\"UI will launch with custom default settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Common Scenarios\n",
    "\n",
    "Real-world examples for common use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: One-Time Document Ingestion\n",
    "\n",
    "Process a single document quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import run_pipeline\n",
    "\n",
    "# Simplest way - just specify the file\n",
    "status = await run_pipeline(input_glob=\"../../samples/sample_pages_test.pdf\")\n",
    "\n",
    "print(f\"‚úÖ Done! Indexed {status.total_chunks_indexed} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: Batch Processing Multiple Documents\n",
    "\n",
    "Process an entire directory with optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import run_pipeline\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Process all documents in parallel\n",
    "status = await run_pipeline(\n",
    "    input_glob=\"../../documents/**/*.pdf\",\n",
    "    performance_max_workers=4,              # 4 docs in parallel\n",
    "    azure_openai_max_concurrency=10,        # Fast embeddings\n",
    "    use_integrated_vectorization=True       # Server-side (fastest)\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nüìä Batch Results:\")\n",
    "print(f\"Documents: {status.successful_documents} succeeded, {status.failed_documents} failed\")\n",
    "print(f\"Chunks: {status.total_chunks_indexed}\")\n",
    "print(f\"Time: {elapsed:.2f}s\")\n",
    "print(f\"Throughput: {status.total_chunks_indexed / elapsed:.2f} chunks/sec\")\n",
    "\n",
    "# Show per-document times\n",
    "print(f\"\\nPer-document breakdown:\")\n",
    "for result in status.results:\n",
    "    icon = \"‚úÖ\" if result.success else \"‚ùå\"\n",
    "    print(f\"  {icon} {result.filename}: {result.processing_time_seconds:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 3: Update Existing Documents\n",
    "\n",
    "Re-process documents that are already indexed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import run_pipeline\n",
    "\n",
    "# The pipeline automatically deletes old chunks before adding new ones\n",
    "# This ensures a clean update (no duplicates)\n",
    "status = await run_pipeline(\n",
    "    input_glob=\"../../samples/updated_document.pdf\",\n",
    "    document_action=\"add\"  # Default: deletes old + adds new\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Document updated successfully\")\n",
    "print(f\"   New chunks indexed: {status.total_chunks_indexed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 4: Remove Documents from Index\n",
    "\n",
    "Delete specific documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import run_pipeline\n",
    "\n",
    "# Remove specific document(s)\n",
    "status = await run_pipeline(\n",
    "    input_glob=\"../../samples/old_document.pdf\",\n",
    "    document_action=\"remove\"  # Only remove, don't add\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Documents removed from index\")\n",
    "\n",
    "# Remove ALL documents (be careful!)\n",
    "# status = await run_pipeline(document_action=\"remove_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 5: Different Environments (Dev/Staging/Prod)\n",
    "\n",
    "Use different configurations for different environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from ingestor import PipelineConfig, Pipeline\n",
    "\n",
    "# Select environment\n",
    "environment = \"development\"  # or \"staging\", \"production\"\n",
    "\n",
    "# Load appropriate .env file\n",
    "env_file = f\"../../.env.{environment}\"\n",
    "load_dotenv(dotenv_path=env_file, override=True)\n",
    "\n",
    "# Create config from environment\n",
    "config = PipelineConfig.from_env()\n",
    "\n",
    "print(f\"Configuration for: {environment}\")\n",
    "print(f\"  Index: {config.search.index_name}\")\n",
    "print(f\"  Max workers: {config.performance.max_workers}\")\n",
    "\n",
    "# Process with environment-specific config\n",
    "pipeline = Pipeline(config)\n",
    "try:\n",
    "    status = await pipeline.run()\n",
    "    print(f\"\\n‚úÖ Processed in {environment} environment\")\n",
    "finally:\n",
    "    await pipeline.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 6: Custom Chunking Strategy\n",
    "\n",
    "Fine-tune how documents are split into chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import run_pipeline\n",
    "\n",
    "# Custom chunking for long-form content\n",
    "status = await run_pipeline(\n",
    "    input_glob=\"../../samples/long_document.pdf\",\n",
    "    \n",
    "    # Chunking configuration\n",
    "    chunking_max_tokens=1500,       # Larger chunks\n",
    "    chunking_overlap_percent=20,     # More overlap for context\n",
    "    chunking_cross_page_overlap=True # Preserve context across pages\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Processed with custom chunking\")\n",
    "print(f\"   Chunks: {status.total_chunks_indexed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 7: Process Office Documents\n",
    "\n",
    "Handle DOCX and PPTX files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import run_pipeline\n",
    "\n",
    "# Process Office documents (DOCX, PPTX)\n",
    "status = await run_pipeline(\n",
    "    input_glob=\"../../documents/**/*.{docx,pptx}\",\n",
    "    \n",
    "    # Office-specific settings\n",
    "    office_extractor_mode=\"hybrid\",          # Try Azure DI first, fallback to offline\n",
    "    office_extractor_offline_fallback=True   # Enable fallback\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Processed Office documents\")\n",
    "print(f\"   Documents: {status.successful_documents}\")\n",
    "print(f\"   Chunks: {status.total_chunks_indexed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 8: Monitoring and Error Handling\n",
    "\n",
    "Track progress and handle failures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import run_pipeline\n",
    "import time\n",
    "\n",
    "print(\"üîÑ Starting batch processing...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "try:\n",
    "    status = await run_pipeline(\n",
    "        input_glob=\"../../documents/**/*.pdf\",\n",
    "        performance_max_workers=4\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Success summary\n",
    "    print(f\"\\n‚úÖ Processing complete in {elapsed:.2f}s\")\n",
    "    print(f\"   Success: {status.successful_documents}\")\n",
    "    print(f\"   Failed: {status.failed_documents}\")\n",
    "    print(f\"   Chunks: {status.total_chunks_indexed}\")\n",
    "    \n",
    "    # Handle failures\n",
    "    if status.failed_documents > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Failed documents:\")\n",
    "        for result in status.results:\n",
    "            if not result.success:\n",
    "                print(f\"   ‚ùå {result.filename}\")\n",
    "                print(f\"      Error: {result.error_message}\")\n",
    "        \n",
    "        # Retry failed documents with conservative settings\n",
    "        print(f\"\\nüîÑ Retrying failed documents...\")\n",
    "        failed_files = [r.filename for r in status.results if not r.success]\n",
    "        \n",
    "        for file in failed_files:\n",
    "            try:\n",
    "                retry_status = await run_pipeline(\n",
    "                    input_glob=file,\n",
    "                    performance_max_workers=1,\n",
    "                    azure_openai_max_concurrency=3\n",
    "                )\n",
    "                if retry_status.successful_documents > 0:\n",
    "                    print(f\"   ‚úÖ Retry succeeded: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Retry failed: {file}: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Pipeline error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Configuration Guide\n",
    "\n",
    "All the ways to configure the ingestor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# === Azure Services ===\nAZURE_SEARCH_SERVICE=your-service\nAZURE_SEARCH_INDEX=documents-index\nAZURE_SEARCH_KEY=your-key\n\nAZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://your-di.cognitiveservices.azure.com/\nAZURE_DOCUMENT_INTELLIGENCE_KEY=your-key\n\nAZURE_OPENAI_ENDPOINT=https://your-openai.openai.azure.com\nAZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002\nAZURE_OPENAI_KEY=your-key\n\n# === Performance Optimizations ===\nMAX_WORKERS=4\nAZURE_OPENAI_MAX_CONCURRENCY=10\nAZURE_DI_MAX_CONCURRENCY=5\nAZURE_USE_INTEGRATED_VECTORIZATION=true\n\n# === Input/Output ===\nINPUT_MODE=local\nLOCAL_GLOB=documents/**/*.pdf\nARTIFACTS_MODE=blob\n\n# === Chunking ===\nCHUNKING_MAX_TOKENS=1000\nCHUNKING_OVERLAP_PERCENT=15\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example .env file\n\nenv_template = \"\"\"\n# === Azure Services ===\nAZURE_SEARCH_SERVICE=your-service\nAZURE_SEARCH_INDEX=documents-index\nAZURE_SEARCH_KEY=your-key\n\nAZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://your-di.cognitiveservices.azure.com/\nAZURE_DOCUMENT_INTELLIGENCE_KEY=your-key\n\nAZURE_OPENAI_ENDPOINT=https://your-openai.openai.azure.com\nAZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002\nAZURE_OPENAI_KEY=your-key\n\n# === Performance Optimizations ===\nMAX_WORKERS=4\nAZURE_OPENAI_MAX_CONCURRENCY=10\nAZURE_DI_MAX_CONCURRENCY=5\nAZURE_USE_INTEGRATED_VECTORIZATION=true\n\n# === Input/Output ===\nINPUT_MODE=local\nLOCAL_GLOB=documents/**/*.pdf\nARTIFACTS_MODE=blob\n\n# === Chunking ===\nCHUNKING_MAX_TOKENS=1000\nCHUNKING_OVERLAP_PERCENT=15\n\"\"\"\n\nprint(\"Save this as .env file:\")\nprint(env_template)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Method 2: Programmatic (Python)\n",
    "\n",
    "Set configuration in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import run_pipeline\n",
    "\n",
    "# All settings as function parameters\n",
    "status = await run_pipeline(\n",
    "    # Input\n",
    "    input_glob=\"../../documents/*.pdf\",\n",
    "    \n",
    "    # Azure services\n",
    "    azure_search_service=\"your-service\",\n",
    "    azure_search_index=\"documents-index\",\n",
    "    azure_search_key=\"your-key\",\n",
    "    \n",
    "    # Performance\n",
    "    performance_max_workers=4,\n",
    "    azure_openai_max_concurrency=10,\n",
    "    azure_di_max_concurrency=5,\n",
    "    use_integrated_vectorization=True,\n",
    "    \n",
    "    # Chunking\n",
    "    chunking_max_tokens=1000,\n",
    "    chunking_overlap_percent=15,\n",
    "    \n",
    "    # Action\n",
    "    document_action=\"add\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Configured programmatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Method 3: Hybrid (Env + Overrides)\n",
    "\n",
    "Load from `.env` and override specific values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import create_config\n",
    "\n",
    "# Load from .env and override specific settings\n",
    "config = create_config(\n",
    "    env_path=\"../../.env\",               # Load from .env\n",
    "    use_env=True,                         # Enable env loading\n",
    "    \n",
    "    # Override specific settings\n",
    "    azure_search_index=\"custom-index\",   # Override index\n",
    "    performance_max_workers=8,            # Override max_workers\n",
    "    chunking_max_tokens=1500              # Override chunking\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Hybrid configuration created\")\n",
    "print(f\"   Index: {config.search.index_name}\")\n",
    "print(f\"   Max workers: {config.performance.max_workers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Quick Reference\n",
    "\n",
    "Cheat sheet for common operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python API Quick Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_reference = \"\"\"\n",
    "# === Simple Usage ===\n",
    "from ingestor import run_pipeline\n",
    "status = await run_pipeline(input_glob=\"docs/*.pdf\")\n",
    "\n",
    "# === With Optimizations ===\n",
    "status = await run_pipeline(\n",
    "    input_glob=\"docs/*.pdf\",\n",
    "    performance_max_workers=4,\n",
    "    azure_openai_max_concurrency=10,\n",
    "    use_integrated_vectorization=True\n",
    ")\n",
    "\n",
    "# === From Environment ===\n",
    "from ingestor import PipelineConfig, Pipeline\n",
    "config = PipelineConfig.from_env()\n",
    "pipeline = Pipeline(config)\n",
    "status = await pipeline.run()\n",
    "await pipeline.close()\n",
    "\n",
    "# === ConfigBuilder ===\n",
    "from ingestor import ConfigBuilder\n",
    "config = ConfigBuilder().with_search(...).with_input(...).build()\n",
    "\"\"\"\n",
    "\n",
    "print(python_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI Quick Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_reference = \"\"\"\n",
    "# === Basic ===\n",
    "python -m ingestor.cli --input \"docs/*.pdf\"\n",
    "\n",
    "# === With Options ===\n",
    "python -m ingestor.cli \\\n",
    "  --input \"docs/**/*.pdf\" \\\n",
    "  --max-workers 4 \\\n",
    "  --integrated-vectorization\n",
    "\n",
    "# === Custom Env ===\n",
    "python -m ingestor.cli \\\n",
    "  --input \"docs/*.pdf\" \\\n",
    "  --env-path \".env.production\"\n",
    "\n",
    "# === Remove ===\n",
    "python -m ingestor.cli --input \"doc.pdf\" --action remove\n",
    "python -m ingestor.cli --action remove-all\n",
    "\"\"\"\n",
    "\n",
    "print(cli_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UI Quick Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_reference = \"\"\"\n",
    "# === Launch UI ===\n",
    "python -m ingestor.gradio_app\n",
    "\n",
    "# === Custom Port ===\n",
    "python -m ingestor.gradio_app --port 7860\n",
    "\n",
    "# === From Python ===\n",
    "from ingestor.gradio_app import create_interface\n",
    "demo = create_interface()\n",
    "demo.launch()\n",
    "\n",
    "# Then open: http://localhost:7860\n",
    "\"\"\"\n",
    "\n",
    "print(ui_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "You now know **ALL** the ways to use the ingestor:\n",
    "\n",
    "## Methods\n",
    "\n",
    "‚úÖ **Python API** - Import and use programmatically  \n",
    "‚úÖ **CLI** - Command-line interface for scripts  \n",
    "‚úÖ **UI** - Gradio web interface for interactive use  \n",
    "\n",
    "## Configuration\n",
    "\n",
    "‚úÖ **.env files** - Recommended for production  \n",
    "‚úÖ **Programmatic** - Set in Python code  \n",
    "‚úÖ **ConfigBuilder** - Fluent API for building config  \n",
    "‚úÖ **Hybrid** - Mix environment + overrides  \n",
    "\n",
    "## Features\n",
    "\n",
    "‚úÖ **Parallel processing** - 75-85% faster  \n",
    "‚úÖ **Batch operations** - Multiple documents  \n",
    "‚úÖ **Error handling** - Graceful failures + retry  \n",
    "‚úÖ **Progress tracking** - Real-time monitoring  \n",
    "‚úÖ **Multiple environments** - Dev/staging/prod  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **[01_quickstart.ipynb](01_quickstart.ipynb)** - Get started quickly\n",
    "- **[08_batch_processing.ipynb](08_batch_processing.ipynb)** - Batch processing deep dive\n",
    "- **[07_performance_tuning.ipynb](07_performance_tuning.ipynb)** - Optimization guide\n",
    "- **[QUICK_START_OPTIMIZATIONS.md](../../QUICK_START_OPTIMIZATIONS.md)** - Performance tips"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}