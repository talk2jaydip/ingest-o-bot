{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestor Quick Start Guide\n",
    "\n",
    "This notebook demonstrates the basics of using the ingestor library to process documents and index them into Azure AI Search.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Installing and importing ingestor\n",
    "- Processing your first document\n",
    "- Using convenience functions\n",
    "- Viewing results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Azure AI Search service\n",
    "- Azure Document Intelligence service\n",
    "- Azure OpenAI service (for embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installation\n",
    "\n",
    "If you haven't installed ingestor yet, run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install ingestor (uncomment if needed)\n",
    "# !pip install -e ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Environment Variables\n",
    "\n",
    "Create a `.env` file in the project root with your Azure credentials, or set them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\n\n# Option 1: Load from .env file (RECOMMENDED)\nfrom dotenv import load_dotenv\n\n# Load .env from project root or specify custom path\nenv_path = Path(\"../../.env\")  # Adjust path as needed\nif env_path.exists():\n    load_dotenv(dotenv_path=env_path)\n    print(f\"‚úÖ Loaded environment from: {env_path.absolute()}\")\nelse:\n    load_dotenv()  # Try default locations\n    print(\"‚úÖ Loaded environment from default location\")\n\n# Option 2: Load environment-specific .env files\n# load_dotenv(dotenv_path=\"../../.env.production\")\n# load_dotenv(dotenv_path=\"../../.env.development\")\n\n# Verify critical variables are set\nrequired_vars = [\n    \"AZURE_SEARCH_SERVICE\",\n    \"AZURE_SEARCH_INDEX\",\n    \"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\",\n    \"AZURE_OPENAI_ENDPOINT\"\n]\n\nmissing_vars = [var for var in required_vars if not os.getenv(var)]\nif missing_vars:\n    print(f\"\\n‚ö†Ô∏è  Warning: Missing environment variables: {missing_vars}\")\n    print(\"   Set them in your .env file or manually below\")\nelse:\n    print(f\"‚úÖ All required environment variables are set\")\n\n# Option 3: Set manually (for testing only - never commit credentials!)\n# os.environ[\"AZURE_SEARCH_SERVICE\"] = \"your-service\"\n# os.environ[\"AZURE_SEARCH_KEY\"] = \"your-key\"\n# os.environ[\"AZURE_SEARCH_INDEX\"] = \"documents-index\"\n# os.environ[\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\"] = \"https://your-di.cognitiveservices.azure.com/\"\n# os.environ[\"AZURE_DOCUMENT_INTELLIGENCE_KEY\"] = \"your-key\"\n# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://your-openai.openai.azure.com\"\n# os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"] = \"text-embedding-ada-002\"\n# os.environ[\"AZURE_OPENAI_KEY\"] = \"your-key\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Ingestor\n",
    "\n",
    "Import the main functions you'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import run_pipeline, create_config\n",
    "from ingestor.config import InputMode\n",
    "\n",
    "print(\"‚úÖ Ingestor imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process Your First Document\n",
    "\n",
    "The simplest way to process documents is using the `run_pipeline` convenience function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single PDF file\n",
    "status = await run_pipeline(\n",
    "    input_glob=\"../../sample_documents/*.pdf\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Processing complete!\")\n",
    "print(f\"Documents processed: {status.successful_documents}\")\n",
    "print(f\"Chunks indexed: {status.total_chunks_indexed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: View Detailed Results\n",
    "\n",
    "Inspect per-document results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_data = []\n",
    "for result in status.results:\n",
    "    results_data.append({\n",
    "        \"Filename\": result.filename,\n",
    "        \"Success\": \"‚úÖ\" if result.success else \"‚ùå\",\n",
    "        \"Chunks\": result.chunks_indexed,\n",
    "        \"Duration (s)\": f\"{result.processing_time_seconds:.2f}\",\n",
    "        \"Error\": result.error_message or \"-\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Process Multiple Documents\n",
    "\n",
    "Process an entire directory of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process all PDFs in a directory WITH PARALLEL PROCESSING (NEW!)\n# This processes multiple documents concurrently for maximum throughput\n\nstatus = await run_pipeline(\n    input_glob=\"../../documents/**/*.pdf\",  # Recursive glob\n    # Performance optimizations (NEW in v4.0)\n    performance_max_workers=4,              # Process 4 docs in parallel\n    azure_openai_max_concurrency=10,        # Parallel embedding batches\n    azure_di_max_concurrency=5,             # Parallel DI requests\n    use_integrated_vectorization=True       # Server-side embeddings (fastest!)\n)\n\nprint(f\"\\nüìä Batch Processing Results:\")\nprint(f\"Successful: {status.successful_documents}\")\nprint(f\"Failed: {status.failed_documents}\")\nprint(f\"Total chunks: {status.total_chunks_indexed}\")\n\n# Show per-document processing times\nprint(f\"\\nPer-document results:\")\nfor result in status.results:\n    status_icon = \"‚úÖ\" if result.success else \"‚ùå\"\n    print(f\"  {status_icon} {result.filename}: {result.processing_time_seconds:.2f}s ({result.chunks_indexed} chunks)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Using Custom Configuration\n",
    "\n",
    "For more control, create a custom configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom config\n",
    "config = create_config(\n",
    "    input_glob=\"../../sample_documents/*.pdf\",\n",
    "    azure_search_index=\"my-custom-index\"\n",
    ")\n",
    "\n",
    "# Customize chunking settings\n",
    "config.chunking.target_chunk_size = 1000\n",
    "config.chunking.chunk_overlap = 200\n",
    "\n",
    "# Run with custom config\n",
    "status = await run_pipeline(config=config)\n",
    "\n",
    "print(f\"‚úÖ Processed with custom settings\")\n",
    "print(f\"Chunks indexed: {status.total_chunks_indexed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verify in Azure Search\n",
    "\n",
    "Query the index to verify documents were indexed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Create search client\n",
    "search_service = os.getenv(\"AZURE_SEARCH_SERVICE\")\n",
    "search_key = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX\", \"documents-index\")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=f\"https://{search_service}.search.windows.net\",\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(search_key)\n",
    ")\n",
    "\n",
    "# Get document count\n",
    "results = search_client.search(search_text=\"*\", top=0, include_total_count=True)\n",
    "print(f\"\\nüìä Index Statistics:\")\n",
    "print(f\"Total documents in index: {results.get_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Perform a Test Search\n",
    "\n",
    "Search the indexed documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for content\n",
    "results = search_client.search(\n",
    "    search_text=\"your search query here\",\n",
    "    top=5,\n",
    "    select=[\"id\", \"filename\", \"title\", \"content\", \"pageNumber\"]\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Search Results:\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result['filename']} (Page {result.get('pageNumber', 'N/A')})\")\n",
    "    print(f\"   Title: {result.get('title', 'N/A')}\")\n",
    "    print(f\"   Content preview: {result['content'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully:\n",
    "\n",
    "‚úÖ Installed and imported ingestor  \n",
    "‚úÖ Processed documents using the convenience function  \n",
    "‚úÖ Viewed processing results  \n",
    "‚úÖ Used custom configuration  \n",
    "‚úÖ Verified documents in Azure Search  \n",
    "‚úÖ Performed test searches  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **02_configuration.ipynb**: Learn about all configuration options\n",
    "- **03_advanced_features.ipynb**: Explore advanced features like chunking strategies\n",
    "- **06_troubleshooting.ipynb**: Debug common issues\n",
    "- **07_performance_tuning.ipynb**: Optimize for large-scale processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}