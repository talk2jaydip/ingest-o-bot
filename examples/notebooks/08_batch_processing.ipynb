{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing Guide - Parallel Document Ingestion\n",
    "\n",
    "This notebook demonstrates how to process multiple documents efficiently using parallel processing optimizations introduced in v4.0.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- ‚úÖ Batch processing multiple documents in parallel\n",
    "- ‚úÖ Performance optimization settings\n",
    "- ‚úÖ Environment variable configuration\n",
    "- ‚úÖ Monitoring batch progress\n",
    "- ‚úÖ Error handling in batch operations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Azure services configured (Search, Document Intelligence, OpenAI)\n",
    "- Multiple documents to process (PDFs, DOCX, PPTX)\n",
    "- `.env` file with credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Load environment variables from `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "env_path = Path(\"../../.env\")\n",
    "if env_path.exists():\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "    print(f\"‚úÖ Loaded environment from: {env_path.absolute()}\")\n",
    "else:\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ Loaded environment from default location\")\n",
    "\n",
    "# Verify required variables\n",
    "required_vars = [\n",
    "    \"AZURE_SEARCH_SERVICE\",\n",
    "    \"AZURE_SEARCH_INDEX\",\n",
    "    \"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\"\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "if missing_vars:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing: {missing_vars}\")\n",
    "else:\n",
    "    print(f\"‚úÖ All required variables set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Ingestor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestor import run_pipeline, create_config, Pipeline\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ Ingestor imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Basic Batch Processing\n",
    "\n",
    "Process multiple documents with default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all PDFs in a directory\n",
    "start = time.time()\n",
    "\n",
    "status = await run_pipeline(\n",
    "    input_glob=\"../../documents/**/*.pdf\"  # Adjust path as needed\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nüìä Batch Processing Results:\")\n",
    "print(f\"Documents processed: {status.successful_documents}\")\n",
    "print(f\"Documents failed: {status.failed_documents}\")\n",
    "print(f\"Total chunks: {status.total_chunks_indexed}\")\n",
    "print(f\"Total time: {elapsed:.2f}s\")\n",
    "print(f\"Avg time per document: {elapsed / status.successful_documents:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Optimized Batch Processing (RECOMMENDED)\n",
    "\n",
    "Enable parallel processing for maximum throughput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized batch processing with parallel execution\n",
    "start = time.time()\n",
    "\n",
    "status = await run_pipeline(\n",
    "    input_glob=\"../../documents/**/*.pdf\",\n",
    "    \n",
    "    # === PARALLEL PROCESSING SETTINGS ===\n",
    "    performance_max_workers=4,          # Process 4 documents in parallel\n",
    "    \n",
    "    # === CONCURRENCY SETTINGS ===\n",
    "    azure_openai_max_concurrency=10,    # Parallel embedding batches\n",
    "    azure_di_max_concurrency=5,         # Parallel DI requests\n",
    "    \n",
    "    # === OPTIMIZATION FLAGS ===\n",
    "    use_integrated_vectorization=True   # Server-side embeddings (fastest!)\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nüöÄ OPTIMIZED Batch Processing Results:\")\n",
    "print(f\"Documents processed: {status.successful_documents}\")\n",
    "print(f\"Documents failed: {status.failed_documents}\")\n",
    "print(f\"Total chunks: {status.total_chunks_indexed}\")\n",
    "print(f\"Total time: {elapsed:.2f}s\")\n",
    "print(f\"Avg time per document: {elapsed / status.successful_documents:.2f}s\")\n",
    "print(f\"Throughput: {status.total_chunks_indexed / elapsed:.2f} chunks/sec\")\n",
    "\n",
    "# Show per-document results\n",
    "print(f\"\\nPer-document breakdown:\")\n",
    "for result in status.results:\n",
    "    status_icon = \"‚úÖ\" if result.success else \"‚ùå\"\n",
    "    print(f\"  {status_icon} {result.filename}\")\n",
    "    print(f\"     Time: {result.processing_time_seconds:.2f}s\")\n",
    "    print(f\"     Chunks: {result.chunks_indexed}\")\n",
    "    if not result.success:\n",
    "        print(f\"     Error: {result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure from Environment Variables\n",
    "\n",
    "Set optimization settings in your `.env` file for consistent behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env file with these settings:\n",
    "\"\"\"\n",
    "# Batch Processing Optimizations\n",
    "AZURE_MAX_WORKERS=4\n",
    "AZURE_OPENAI_MAX_CONCURRENCY=10\n",
    "AZURE_DI_MAX_CONCURRENCY=5\n",
    "AZURE_USE_INTEGRATED_VECTORIZATION=true\n",
    "\"\"\"\n",
    "\n",
    "# Then just load from environment:\n",
    "from ingestor import PipelineConfig\n",
    "\n",
    "config = PipelineConfig.from_env()  # Loads all settings from .env\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded from environment:\")\n",
    "print(f\"   Max workers: {config.performance.max_workers}\")\n",
    "print(f\"   OpenAI concurrency: {config.azure_openai.max_concurrency}\")\n",
    "print(f\"   DI concurrency: {config.document_intelligence.max_concurrency}\")\n",
    "print(f\"   Integrated vectorization: {config.use_integrated_vectorization}\")\n",
    "\n",
    "# Use the config\n",
    "pipeline = Pipeline(config)\n",
    "status = await pipeline.run()\n",
    "await pipeline.close()\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {status.successful_documents} documents using env config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Different Environment Configurations\n",
    "\n",
    "Use different `.env` files for different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load development settings (conservative)\n",
    "load_dotenv(dotenv_path=\"../../.env.development\", override=True)\n",
    "dev_config = PipelineConfig.from_env()\n",
    "print(f\"Development config: max_workers={dev_config.performance.max_workers}\")\n",
    "\n",
    "# Load production settings (optimized)\n",
    "load_dotenv(dotenv_path=\"../../.env.production\", override=True)\n",
    "prod_config = PipelineConfig.from_env()\n",
    "print(f\"Production config: max_workers={prod_config.performance.max_workers}\")\n",
    "\n",
    "# Example .env.development:\n",
    "\"\"\"\n",
    "AZURE_MAX_WORKERS=2\n",
    "AZURE_OPENAI_MAX_CONCURRENCY=5\n",
    "AZURE_DI_MAX_CONCURRENCY=3\n",
    "\"\"\"\n",
    "\n",
    "# Example .env.production:\n",
    "\"\"\"\n",
    "AZURE_MAX_WORKERS=8\n",
    "AZURE_OPENAI_MAX_CONCURRENCY=15\n",
    "AZURE_DI_MAX_CONCURRENCY=8\n",
    "AZURE_USE_INTEGRATED_VECTORIZATION=true\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Performance Comparison\n",
    "\n",
    "Compare sequential vs parallel processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Test different configurations\n",
    "test_configs = [\n",
    "    {\n",
    "        \"name\": \"Sequential (Old)\",\n",
    "        \"max_workers\": 1,\n",
    "        \"openai_concurrency\": 1,\n",
    "        \"integrated_vectorization\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Parallel Batching\",\n",
    "        \"max_workers\": 1,\n",
    "        \"openai_concurrency\": 10,\n",
    "        \"integrated_vectorization\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Parallel Documents\",\n",
    "        \"max_workers\": 4,\n",
    "        \"openai_concurrency\": 10,\n",
    "        \"integrated_vectorization\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Full Optimization\",\n",
    "        \"max_workers\": 4,\n",
    "        \"openai_concurrency\": 10,\n",
    "        \"integrated_vectorization\": True\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for test in test_configs:\n",
    "    print(f\"\\nTesting: {test['name']}...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    status = await run_pipeline(\n",
    "        input_glob=\"../../sample_documents/*.pdf\",  # Small test set\n",
    "        performance_max_workers=test[\"max_workers\"],\n",
    "        azure_openai_max_concurrency=test[\"openai_concurrency\"],\n",
    "        use_integrated_vectorization=test[\"integrated_vectorization\"]\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        \"Configuration\": test[\"name\"],\n",
    "        \"Time (s)\": f\"{elapsed:.2f}\",\n",
    "        \"Documents\": status.successful_documents,\n",
    "        \"Chunks\": status.total_chunks_indexed,\n",
    "        \"Throughput\": f\"{status.total_chunks_indexed / elapsed:.2f} chunks/s\"\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úÖ Completed in {elapsed:.2f}s\")\n",
    "\n",
    "# Display comparison\n",
    "df = pd.DataFrame(results)\n",
    "print(f\"\\nüìä Performance Comparison:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Monitoring Batch Progress\n",
    "\n",
    "Monitor processing in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import time\n",
    "\n",
    "# Create config with progress tracking\n",
    "config = create_config(\n",
    "    input_glob=\"../../documents/**/*.pdf\",\n",
    "    performance_max_workers=4,\n",
    "    azure_openai_max_concurrency=10,\n",
    "    use_integrated_vectorization=True\n",
    ")\n",
    "\n",
    "# Process with pipeline (gives more control)\n",
    "pipeline = Pipeline(config)\n",
    "\n",
    "print(\"üîÑ Processing documents in parallel...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "status = await pipeline.run()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n‚úÖ Batch processing complete!\")\n",
    "print(f\"   Total time: {elapsed:.2f}s\")\n",
    "print(f\"   Successful: {status.successful_documents}\")\n",
    "print(f\"   Failed: {status.failed_documents}\")\n",
    "print(f\"   Total chunks: {status.total_chunks_indexed}\")\n",
    "\n",
    "await pipeline.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Error Handling in Batch Operations\n",
    "\n",
    "Handle failures gracefully in batch processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process batch and handle errors\n",
    "status = await run_pipeline(\n",
    "    input_glob=\"../../documents/**/*.pdf\",\n",
    "    performance_max_workers=4,\n",
    "    azure_openai_max_concurrency=10\n",
    ")\n",
    "\n",
    "# Check for failures\n",
    "if status.failed_documents > 0:\n",
    "    print(f\"‚ö†Ô∏è  {status.failed_documents} document(s) failed:\\n\")\n",
    "    \n",
    "    for result in status.results:\n",
    "        if not result.success:\n",
    "            print(f\"‚ùå {result.filename}\")\n",
    "            print(f\"   Error: {result.error_message}\")\n",
    "            print(f\"   Time spent: {result.processing_time_seconds:.2f}s\\n\")\n",
    "    \n",
    "    # Retry failed documents\n",
    "    print(\"üîÑ Retrying failed documents...\")\n",
    "    failed_files = [r.filename for r in status.results if not r.success]\n",
    "    \n",
    "    # Process failed documents individually with more conservative settings\n",
    "    for failed_file in failed_files:\n",
    "        try:\n",
    "            retry_status = await run_pipeline(\n",
    "                input_glob=failed_file,\n",
    "                performance_max_workers=1,\n",
    "                azure_openai_max_concurrency=3  # Reduce concurrency\n",
    "            )\n",
    "            if retry_status.successful_documents > 0:\n",
    "                print(f\"   ‚úÖ Retry succeeded: {failed_file}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Retry failed: {failed_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Retry error for {failed_file}: {e}\")\n",
    "else:\n",
    "    print(f\"‚úÖ All {status.successful_documents} documents processed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Recommended .env Configuration\n",
    "\n",
    "Add these settings to your `.env` file for optimal batch processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended .env settings for batch processing\n",
    "recommended_env = \"\"\"\n",
    "# === Azure Services ===\n",
    "AZURE_SEARCH_SERVICE=your-service\n",
    "AZURE_SEARCH_INDEX=documents-index\n",
    "AZURE_SEARCH_KEY=your-key\n",
    "\n",
    "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://your-di.cognitiveservices.azure.com/\n",
    "AZURE_DOCUMENT_INTELLIGENCE_KEY=your-key\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT=https://your-openai.openai.azure.com\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002\n",
    "AZURE_OPENAI_KEY=your-key\n",
    "\n",
    "# === Batch Processing Optimizations (NEW!) ===\n",
    "AZURE_MAX_WORKERS=4                          # Process 4 documents in parallel\n",
    "AZURE_OPENAI_MAX_CONCURRENCY=10              # Parallel embedding batches\n",
    "AZURE_DI_MAX_CONCURRENCY=5                   # Parallel DI requests\n",
    "AZURE_USE_INTEGRATED_VECTORIZATION=true      # Server-side embeddings (fastest!)\n",
    "\n",
    "# === Input/Output ===\n",
    "AZURE_INPUT_MODE=local                       # or 'blob'\n",
    "AZURE_LOCAL_GLOB=documents/**/*.pdf          # Recursive glob\n",
    "AZURE_ARTIFACTS_MODE=blob                    # or 'local'\n",
    "\"\"\"\n",
    "\n",
    "print(\"Copy this to your .env file:\")\n",
    "print(recommended_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully learned:\n",
    "\n",
    "‚úÖ **Batch processing** with parallel document execution  \n",
    "‚úÖ **Performance optimization** with `max_workers` and concurrency settings  \n",
    "‚úÖ **Environment configuration** with `.env` files  \n",
    "‚úÖ **Progress monitoring** and error handling  \n",
    "‚úÖ **Performance comparison** between configurations  \n",
    "\n",
    "## Performance Improvements\n",
    "\n",
    "With optimizations enabled:\n",
    "- **Single 100-page PDF**: 50-60% faster\n",
    "- **4 x 100-page PDFs**: 75-85% faster\n",
    "- **Parallel batching**: 3-5x faster embedding generation\n",
    "- **Integrated vectorization**: Eliminates client-side embedding time\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **07_performance_tuning.ipynb**: Deep dive into performance optimization\n",
    "- **06_troubleshooting.ipynb**: Debug rate limits and errors\n",
    "- **03_advanced_features.ipynb**: Explore advanced configuration options\n",
    "\n",
    "## Related Documentation\n",
    "\n",
    "- [QUICK_START_OPTIMIZATIONS.md](../../QUICK_START_OPTIMIZATIONS.md)\n",
    "- [PARALLEL_OPTIMIZATIONS_APPLIED.md](../../docs/PARALLEL_OPTIMIZATIONS_APPLIED.md)\n",
    "- [PERFORMANCE_SUMMARY.md](../../docs/PERFORMANCE_SUMMARY.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
