# ==========================================
# ENVIRONMENT CONFIGURATION - MASTER TEMPLATE
# ==========================================
#
# ğŸ“– COMPLETE GUIDE: See ENVIRONMENT_CONFIGURATION_GUIDE.md
# ğŸ“– QUICK START: See SETUP_GUIDE.md
# ğŸ“– VALIDATION: Run `python -m ingestor.scenario_validator`
#
# IMPORTANT:
# 1. Copy this file to .env: cp .env.example .env
# 2. Choose your scenario (see below)
# 3. Uncomment and fill in values for your chosen scenario
# 4. Keep other sections commented out if not used
# 5. Run validation: python -m ingestor.cli --validate
#
# ==========================================
# CHOOSE YOUR SCENARIO
# ==========================================
#
# Uncomment ONE of these scenarios and fill in the required values:
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ SCENARIO A: Local Development (No Azure Required)               â”‚
# â”‚ Best for: Testing, development, learning                        â”‚
# â”‚ Cost: FREE | Setup: 5 min | Template: envs/.env.scenario-dev   â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# VECTOR_STORE_MODE=chromadb
# CHROMADB_COLLECTION_NAME=dev-documents
# EMBEDDINGS_MODE=huggingface
# HUGGINGFACE_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
# HUGGINGFACE_DEVICE=cpu
# AZURE_INPUT_MODE=local
# AZURE_LOCAL_GLOB=documents/**/*.pdf
# AZURE_ARTIFACTS_DIR=./artifacts
# AZURE_OFFICE_EXTRACTOR_MODE=markitdown
# AZURE_USE_INTEGRATED_VECTORIZATION=false
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ SCENARIO B: Azure Full Stack (Production)                       â”‚
# â”‚ Best for: Enterprise production, highest quality                â”‚
# â”‚ Cost: $$$ | Setup: 30 min | Template: envs/.env.example        â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# VECTOR_STORE_MODE=azure_search
# AZURE_SEARCH_SERVICE=your-search-service
# AZURE_SEARCH_INDEX=documents
# AZURE_SEARCH_KEY=your-search-admin-key
#
# EMBEDDINGS_MODE=azure_openai
# AZURE_OPENAI_ENDPOINT=https://your-openai.openai.azure.com/
# AZURE_OPENAI_KEY=your-openai-key
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002
#
# AZURE_DOC_INT_ENDPOINT=https://your-di.cognitiveservices.azure.com/
# AZURE_DOC_INT_KEY=your-di-key
#
# AZURE_STORAGE_ACCOUNT=your-storage-account
# AZURE_STORAGE_ACCOUNT_KEY=your-storage-key
# AZURE_STORAGE_CONTAINER=documents
#
# AZURE_INPUT_MODE=blob
# AZURE_OFFICE_EXTRACTOR_MODE=hybrid
# AZURE_USE_INTEGRATED_VECTORIZATION=true
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ SCENARIO C: Cost-Optimized (Azure Search + Local Embeddings)    â”‚
# â”‚ Best for: High volume, cost optimization                        â”‚
# â”‚ Cost: $$ | Setup: 20 min | Template: envs/.env.hybrid.example  â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# VECTOR_STORE_MODE=azure_search
# AZURE_SEARCH_SERVICE=your-search-service
# AZURE_SEARCH_INDEX=documents
# AZURE_SEARCH_KEY=your-search-admin-key
#
# EMBEDDINGS_MODE=huggingface
# HUGGINGFACE_MODEL_NAME=intfloat/multilingual-e5-large
# HUGGINGFACE_DEVICE=cuda  # or cpu, mps
#
# AZURE_STORAGE_ACCOUNT=your-storage-account
# AZURE_STORAGE_ACCOUNT_KEY=your-storage-key
# AZURE_INPUT_MODE=blob
# AZURE_STORAGE_CONTAINER=documents
#
# AZURE_USE_INTEGRATED_VECTORIZATION=false  # IMPORTANT!
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ SCENARIO D: Fully Offline (Air-Gapped/Secure)                   â”‚
# â”‚ Best for: Secure environments, compliance                       â”‚
# â”‚ Cost: FREE | Setup: 10 min | Template: envs/.env.chromadb      â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# VECTOR_STORE_MODE=chromadb
# CHROMADB_COLLECTION_NAME=offline-documents
# CHROMADB_PERSIST_DIR=./chroma_db
#
# EMBEDDINGS_MODE=huggingface
# HUGGINGFACE_MODEL_NAME=sentence-transformers/all-mpnet-base-v2
# HUGGINGFACE_DEVICE=cpu
#
# AZURE_INPUT_MODE=local
# AZURE_LOCAL_GLOB=documents/**/*.pdf
# AZURE_ARTIFACTS_DIR=./artifacts
# AZURE_OFFICE_EXTRACTOR_MODE=markitdown
# AZURE_USE_INTEGRATED_VECTORIZATION=false
#
# ==========================================
# DETAILED CONFIGURATION (All Options)
# ==========================================
# The sections below document ALL available options.
# Only uncomment what you need for your scenario.
# ==========================================

# ==========================================
# 1. VECTOR STORE CONFIGURATION
# ==========================================
# Choose where to store your vector embeddings

# Vector store type (required)
# Options: azure_search | chromadb
# VECTOR_STORE_MODE=azure_search

# --- Option A: Azure AI Search ---
# AZURE_SEARCH_SERVICE=your-search-service          # Search service name
# AZURE_SEARCH_ENDPOINT=https://...                 # OR full endpoint URL
# AZURE_SEARCH_INDEX=documents                      # Index name (required)
# AZURE_SEARCH_KEY=your-admin-key                   # Admin key (or use managed identity)

# --- Option B: ChromaDB (Local/Offline) ---
# CHROMADB_COLLECTION_NAME=documents                # Collection name
# CHROMADB_PERSIST_DIR=./chroma_db                  # Persistent storage (omit for in-memory)
# CHROMADB_HOST=localhost                           # For client/server mode (optional)
# CHROMADB_PORT=8000                                # For client/server mode (optional)
# CHROMADB_BATCH_SIZE=1000                          # Upload batch size

# ==========================================
# 2. EMBEDDINGS CONFIGURATION
# ==========================================
# Choose how to generate embeddings

# Embeddings provider (required)
# Options: azure_openai | huggingface | cohere | openai
# EMBEDDINGS_MODE=azure_openai

# --- Option A: Azure OpenAI (Default) ---
# AZURE_OPENAI_ENDPOINT=https://your-openai.openai.azure.com/
# AZURE_OPENAI_KEY=your-key
# AZURE_OPENAI_API_VERSION=2024-12-01-preview
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002
# AZURE_OPENAI_EMBEDDING_MODEL=text-embedding-ada-002
# AZURE_OPENAI_EMBEDDING_DIMENSIONS=1536            # For text-embedding-3-* models
# AZURE_OPENAI_MAX_CONCURRENCY=5
# AZURE_OPENAI_MAX_RETRIES=3

# --- Option B: HuggingFace (Local/Free) ---
# HUGGINGFACE_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
# HUGGINGFACE_DEVICE=cpu                            # cpu | cuda | mps
# HUGGINGFACE_BATCH_SIZE=32
# HUGGINGFACE_NORMALIZE=true

# Popular HuggingFace models:
# - all-MiniLM-L6-v2 (384 dims, fast, 90MB)
# - all-mpnet-base-v2 (768 dims, quality, 420MB)
# - intfloat/multilingual-e5-large (1024 dims, multilingual, 2.2GB)
# - BAAI/bge-large-en-v1.5 (1024 dims, SOTA English, 1.3GB)

# --- Option C: Cohere (Cloud API) ---
# COHERE_API_KEY=your-cohere-key
# COHERE_MODEL_NAME=embed-multilingual-v3.0        # Or embed-english-v3.0
# COHERE_INPUT_TYPE=search_document
# COHERE_TRUNCATE=END

# --- Option D: OpenAI (Non-Azure) ---
# OPENAI_API_KEY=your-openai-key
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# OPENAI_EMBEDDING_DIMENSIONS=1536
# OPENAI_MAX_RETRIES=3

# ==========================================
# 3. INTEGRATED VECTORIZATION
# ==========================================
# CRITICAL: Set to false when using non-Azure-OpenAI embeddings!

# Use Azure Search integrated vectorization?
# - true: Azure Search generates embeddings (requires Azure OpenAI)
# - false: Client-side embeddings (required for HuggingFace, Cohere, etc.)
# AZURE_USE_INTEGRATED_VECTORIZATION=false

# ==========================================
# 4. DOCUMENT INTELLIGENCE (PDF/Office Extraction)
# ==========================================

# Azure Document Intelligence (for highest quality extraction)
# AZURE_DOC_INT_ENDPOINT=https://your-di.cognitiveservices.azure.com/
# AZURE_DOC_INT_KEY=your-key
# AZURE_DI_MAX_CONCURRENCY=3

# Extraction mode
# Options:
#   - azure_di: Azure DI only (highest quality, no DOC support)
#   - markitdown: Offline only (lower quality, supports DOC)
#   - hybrid: Try Azure DI, fallback to markitdown (RECOMMENDED)
# AZURE_OFFICE_EXTRACTOR_MODE=hybrid

# Fallback configuration (hybrid mode only)
# AZURE_OFFICE_OFFLINE_FALLBACK=true
# AZURE_OFFICE_LIBREOFFICE_PATH=/usr/bin/soffice   # For DOC files (optional)

# ==========================================
# 5. INPUT SOURCE CONFIGURATION
# ==========================================

# Input mode (required)
# Options: local | blob
AZURE_INPUT_MODE=local

# --- Option A: Local Files ---
# AZURE_LOCAL_GLOB=documents/**/*.pdf               # Glob pattern for files
# AZURE_LOCAL_GLOB=documents/**/*.{pdf,docx,txt}    # Multiple extensions

# --- Option B: Azure Blob Storage ---
# AZURE_STORAGE_ACCOUNT=your-storage-account
# AZURE_STORAGE_ACCOUNT_KEY=your-key
# # OR use connection string:
# # AZURE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=...
#
# # Simple approach (auto-creates containers):
# AZURE_STORAGE_CONTAINER=documents                 # Base name
# # Creates: documents-input, documents-pages, documents-chunks, etc.
#
# # OR explicit approach:
# AZURE_BLOB_CONTAINER_IN=documents-input           # Input container
# AZURE_BLOB_PREFIX=                                # Optional prefix filter

# ==========================================
# 6. ARTIFACTS STORAGE CONFIGURATION
# ==========================================

# Where to store processing artifacts (pages, chunks, images)

# --- Option A: Local Directory ---
AZURE_ARTIFACTS_DIR=./artifacts

# --- Option B: Azure Blob Storage ---
# Uses same storage account as input (see section 5)
# Containers auto-created based on AZURE_STORAGE_CONTAINER or explicit names:
# AZURE_BLOB_CONTAINER_OUT_PAGES=pages
# AZURE_BLOB_CONTAINER_OUT_CHUNKS=chunks
# AZURE_BLOB_CONTAINER_OUT_IMAGES=images
# AZURE_BLOB_CONTAINER_CITATIONS=citations

# ==========================================
# 7. MEDIA DESCRIPTION
# ==========================================

# Generate descriptions for images/figures?
# Options: gpt4o | disabled
# AZURE_MEDIA_DESCRIBER=disabled

# If using gpt4o, requires Azure OpenAI with chat deployment:
# AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o-mini

# ==========================================
# 8. TABLE RENDERING
# ==========================================

# How to render tables in output?
# Options: plain | markdown | html
# AZURE_TABLE_RENDER=markdown

# Generate AI summaries for tables?
# AZURE_TABLE_SUMMARIES=false

# ==========================================
# 9. CHUNKING CONFIGURATION
# ==========================================
# Controls how documents are split into searchable chunks

# Generic parameter names (recommended):
# CHUNKING_MAX_TOKENS=500                           # Target tokens per chunk
# CHUNKING_MAX_CHARS=2000                           # Soft character limit
# CHUNKING_OVERLAP_PERCENT=10                       # Overlap between chunks
# CHUNKING_CROSS_PAGE_OVERLAP=false                 # Allow cross-page overlap

# Azure-prefixed names also work (backward compatibility):
# AZURE_CHUNKING_MAX_TOKENS=500
# AZURE_CHUNKING_MAX_CHARS=2000
# AZURE_CHUNKING_OVERLAP_PERCENT=10

# NOTE: Pipeline auto-adjusts limits based on embedding model capacity

# ==========================================
# 10. PERFORMANCE TUNING
# ==========================================

# Parallelization
# AZURE_MAX_WORKERS=4                               # Document processing workers
# AZURE_EMBED_BATCH_SIZE=128                        # Embedding batch size
# AZURE_UPLOAD_BATCH_SIZE=1000                      # Search upload batch size
# AZURE_MAX_IMAGE_CONCURRENCY=8                     # Parallel image processing
# AZURE_MAX_FIGURE_CONCURRENCY=5                    # Parallel figure extraction

# ==========================================
# 11. LOGGING CONFIGURATION
# ==========================================

# Log levels: DEBUG | INFO | WARNING | ERROR | CRITICAL
# LOG_LEVEL=INFO                                    # Console output
# LOG_FILE_LEVEL=DEBUG                              # File output
# LOG_ARTIFACTS=true                                # Write detailed artifact logs
# LOG_USE_COLORS=true                               # Colorful console output

# ==========================================
# 12. AZURE SERVICE PRINCIPAL (Optional)
# ==========================================
# For Key Vault and managed identity authentication

# AZURE_TENANT_ID=your-tenant-id
# AZURE_CLIENT_ID=your-client-id
# AZURE_CLIENT_SECRET=your-client-secret

# ==========================================
# 13. AZURE KEY VAULT (Optional)
# ==========================================
# Store secrets in Azure Key Vault

# KEY_VAULT_URI=https://your-keyvault.vault.azure.net/

# ==========================================
# 14. GRADIO UI (Optional)
# ==========================================

# GRADIO_SHARE=false                                # Share publicly?
# GRADIO_SERVER_PORT=7860                           # UI port

# ==========================================
# VALIDATION & NEXT STEPS
# ==========================================
#
# After configuring:
#
# 1. Validate configuration:
#    python -m ingestor.scenario_validator
#
# 2. Pre-check everything:
#    python -m ingestor.cli --validate
#
# 3. Setup index (if using Azure Search):
#    python -m ingestor.cli --setup-index
#
# 4. Process documents:
#    python -m ingestor.cli --glob "documents/*.pdf"
#
# For help:
# - Quick start: SETUP_GUIDE.md
# - Complete guide: ENVIRONMENT_CONFIGURATION_GUIDE.md
# - Examples: envs/ directory
# - Validation: python -m ingestor.scenario_validator
#
# ==========================================
# SECURITY NOTES
# ==========================================
# 1. NEVER commit .env to version control
# 2. Add .env to .gitignore (already done)
# 3. Use Azure Key Vault for production
# 4. Rotate keys regularly
# 5. Keep backups of .env in secure location
# ==========================================
